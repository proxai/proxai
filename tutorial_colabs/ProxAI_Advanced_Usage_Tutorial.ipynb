{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/proxai/proxai/blob/main/tutorial_colabs/ProxAI_Advanced_Usage_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z6UUzIzGzbW"
      },
      "source": [
        "# üöÄ ProxAI Advanced Usage Tutorial üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NPds7cLG1bi"
      },
      "source": [
        "## üëã Introduction\n",
        "\n",
        "Welcome to the ProxAI Advanced Usage Tutorial! This notebook will guide you through some of the advanced features of the ProxAI library. ProxAI unifies AI connections across different providers and offers powerful tools to manage your AI interactions.\n",
        "\n",
        "In this tutorial, we will cover:\n",
        "1. ‚ö°Ô∏è Setting up ProxAI in Google Colab\n",
        "2. üîã List Available Models\n",
        "3. ü§ñ Generate Text\n",
        "4. üîÆ Set Global Model\n",
        "5. üß™ Experiment Path\n",
        "6. üéûÔ∏è Logs Management\n",
        "7. üíæ Cache System - ‚≠êÔ∏è Highly Recommended! ‚≠êÔ∏è\n",
        "8. üîå ProxDash Connection\n",
        "9. üö¶ Strict Feature Test\n",
        "10. ‚ö†Ô∏è Suppress Provider Errors\n",
        "11. üìù Get Current Options\n",
        "12. üçæ Final Thoughts and Next Steps\n",
        "\n",
        "Remember, the most effective way to master ProxAI (or any tool!) is to **dive in and use it directly to solve problems.** This tutorial is useful for people who want to formally dive deep into the powerful features of ProxAI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxLuPcknG-BS"
      },
      "source": [
        "# 1. ‚ö°Ô∏è Setup in Google Colab\n",
        "Documentation: [proxai.co/proxai-docs](https://www.proxai.co/proxai-docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMzcnmqIG_lx"
      },
      "source": [
        "## 1.1. üíª Installation (UPDATE THIS TO PIP)\n",
        "\n",
        "First, let's install the ProxAI library. We'll clone the repository and install it.\n",
        "\n",
        "You can track releases on the [roadmap page](/resources/roadmap) üó∫Ô∏è.\n",
        "\n",
        "**Note:** After running the installation cell, you will likely need to **üîÑ restart the Colab session** using the button that appears in the output of the cell or by going to `Runtime > Restart session`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRecvegyGfwT"
      },
      "outputs": [],
      "source": [
        "!pip install proxai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsWwX2J-HPKd"
      },
      "source": [
        "## 1.2. üîë API Key Management\n",
        "\n",
        "ProxAI works with various AI providers. You'll need to add your API keys as secrets in Google Colab. This is the safest way to handle them.\n",
        "\n",
        "1.  Click on the **üîë icon (Secrets)** in the left sidebar of Colab.\n",
        "2.  Add your API keys with the names ProxAI expects (e.g., `OPENAI_API_KEY`, `GEMINI_API_KEY`, `PROXDASH_API_KEY`, etc.). Refer to the [Provider Integrations documentation](https://www.proxai.co/proxai-docs/provider-integrations) for the full list of environment keys.\n",
        "\n",
        "Run the following cell to load your API keys from Colab secrets into the environment.\n",
        "\n",
        "<div style=\"background-color: #ffebee; border-left: 6px solid #f44336; padding: 10px; margin-bottom: 15px;\">\n",
        "  <p style=\"margin: 0; font-weight: bold; color: #c62828;\">üö´ Important Security Note:</p>\n",
        "  <p style=\"margin: 0; color: #c62828;\">Never directly add API key values as string variables inside the Colab cells. Even after deletion, they can be retrieved from the Colab history.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEVpT_-7HdqU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from dataclasses import asdict\n",
        "from pprint import pprint\n",
        "\n",
        "API_KEY_LIST = [\n",
        "    'GEMINI_API_KEY',\n",
        "    'OPENAI_API_KEY',\n",
        "    'ANTHROPIC_API_KEY',\n",
        "    # 'XAI_API_KEY',\n",
        "    # 'DEEPSEEK_API_KEY',\n",
        "    # 'MISTRAL_API_KEY',\n",
        "    # 'CO_API_KEY',\n",
        "    # 'DATABRICKS_HOST',\n",
        "    # 'DATABRICKS_TOKEN',\n",
        "    # 'HUGGINGFACE_API_KEY',\n",
        "    'PROXDASH_API_KEY', # For ProxDash connection\n",
        "]\n",
        "\n",
        "print(\"üîê Attempting to load API keys from Colab secrets...\")\n",
        "for api_key_name in API_KEY_LIST:\n",
        "  try:\n",
        "    os.environ[api_key_name] = userdata.get(api_key_name)\n",
        "    print(f\"  ‚úÖ Successfully loaded {api_key_name}\")\n",
        "  except userdata.SecretNotFoundError:\n",
        "    print(f\"  ‚ö†Ô∏è Secret for {api_key_name} not found. Skipping.\")\n",
        "  except Exception as e:\n",
        "    print(f\"  ‚ùå An error occurred while loading {api_key_name}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZim4oRpKc2J"
      },
      "source": [
        "## 1.3. ‚ñ∂Ô∏è Import ProxAI\n",
        "\n",
        "Ready to go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-0Fp1hzKoWq"
      },
      "outputs": [],
      "source": [
        "import proxai as px"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZpFbUStIVbr"
      },
      "source": [
        "# 2. üîã List Available Models\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/available-models](https://www.proxai.co/proxai-docs/available-models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhysxMyOIxkn"
      },
      "source": [
        "## 2.1. ü™õ Simple Usage\n",
        "\n",
        "Let's list available models in our session! üéâ \\\n",
        "**Note:** This can take for a while for the first run but the results are cached and it will be fast for other runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu7PIc6rI2-l"
      },
      "outputs": [],
      "source": [
        "provider_models = px.models.list_models()\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model.provider:>25} - {provider_model.model}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t4gdLhiM7fa"
      },
      "source": [
        "## 2.2. üî≠ Different Model Sizes\n",
        "\n",
        "It is possible to filter out models according to ProxAI sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUre0elWNV7M"
      },
      "outputs": [],
      "source": [
        "provider_models = px.models.list_models(model_size='small')\n",
        "print('ü•ö Small models:')\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
        "\n",
        "provider_models = px.models.list_models(model_size='medium')\n",
        "print('üê£ Medium models:')\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
        "\n",
        "provider_models = px.models.list_models(model_size='large')\n",
        "print('üê• Large models:')\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
        "\n",
        "provider_models = px.models.list_models(model_size='largest')\n",
        "print('üêì Largest models of each provider:')\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model.provider:>25} - {provider_model.model}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er6mKLlWOPFj"
      },
      "source": [
        "## 2.3. üìÉ Details of Model List\n",
        "\n",
        "You can get detailed metadata of available models:\n",
        "* `working_models`: List of working models\n",
        "* `failed_models`: List of failed models\n",
        "* `provider_queries`: Dictionary of provider queries that each key has `px.types.ProviderModelType` and each value has `px.types.LoggingRecord`. You can check error messages from these queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJkKozHIOeW_"
      },
      "outputs": [],
      "source": [
        "model_status = px.models.list_models(return_all=True)\n",
        "\n",
        "print(f'‚úÖ Available models: {len(model_status.working_models)}')\n",
        "print(f'‚ùå Failed models: {len(model_status.failed_models)}\\n')\n",
        "errors = []\n",
        "for provider_model, query in model_status.provider_queries.items():\n",
        "  if query.response_record.error:\n",
        "    errors.append((str(provider_model), query.response_record.error))\n",
        "pprint(errors[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9jEL82ZRIZJ"
      },
      "source": [
        "# 3. ü§ñ Generate Text\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/generate-text](https://www.proxai.co/proxai-docs/generate-text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0D2_cwHROl6"
      },
      "source": [
        "## 3.1. üê∂ The Simplest Usage\n",
        "\n",
        "You can directly call `px.generate_text()` without any additional paramters. ProxAI picks default model or fallback models if default model is not working."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylz2eGPpROGT"
      },
      "outputs": [],
      "source": [
        "response = px.generate_text('Hello! Which model are you?')\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QQ8rJbrR-yD"
      },
      "source": [
        "## 3.2. ‚úèÔ∏è Setting Provider Model\n",
        "\n",
        "There is two different way of directly setting model on `px.generate_text()`\n",
        "* Tuple with provider and model string\n",
        "* `px.types.ProviderModelType` value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BSRsKI5SS4t"
      },
      "outputs": [],
      "source": [
        "print('‚úèÔ∏è Tuple provider_model value:')\n",
        "response = px.generate_text(\n",
        "    'Hello! Which model are you?',\n",
        "    provider_model=('claude', 'haiku'))\n",
        "print(response)\n",
        "\n",
        "print('\\n‚úíÔ∏è px.types.ProviderModelType value:')\n",
        "response = px.generate_text(\n",
        "    'Hello! Which model are you?',\n",
        "    provider_model=px.models.get_model('gemini', 'gemini-1.5-flash'))\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5lOHpxwTK1L"
      },
      "source": [
        "## 3.3. üëë System Prompt\n",
        "\n",
        "You can set system prompt as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWSxDa9WTMzK"
      },
      "outputs": [],
      "source": [
        "response = px.generate_text(\n",
        "    'Hello! Which model are you?',\n",
        "    system=\"You are an helpful assitant that allways answers in Japan.\",\n",
        "    provider_model=('claude', 'haiku'))\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OSqVD_RlsJq"
      },
      "source": [
        "## 3.4. üí¨ Message History\n",
        "\n",
        "It is possible to give the history of conversations to the model via messages. This helps to give context to the model.\n",
        "* (role=user/assistant, content=text) is a common format for AI provider APIs.\n",
        "* Some provider uses different tags but you don't need to worry about it. ProxAI handles these integrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEAkI2pslrmJ"
      },
      "outputs": [],
      "source": [
        "response = px.generate_text(\n",
        "    system=\"No matter what, always answer with single integer.\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Hello AI Model!\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"17\"},\n",
        "        {\"role\": \"user\", \"content\": \"How are you today?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"923123\"},\n",
        "        {\"role\": \"user\",\n",
        "         \"content\": \"Can you answer question without any integer?\"}\n",
        "    ],\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaZFlHddmgMr"
      },
      "source": [
        "## 3.5. ‚úã Max Tokens\n",
        "\n",
        "Limit the token size to avoid cost and delay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9GqNRx_mtVs"
      },
      "outputs": [],
      "source": [
        "response = px.generate_text(\n",
        "    'Can you write all numbers from 1 to 1000?',\n",
        "    max_tokens=20)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEUIopUXnQZE"
      },
      "source": [
        "## 3.6. üå°Ô∏è Tempreture\n",
        "\n",
        "If you are looking for more creative answers and more randomness, you can set tempreture to lower values than 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJtjOOpPnQOC"
      },
      "outputs": [],
      "source": [
        "response = px.generate_text(\n",
        "    'If 5 + 20 would be a poem, what life be look like?',\n",
        "    temperature=0.01)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yu0YwPKmfum"
      },
      "source": [
        "## 3.7. ‚òïÔ∏è Extensive Return\n",
        "\n",
        "You can get all details and metadata about query made to the provider by setting `extensive_return` to `True`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vzn8NhUnFRg"
      },
      "outputs": [],
      "source": [
        "response = px.generate_text(\n",
        "    'Hello! Which model are you?',\n",
        "    extensive_return=True)\n",
        "pprint(asdict(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_FjIokkntlY"
      },
      "source": [
        "## 3.8. ‚ö†Ô∏è Suppress Provider Errors\n",
        "\n",
        "If you don't want to raise error when provider fails and just want to continue with error message, you can set `suppress_provider_errors` to `True`.\n",
        "* Check error message on `logging_record.response_record.error`\n",
        "* Check error traceback on `logging_record.response_record.error_traceback`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rTWF9Mmnt4G"
      },
      "outputs": [],
      "source": [
        "# First, lets pick already failing model:\n",
        "model_status = px.models.list_models(return_all=True)\n",
        "if not model_status.failed_models:\n",
        "  raise ValueError(\n",
        "      \"There is no failed models to try \\'suppress_provider_errors\\' option.\")\n",
        "provider_model = list(model_status.failed_models)[0]\n",
        "\n",
        "# Second, make call with suppress_provider_errors=True:\n",
        "response = px.generate_text(\n",
        "    'If 5 + 20 would be a poem, what life be look like?',\n",
        "    provider_model=provider_model,\n",
        "    suppress_provider_errors=True,\n",
        "    extensive_return=True)\n",
        "\n",
        "# No error raised before printing the response or error:\n",
        "print(f'ü§ñ Model: {response.query_record.provider_model}')\n",
        "print(f'üí¨ Response: {response.response_record.response}')\n",
        "print(f'‚ùå Error: {response.response_record.error.strip()}')\n",
        "print(f'‚ö†Ô∏è Error Traceback:\\n{response.response_record.error_traceback.strip()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8qgLfKrqlQM"
      },
      "source": [
        "# 4. üîÆ Set Global Model\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/set-global-model](https://www.proxai.co/proxai-docs/set-global-model)\n",
        "\n",
        "You can set global default model by `px.set_model()` instead of using what ProxAI picks for you. All unspecified `px.generate_text()` calls will use this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ujKRyXrrtct"
      },
      "outputs": [],
      "source": [
        "# Let's define python method that doesn't specify provider_model\n",
        "def simple_request():\n",
        "  return px.generate_text(\n",
        "      'Hey AI model! This is simple request. Give an answer. Quick!',\n",
        "  ).strip().replace('\\n', ' ')[:80]\n",
        "\n",
        "# We can change default model by px.set_model\n",
        "for provider_model in px.models.list_models():\n",
        "  px.set_model(provider_model)\n",
        "  response = simple_request()\n",
        "  print(f'{provider_model} - {response}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quLssC0fsozQ"
      },
      "source": [
        "# 5. ‚ù§Ô∏è‚Äçü©π Check Health\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/check-health](https://www.proxai.co/proxai-docs/check-health)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrNmqGRAvY4w"
      },
      "source": [
        "## 5.1. üê± Simple Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAVal1wCvmyF"
      },
      "outputs": [],
      "source": [
        "px.check_health()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPeVoeo82ldg"
      },
      "source": [
        "## 5.2. üìÉ Extensive Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz7GkjRt2q0e"
      },
      "outputs": [],
      "source": [
        "model_status = px.check_health(verbose=False, extensive_return=True)\n",
        "print('--- model_status.working_models:')\n",
        "pprint(model_status.working_models)\n",
        "print('--- model_status.failed_models:')\n",
        "pprint(model_status.failed_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikUGCN2s2uwi"
      },
      "source": [
        "# 6. üß™ Experiment Path\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/advanced/experiment-path](https://www.proxai.co/proxai-docs/advanced/experiment-path)\n",
        "\n",
        "To able unlock experiments path features, please be sure you complated following steps:\n",
        "1. Open ProxAI account from [proxai.co/signup](https://www.proxai.co/signup)\n",
        "2. Create ProxAI API key from [proxai.co/dashboard/api-keys](https://www.proxai.co/dashboard/api-keys)\n",
        "3. Add new API key to üîë Colab Secrets from left as `PROXDASH_API_KEY`\n",
        "4. Run \"1.2. üîë API Key Management\" cell again to load API key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbvLstu23W_q"
      },
      "source": [
        "## 6.1. üê≠ Simple Usage with ProxDash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53BN-DcP3Whk"
      },
      "outputs": [],
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_1')\n",
        "\n",
        "px.generate_text('Please recommend me a good movie.')\n",
        "\n",
        "print('1 - Open ProxDash experiments page: https://proxai.co/dashboard/experiments')\n",
        "print('2 - Open colab_experiments/advanced_features/run_1 from experiment tree. (Refresh experiments if necessary)')\n",
        "print('3 - Check logging records tab to see movie recommendation query.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMkcmVfH55EQ"
      },
      "source": [
        "# 7. üéûÔ∏è Logs Management\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/advanced/logs-management](https://www.proxai.co/proxai-docs/advanced/logs-management)\n",
        "\n",
        "This feature can be more useful in local runs rather than Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpihvARS6aei"
      },
      "source": [
        "## 7.1. üêπ Simple Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFW76TzX6Zs-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_1',\n",
        "    logging_path='/content/')\n",
        "\n",
        "px.generate_text('Hello model!')\n",
        "\n",
        "print(os.listdir('/content/colab_experiments/advanced_features/run_1'))\n",
        "\n",
        "with open('/content/colab_experiments/advanced_features/run_1/provider_queries.log', 'r') as f:\n",
        "  for line in f:\n",
        "    pprint(json.loads(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yciz8x4Q6oxY"
      },
      "source": [
        "## 7.2 üïµÔ∏è Hide Sensitive Content\n",
        "\n",
        "You can hide sensitive contents like prompt, response, messages etc. from log files to make proxai more secure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjFKQGFS6oGp"
      },
      "outputs": [],
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_2',\n",
        "    logging_options=px.types.LoggingOptions(\n",
        "        logging_path='/content/',\n",
        "        hide_sensitive_content=True,\n",
        "    ))\n",
        "\n",
        "px.generate_text('Hello model!')\n",
        "\n",
        "print(os.listdir('/content/colab_experiments/advanced_features/run_2'))\n",
        "\n",
        "print('Following file should\\'t show the sensitive information:')\n",
        "with open('/content/colab_experiments/advanced_features/run_2/provider_queries.log', 'r') as f:\n",
        "  for line in f:\n",
        "    pprint(json.loads(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiXCZ0X26n0J"
      },
      "source": [
        "## 7.3. üñ•Ô∏è Stdout\n",
        "\n",
        "There is option for printing all logs to the stdout. It is useful for debugging cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN_CP6FZ7U0A"
      },
      "outputs": [],
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_2',\n",
        "    logging_options=px.types.LoggingOptions(\n",
        "        logging_path='/content/',\n",
        "        stdout=True,\n",
        "    ))\n",
        "\n",
        "print('You should be able to see logging record on cell output for following:')\n",
        "response = px.generate_text('Hello model!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmkM9-E77odn"
      },
      "source": [
        "# 8. üíæ Cache System - ‚≠êÔ∏è Highly Recommended! ‚≠êÔ∏è\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/advanced/cache-system](https://www.proxai.co/proxai-docs/advanced/cache-system)\n",
        "\n",
        "This feature is very useful at development stage. Without this feature, experiments can get very painful very easily.\n",
        "\n",
        "First, let's define simple method to get response and duration as in following section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sexgs-w98Dsv"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "def test_cache():\n",
        "  fixed_int = random.randint(10000, 20000)\n",
        "\n",
        "  def test_prompt():\n",
        "    start = time.time()\n",
        "    response = px.generate_text(\n",
        "        'Can you pick 100 different random positive integers which are less '\n",
        "        f'than {fixed_int}? Can you also explain why you picked these numbers? '\n",
        "        'Please think deeply about your decision and answer accordingly. '\n",
        "        'Start your sentence with random simple poem.',\n",
        "        temperature=0.3)\n",
        "    duration = time.time() - start\n",
        "    response = response.strip().replace('\\n', ' ')[:80]\n",
        "    return response, duration\n",
        "\n",
        "  for i in range(1, 7):\n",
        "    response, duration = test_prompt()\n",
        "    print(f'{i}: {duration:.3f} sec - {response}')\n",
        "\n",
        "# Also, set use simpler model:\n",
        "px.set_model(('gemini', 'gemini-2.0-flash'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LpQePog8tKI"
      },
      "source": [
        "## 8.1. üê∞ Simple Usage\n",
        "\n",
        "Following example shows how to use simple query cache by only setting cache_path.\n",
        "* All responses returned from cache after first query.\n",
        "* First query takes longer than other queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zucslnKA8rez"
      },
      "outputs": [],
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_3',\n",
        "    cache_path='/content/')\n",
        "\n",
        "test_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP8P_Coo9Vw0"
      },
      "source": [
        "## 8.2. üï∂Ô∏è Unique Response Limit\n",
        "\n",
        "If you want more diverse results, you can set `unique_response_limit` option.\n",
        "* This ensures that it makes at least `unique_response_limit` actual provider queries.\n",
        "* Cache responses returned in round robin fashion.\n",
        "\n",
        "Following examples shows that first three responses are from provider and takes longer than other three responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dexR3qVX9WGC"
      },
      "outputs": [],
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_3',\n",
        "    cache_options=px.CacheOptions(\n",
        "        cache_path='/content/',\n",
        "        unique_response_limit=3\n",
        "    ))\n",
        "\n",
        "test_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSdbvOub-0q4"
      },
      "source": [
        "## 8.3 üèì Skip Cache\n",
        "\n",
        "It is possible to skip cache and ensure actual provider queries are made. Set `usa_cache=False` for `px.generate_text()` method. This ensures for that generate text query, result is always from provider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wjo9zCWs-0cl"
      },
      "outputs": [],
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_3',\n",
        "    cache_path='/content/')\n",
        "\n",
        "response = px.generate_text(\n",
        "    'Hello model!',\n",
        "    extensive_return=True)\n",
        "print(response.response_source)\n",
        "\n",
        "response = px.generate_text(\n",
        "    'Hello model!',\n",
        "    extensive_return=True)\n",
        "print(response.response_source)\n",
        "\n",
        "response = px.generate_text(\n",
        "    'Hello model!',\n",
        "    use_cache=False,\n",
        "    extensive_return=True)\n",
        "print(response.response_source)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UmGm39g_lmX"
      },
      "source": [
        "## 8.4. üõÄ Clear Cache and Override Params\n",
        "\n",
        "It is possible clear cache on connect to ensure session doesn't have any cache.\\\n",
        "Let's also override the `unique_response_limit` on `px.generate_text` observe more control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mq_4epT_lGo"
      },
      "outputs": [],
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_3',\n",
        "    cache_options=px.CacheOptions(\n",
        "        cache_path='/content/',\n",
        "        unique_response_limit=3,\n",
        "        clear_query_cache_on_connect=True,\n",
        "    ))\n",
        "\n",
        "response = px.generate_text(\n",
        "    'Hello model!',\n",
        "    unique_response_limit=1,\n",
        "    extensive_return=True)\n",
        "print(response.response_source)\n",
        "\n",
        "response = px.generate_text(\n",
        "    'Hello model!',\n",
        "    unique_response_limit=1,\n",
        "    extensive_return=True)\n",
        "print(response.response_source)\n",
        "\n",
        "response = px.generate_text(\n",
        "    'Hello model!',\n",
        "    unique_response_limit=1,\n",
        "    extensive_return=True)\n",
        "print(response.response_source)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJYNn3jaAWHf"
      },
      "source": [
        "# 9. üîå ProxDash Connection\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/advanced/proxdash-connection](https://www.proxai.co/proxai-docs/advanced/proxdash-connection)\n",
        "\n",
        "There are number of advantages to use ProxAI with ProxDash. Please, refer advantages on [proxai.co](https://www.proxai.co/) and [resources](https://www.proxai.co/resources/why)\n",
        "\n",
        "In \"6. üß™ Experiment Path\" section, we already used proxdash. Please, check the steps required over there if you skipped that section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RnK1D9CBvxJ"
      },
      "source": [
        "## 9.1. ü¶ä Simple Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5ZhyUhkB1_Q"
      },
      "outputs": [],
      "source": [
        "px.connect()\n",
        "\n",
        "print('By default, this should appear on ProxDash logging history if ProxDash\\n'\n",
        "      'API key set in colab.\\n\\n'\n",
        "      'Please check https://www.proxai.co/dashboard/logging if you can see\\n'\n",
        "      'following query on ProxDash.')\n",
        "response = px.generate_text('This is temp proxdash test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y25QJ-ebCjSV"
      },
      "source": [
        "## 9.2. üìÇ Setting Experiment Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTb-uDgGCjDU"
      },
      "outputs": [],
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_4')\n",
        "\n",
        "print('By default, this should appear on ProxDash logging history and\\n'\n",
        "      'experiments with provided experiment path.\\n\\n'\n",
        "      'Please check https://www.proxai.co/dashboard/experiments if you\\n'\n",
        "      'can see following experiment path:\\n'\n",
        "      '> colab_experiments/advanced_features/run_4\\n\\n'\n",
        "      'If you open this experiment and go logging record tab, you should be\\n'\n",
        "      'able to see following query on ProxDash.')\n",
        "response = px.generate_text('This is temp proxdash experiment path test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TRqEuHgDuSC"
      },
      "source": [
        "## 9.3. üïµÔ∏è Hide Sensitive Content\n",
        "\n",
        "Normally, ProxDash respects the privacy level you set on the API key generation page. However, you still have control over the fields you want to send to ProxDash in case:\n",
        "* API key has the permission but you don‚Äôt want to send some fields to ProxDash anyway.\n",
        "* API key doesn‚Äôt have the permission and you want to ensure to block the fields rather relying on the ProxDash permission level.\n",
        "\n",
        "To do this, you can use the hide_sensitive_content option in the proxdash_options parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHZug30NDtrV"
      },
      "outputs": [],
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_4',\n",
        "    proxdash_options=px.ProxDashOptions(\n",
        "        hide_sensitive_content=True\n",
        "    ))\n",
        "\n",
        "print('Following query record should appear on ProxDash logging history but\\n'\n",
        "      'the content of the prompt and response cannot be visible.\\n'\n",
        "      'Please check the latest logging record on '\n",
        "      'https://www.proxai.co/dashboard/logging to confirm that.')\n",
        "response = px.generate_text(\n",
        "    'This record should appear on ProxDash but the prompt content '\n",
        "    'and the response content from AI provider shouldn\\'t appear on ProxDash.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_XhV8YmEypX"
      },
      "source": [
        "## 9.4. üì£ Print ProxDash Connection Logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXBZ0HpSEuA3"
      },
      "outputs": [],
      "source": [
        "print('You should be able to see ProxDash connection status:')\n",
        "px.reset_state()\n",
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_4',\n",
        "    proxdash_options=px.ProxDashOptions(\n",
        "        stdout=True\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeP2kECOFNGq"
      },
      "source": [
        "## 9.5. üöß Disable ProxDash\n",
        "\n",
        "You can remove `PROXDASH_API_KEY` from environment variables to disable ProxDash but there is simpler way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC6VintAFMnu"
      },
      "outputs": [],
      "source": [
        "px.connect(\n",
        "    proxdash_options=px.ProxDashOptions(\n",
        "        stdout=True,\n",
        "        disable_proxdash=True,\n",
        "    ))\n",
        "\n",
        "print('Following record should not appear on ProxDash logging history:\\n'\n",
        "      'https://www.proxai.co/dashboard/logging')\n",
        "reponse = px.generate_text('This prompt should not appear on proxdash')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dPyCyYEGDSc"
      },
      "source": [
        "# 10. üö¶ Strict feature Test\n",
        "\n",
        "Documentation: [https://www.proxai.co/proxai-docs/advanced/strict-feature-test](https://www.proxai.co/proxai-docs/advanced/strict-feature-test)\n",
        "\n",
        "Not all models are supporting all features. ProxAI tries best effort to handle feature requirements.\n",
        "* If you want to fail if feature is not supported on model, you can set `strict_feature_test=False` on `px.connect()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezIsh3-yGCt_"
      },
      "outputs": [],
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_6',\n",
        "    strict_feature_test=True)\n",
        "\n",
        "try:\n",
        "  px.generate_text(\n",
        "      'Create me a simple poem about birds.',\n",
        "      provider_model=('openai', 'o1'),\n",
        "      temperature=0.3)\n",
        "except Exception as e:\n",
        "  print(\n",
        "      'This query raises error because temperature feature is not supported on '\n",
        "      'OpenAI\\'s o1 model.')\n",
        "  print(f'Error: {e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPD0E0inHq-I"
      },
      "source": [
        "# 11. ‚ö†Ô∏è Suppress Provider Errors\n",
        "\n",
        "Documentation: [https://www.proxai.co/proxai-docs/advanced/suppress-provider-errors](https://www.proxai.co/proxai-docs/advanced/suppress-provider-errors)\n",
        "\n",
        "Instead of raising AI provider errors, you can get `logging_record.response_query.error` and `logging_record.response_query.error_traceback` by setting `suppress_provider_errors=True`.\n",
        "\n",
        "This allows you to continue the processing and check what errors are happening from ProxDash."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m62vMzthHuDG"
      },
      "outputs": [],
      "source": [
        "px.connect(suppress_provider_errors=True)\n",
        "\n",
        "# First, lets pick already failing model:\n",
        "model_status = px.models.list_models(return_all=True)\n",
        "if not model_status.failed_models:\n",
        "  raise ValueError(\n",
        "      \"There is no failed models to try \\'suppress_provider_errors\\' option.\")\n",
        "provider_model = list(model_status.failed_models)[0]\n",
        "\n",
        "# Second, make call with suppress_provider_errors=True:\n",
        "response = px.generate_text(\n",
        "    'If 5 + 20 would be a poem, what life be look like?',\n",
        "    provider_model=provider_model,\n",
        "    extensive_return=True)\n",
        "\n",
        "# No error raised before printing the response or error:\n",
        "print(f'ü§ñ Model: {response.query_record.provider_model}')\n",
        "print(f'üí¨ Response: {response.response_record.response}')\n",
        "print(f'‚ùå Error: {response.response_record.error.strip()}')\n",
        "print(f'‚ö†Ô∏è Error Traceback:\\n{response.response_record.error_traceback.strip()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaDw5jc8I56Y"
      },
      "source": [
        "# 12. üìù Get Current Options\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/advanced/get-current-options](https://www.proxai.co/proxai-docs/advanced/get-current-options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fskpsoZJd6e"
      },
      "source": [
        "## 12.1. üêª Simple Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO2gvLY5JOgw"
      },
      "outputs": [],
      "source": [
        "pprint(asdict(px.get_current_options()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAnq5PdYJoko"
      },
      "source": [
        "## 12.2. ü¶Å Also Another Simple Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGncnCXvJmtk"
      },
      "outputs": [],
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_5',\n",
        "    logging_path='/content/',\n",
        "    logging_options=px.LoggingOptions(\n",
        "        stdout=True,\n",
        "        hide_sensitive_content=True),\n",
        "    cache_path='/content/',\n",
        "    cache_options=px.CacheOptions(\n",
        "        retry_if_error_cached=True,\n",
        "        unique_response_limit=3),\n",
        "    proxdash_options=px.ProxDashOptions(\n",
        "        stdout=True,\n",
        "        hide_sensitive_content=True),\n",
        "    strict_feature_test=True,\n",
        "    allow_multiprocessing=False,\n",
        "    suppress_provider_errors=True)\n",
        "\n",
        "pprint(px.get_current_options(json=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Py-bKdJ28Y"
      },
      "source": [
        "# üçæ Final Thoughts and Next Steps\n",
        "\n",
        "### üéâ Congratulationsüéâ\n",
        "\n",
        "Congratulations on completing the ProxAI Advanced Usage Tutorial!\n",
        "\n",
        "### üöÄ Create, Innovate, and Share!\n",
        "\n",
        "The real magic happens when you start building! We wholeheartedly encourage you to:\n",
        "* ü•á **Develop cool scripts, intricate code examples, and innovative projects** using ProxAI.\n",
        "* üèÜ **Share your creations!** Whether it's with the ProxAI community, on your blog, in forums, or with colleagues, your examples can inspire and help others.\n",
        "\n",
        "### üå± Learn by Doing\n",
        "\n",
        "We've covered a lot of ground, and going through each feature meticulously one by one can sometimes feel daunting. Remember, the most effective way to master ProxAI (or any tool!) is to **dive in and use it directly to solve problems.**\n",
        "\n",
        "### ü§ù Contribute to ProxAI\n",
        "\n",
        "ProxAI is an open-source project, and its strength grows with its community. We warmly invite you to contribute!\n",
        "* **Report bugs or suggest new features:** Your feedback is invaluable. (See [Reporting Bugs & Feature Requests](https://www.proxai.co/resources/community))\n",
        "* **Improve documentation:** Help us make the docs clearer and more comprehensive.\n",
        "* **Write code:** Contribute fixes, new features, or new provider integrations.\n",
        "\n",
        "Check out our **[ProxAI GitHub repository](https://github.com/proxai/proxai)** and the [Contribution Guidelines](https://www.proxai.co/resources/community/guidelines) to get started.\n",
        "\n",
        "### üìû Get in Touch\n",
        "\n",
        "We're here to help and love hearing from our users!\n",
        "\n",
        "* **Discord Community:** For real-time chat, support, and discussions: [discord.gg/QhrDkzMHrP](https://discord.gg/QhrDkzMHrP)\n",
        "* **GitHub Issues:** For technical questions, bug reports, and feature requests: [github.com/proxai/proxai/issues](https://github.com/proxai/proxai/issues)\n",
        "* **Email Contacts:**\n",
        "    * Feedback & Feature Suggestions: [feedback@proxai.co](feedback@proxai.co) ‚≠êÔ∏è\n",
        "    * Development & Contribution Support: [dev@proxai.co](dev@proxai.co)\n",
        "    * General Community Inquiries: [community@proxai.co](community@proxai.co)\n",
        "\n",
        "Thank you for learning with ProxAI. We can't wait to see what you build! üöÄ"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPSC43+WCOxo51kxwanq/k+",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
