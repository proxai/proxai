{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyNKDPrfr+1GrHhWdycZPt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/proxai/proxai/blob/main/docs/tutorial_colabs/ProxAI_Quick_Start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèÉ‚Äç‚ôÇÔ∏è‚Äç‚û°Ô∏è ProxAI Quick Start Tutorial üèÉ‚Äç‚ôÇÔ∏è‚Äç‚û°Ô∏è"
      ],
      "metadata": {
        "id": "pjbMJjs52Muk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üëã Introduction\n",
        "\n",
        "Welcome to the ProxAI Quick Start Tutorial! This notebook will guide you through some of the features of the ProxAI library.\n",
        "\n",
        "In this tutorial, we will cover:\n",
        "1. ‚ö°Ô∏è Setting up ProxAI in Google Colab\n",
        "2. üîã List Available Models\n",
        "3. ü§ñ Generate Text\n",
        "4. üîÆ Set Global Model"
      ],
      "metadata": {
        "id": "Cn4yLuib4RC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. ‚ö°Ô∏è Setup in Google Colab\n",
        "Documentation: [proxai.co/proxai-docs](https://www.proxai.co/proxai-docs)"
      ],
      "metadata": {
        "id": "lhUb737sqIIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. üíª Installation (UPDATE THIS TO PIP)\n",
        "\n",
        "First, let's install the ProxAI library. We'll clone the repository and install it.\n",
        "\n",
        "You can track releases on the [roadmap page](/resources/roadmap) üó∫Ô∏è.\n",
        "\n",
        "**Note:** After running the installation cell, you will likely need to **üîÑ restart the Colab session** using the button that appears in the output of the cell or by going to `Runtime > Restart session`."
      ],
      "metadata": {
        "id": "HuNUNooRqOIm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av5yOW1mqBug",
        "outputId": "5c411ed0-1ea1-49e6-81ca-8ef4fff0fe1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'proxai'...\n",
            "fatal: Remote branch updates not found in upstream origin\n",
            "[Errno 2] No such file or directory: '/content/proxai'\n",
            "/content\n",
            "\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!git clone -b updates https://github.com/proxai/proxai.git\n",
        "%cd /content/proxai\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. üîë API Key Management\n",
        "\n",
        "ProxAI works with various AI providers. You'll need to add your API keys as secrets in Google Colab. This is the safest way to handle them.\n",
        "\n",
        "1.  Click on the **üîë icon (Secrets)** in the left sidebar of Colab.\n",
        "2.  Add your API keys with the names ProxAI expects (e.g., `OPENAI_API_KEY`, `GEMINI_API_KEY`, `PROXDASH_API_KEY`, etc.). Refer to the [Provider Integrations documentation](https://www.proxai.co/proxai-docs/provider-integrations) for the full list of environment keys.\n",
        "\n",
        "Run the following cell to load your API keys from Colab secrets into the environment.\n",
        "\n",
        "<div style=\"background-color: #ffebee; border-left: 6px solid #f44336; padding: 10px; margin-bottom: 15px;\">\n",
        "  <p style=\"margin: 0; font-weight: bold; color: #c62828;\">üö´ Important Security Note:</p>\n",
        "  <p style=\"margin: 0; color: #c62828;\">Never directly add API key values as string variables inside the Colab cells. Even after deletion, they can be retrieved from the Colab history.</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "yItxyArt4sK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from dataclasses import asdict\n",
        "from pprint import pprint\n",
        "\n",
        "API_KEY_LIST = [\n",
        "    'GEMINI_API_KEY',\n",
        "    'OPENAI_API_KEY',\n",
        "    'ANTHROPIC_API_KEY',\n",
        "    # 'XAI_API_KEY',\n",
        "    # 'DEEPSEEK_API_KEY',\n",
        "    # 'MISTRAL_API_KEY',\n",
        "    # 'CO_API_KEY',\n",
        "    # 'DATABRICKS_HOST',\n",
        "    # 'DATABRICKS_TOKEN',\n",
        "    # 'HUGGINGFACE_API_KEY',\n",
        "    'PROXDASH_API_KEY', # For ProxDash connection\n",
        "]\n",
        "\n",
        "print(\"üîê Attempting to load API keys from Colab secrets...\")\n",
        "for api_key_name in API_KEY_LIST:\n",
        "  try:\n",
        "    os.environ[api_key_name] = userdata.get(api_key_name)\n",
        "    print(f\"  ‚úÖ Successfully loaded {api_key_name}\")\n",
        "  except userdata.SecretNotFoundError:\n",
        "    print(f\"  ‚ö†Ô∏è Secret for {api_key_name} not found. Skipping.\")\n",
        "  except Exception as e:\n",
        "    print(f\"  ‚ùå An error occurred while loading {api_key_name}: {e}\")"
      ],
      "metadata": {
        "id": "JUzEyL3PqcN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c35a5e34-064a-4483-c4d6-c1051ba8889c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîê Attempting to load API keys from Colab secrets...\n",
            "  ‚úÖ Successfully loaded GEMINI_API_KEY\n",
            "  ‚úÖ Successfully loaded OPENAI_API_KEY\n",
            "  ‚úÖ Successfully loaded ANTHROPIC_API_KEY\n",
            "  ‚úÖ Successfully loaded PROXDASH_API_KEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. ‚ñ∂Ô∏è Import ProxAI\n",
        "\n",
        "Ready to go!"
      ],
      "metadata": {
        "id": "rYRptpaO5LVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import proxai as px"
      ],
      "metadata": {
        "id": "Dsx0B36W5NFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. üîã List Available Models\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/available-models](https://www.proxai.co/proxai-docs/available-models)"
      ],
      "metadata": {
        "id": "SDHnRjkW5Ph5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. ü™õ Simple Usage\n",
        "\n",
        "Let's list available models in our session! üéâ \\\n",
        "**Note:** This can take for a while for the first run but the results are cached and it will be fast for other runs."
      ],
      "metadata": {
        "id": "jB_CFueU5RUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "provider_models = px.models.list_models()\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model}')"
      ],
      "metadata": {
        "id": "JAwC8WLW5TZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092b97e5-5c35-418d-88d6-147397f4e02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(claude, 3-haiku)\n",
            "(claude, 3-sonnet)\n",
            "(claude, 3.5-sonnet)\n",
            "(claude, 3.5-sonnet-v2)\n",
            "(claude, haiku)\n",
            "(claude, opus)\n",
            "(claude, sonnet)\n",
            "(gemini, gemini-1.5-flash)\n",
            "(gemini, gemini-1.5-flash-8b)\n",
            "(gemini, gemini-1.5-pro)\n",
            "(gemini, gemini-2.0-flash)\n",
            "(gemini, gemini-2.0-flash-lite)\n",
            "(gemini, gemini-2.5-pro-preview-03-25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. üî≠ Different Model Sizes\n",
        "\n",
        "It is possible to filter out models according to ProxAI sizes."
      ],
      "metadata": {
        "id": "9dAVv5Ld5VAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "provider_models = px.models.list_models(model_size='small')\n",
        "print('ü•ö Small models:')\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
        "\n",
        "provider_models = px.models.list_models(model_size='medium')\n",
        "print('üê£ Medium models:')\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
        "\n",
        "provider_models = px.models.list_models(model_size='large')\n",
        "print('üê• Large models:')\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
        "\n",
        "provider_models = px.models.list_models(model_size='largest')\n",
        "print('üêì Largest models of each provider:')\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model.provider:>25} - {provider_model.model}')"
      ],
      "metadata": {
        "id": "Z3zKDEnV5Vfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe5af2b0-745f-4166-b626-d7cafbb2568b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü•ö Small models:\n",
            "                   claude - 3-haiku\n",
            "                   gemini - gemini-1.5-flash\n",
            "                   gemini - gemini-1.5-flash-8b\n",
            "                   gemini - gemini-2.0-flash\n",
            "                   gemini - gemini-2.0-flash-lite\n",
            "üê£ Medium models:\n",
            "                   claude - haiku\n",
            "                   gemini - gemini-1.5-pro\n",
            "üê• Large models:\n",
            "                   claude - opus\n",
            "                   claude - sonnet\n",
            "                   gemini - gemini-2.5-pro-preview-03-25\n",
            "üêì Largest models of each provider:\n",
            "                   claude - sonnet\n",
            "                   gemini - gemini-2.5-pro-preview-03-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. ü§ñ Generate Text\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/generate-text](https://www.proxai.co/proxai-docs/generate-text)"
      ],
      "metadata": {
        "id": "fevDRQa65jOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. üê∂ The Simplest Usage\n",
        "\n",
        "You can directly call `px.generate_text()` without any additional paramters. ProxAI picks default model or fallback models if default model is not working."
      ],
      "metadata": {
        "id": "98ExB5P65jz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = px.generate_text('Hello! Which model are you?')\n",
        "print(response)"
      ],
      "metadata": {
        "id": "BiJcBYYp5oCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b40bbe1-467e-488b-e74a-f027b0acbcff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a large language model, trained by Google.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. ‚úèÔ∏è Setting Provider Model\n",
        "\n",
        "You can specify `provider_model` parameter for `px.generate_text()` as `(provider, model)` tuple of strings.\n",
        "* Check [Provider Integrations documentation](https://www.proxai.co/proxai-docs/provider-integrations) to see all available models."
      ],
      "metadata": {
        "id": "xtJSwNzV5tZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('‚úèÔ∏è Tuple provider_model value:')\n",
        "response = px.generate_text(\n",
        "    'Hello! Which model are you?',\n",
        "    provider_model=('claude', 'haiku'))\n",
        "print(response)"
      ],
      "metadata": {
        "id": "KlqvKC7R5uHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17113150-e4ad-45fc-f07a-9d1283c30d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úèÔ∏è Tuple provider_model value:\n",
            "I want to be direct with you. I'm Claude, an AI created by Anthropic. I aim to be helpful, honest, and harmless.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. üí¨ Additional Features\n",
        "\n",
        "You can use `system`, `messages`, `temperature`, `stop`, and many more on `px.generate_text()`.\n",
        "* Check [Generate Text documentation](https://www.proxai.co/proxai-docs/generate-text) to see all available options."
      ],
      "metadata": {
        "id": "4rOS-GeJ6V8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = px.generate_text(\n",
        "    system=\"Try to impersonate Italian America Chef. Try to use lot's of italian words.\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Hello AI Model!\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Buongiorno!\"},\n",
        "        {\"role\": \"user\", \"content\": \"How are you today?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Molto Bene! How are you amico?\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you recomend me Italian restaurants in NYC?\"}\n",
        "    ],\n",
        "    max_tokens=1000,\n",
        "    temperature=0.7,\n",
        "    stop=[\"\\n\"],\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2dgwvr57HE9",
        "outputId": "314d2d52-cf04-48a6-f818-993dc04a1c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mamma mia, you want the *vero* Italian in New York City? Capisce, I got you covered! Now, NYC, she's got a lot of places, but only a few are *davvero* worth your hard-earned dough. Here's a few of my favorites, depending on what you're lookin' for:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. üîÆ Set Global Model\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/set-global-model](https://www.proxai.co/proxai-docs/set-global-model)\n",
        "\n",
        "You can set global default model by `px.set_model()` instead of using what ProxAI picks for you. All unspecified `px.generate_text()` calls will use this model."
      ],
      "metadata": {
        "id": "ygkY063C73pC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define python method that doesn't specify provider_model\n",
        "def simple_request():\n",
        "  return px.generate_text(\n",
        "      'Hey AI model! This is simple request. Give an answer. Quick!',\n",
        "  ).strip().replace('\\n', ' ')[:80]\n",
        "\n",
        "# We can change default model by px.set_model\n",
        "for provider_model in px.models.list_models():\n",
        "  px.set_model(provider_model)\n",
        "  response = simple_request()\n",
        "  print(f'{provider_model} - {response}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If_C8c5H76Ri",
        "outputId": "eee775f5-b7a5-4708-c4fc-f13b0bbf0324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(claude, 3-haiku) - Okay, I'm ready to provide a quick answer! What's the question?\n",
            "(claude, 3-sonnet) - Okay, here's a quick answer!\n",
            "(claude, 3.5-sonnet) - Sure! I'm ready to help. What's your question? Just let me know what you need an\n",
            "(claude, 3.5-sonnet-v2) - Hi! Ready to help - what's your question?\n",
            "(claude, haiku) - I'm ready. What's your question or request?\n",
            "(claude, opus) - Sure, I'm here to help! What's your question or request? I'll do my best to prov\n",
            "(claude, sonnet) - I'm ready to help! What's your question?\n",
            "(gemini, gemini-1.5-flash) - Okay!\n",
            "(gemini, gemini-1.5-flash-8b) - Okay.  What is your request?\n",
            "(gemini, gemini-1.5-pro) - 42\n",
            "(gemini, gemini-2.0-flash) - Okay! What's your question?\n",
            "(gemini, gemini-2.0-flash-lite) - Okay.\n",
            "(gemini, gemini-2.5-pro-preview-03-25) - Okay!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéä Final Thoughts and Next Steps\n",
        "\n",
        "This is the basic usage of ProxAI and leads you to create enourmous project!\n",
        "\n",
        "If you want to unlock all potential of ProxAI after your toy projects and scripts, please check [üöÄ ProxAI Advanced Usage Tutorial üöÄ](https://www.proxai.co/proxai-docs/google-colab-example) colab.\n",
        "\n",
        "ProxAI offers lots of features that gives you:\n",
        "* 5x development speed\n",
        "* Model picker and benchmarking tools\n",
        "* More control over how queries handled\n",
        "* ProxDash analytics, metrics, debuggers, cost management, and more tools"
      ],
      "metadata": {
        "id": "WXTjoov47-8R"
      }
    }
  ]
}