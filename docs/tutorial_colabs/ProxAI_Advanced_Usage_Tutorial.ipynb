{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyPgmZ9cUzYIMe0vGcVa9/xE",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/proxai/proxai/blob/main/docs/tutorial_colabs/ProxAI_Advanced_Usage_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üöÄ ProxAI Advanced Usage Tutorial üöÄ"
   ],
   "metadata": {
    "id": "3z6UUzIzGzbW"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## üëã Introduction\n\nWelcome to the ProxAI Advanced Usage Tutorial! This notebook will guide you through some of the advanced features of the ProxAI library. ProxAI unifies AI connections across different providers and offers powerful tools to manage your AI interactions.\n\nIn this tutorial, we will cover:\n1. ‚ö°Ô∏è Setting up ProxAI in Google Colab\n2. üîã List Available Models\n3. ü§ñ Generate Text\n4. üåê Web Search\n5. üìã Structured Output with Pydantic\n6. üîÆ Set Global Model\n7. ‚ù§Ô∏è‚Äçü©π Check Health\n8. üß™ Experiment Path\n9. üéûÔ∏è Logs Management\n10. üíæ Cache System - ‚≠êÔ∏è Highly Recommended! ‚≠êÔ∏è\n11. üîå ProxDash Connection\n12. üö¶ Feature Mapping Strategy\n13. ‚ö†Ô∏è Suppress Provider Errors\n14. üìù Get Current Options\n15. üß© Using px.Client\n16. üçæ Final Thoughts and Next Steps\n\nRemember, the most effective way to master ProxAI (or any tool!) is to **dive in and use it directly to solve problems.** This tutorial is useful for people who want to formally dive deep into the powerful features of ProxAI.",
   "metadata": {
    "id": "8NPds7cLG1bi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. ‚ö°Ô∏è Setup in Google Colab\n",
    "Documentation: [proxai.co/proxai-docs](https://www.proxai.co/proxai-docs)"
   ],
   "metadata": {
    "id": "CxLuPcknG-BS"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 1.1. üíª Installation\n\nFirst, let's install the ProxAI library from PyPI.\n\nYou can track releases on the [roadmap page](https://www.proxai.co/resources/roadmap) üó∫Ô∏è.\n\n**Note:** After running the installation cell, you will likely need to **üîÑ restart the Colab session** using the button that appears in the output of the cell or by going to `Runtime > Restart session`.",
   "metadata": {
    "id": "NMzcnmqIG_lx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hRecvegyGfwT",
    "outputId": "97a2b88e-233a-4235-8a3d-1acff0420606"
   },
   "outputs": [],
   "source": "!pip install proxai"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2. üîë API Key Management\n",
    "\n",
    "ProxAI works with various AI providers. You'll need to add your API keys as secrets in Google Colab. This is the safest way to handle them.\n",
    "\n",
    "1.  Click on the **üîë icon (Secrets)** in the left sidebar of Colab.\n",
    "2.  Add your API keys with the names ProxAI expects (e.g., `OPENAI_API_KEY`, `GEMINI_API_KEY`, `PROXDASH_API_KEY`, etc.). Refer to the [Provider Integrations documentation](https://www.proxai.co/proxai-docs/provider-integrations) for the full list of environment keys.\n",
    "\n",
    "Run the following cell to load your API keys from Colab secrets into the environment.\n",
    "\n",
    "<div style=\"background-color: #ffebee; border-left: 6px solid #f44336; padding: 10px; margin-bottom: 15px;\">\n",
    "  <p style=\"margin: 0; font-weight: bold; color: #c62828;\">üö´ Important Security Note:</p>\n",
    "  <p style=\"margin: 0; color: #c62828;\">Never directly add API key values as string variables inside the Colab cells. Even after deletion, they can be retrieved from the Colab history.</p>\n",
    "</div>"
   ],
   "metadata": {
    "id": "UsWwX2J-HPKd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from dataclasses import asdict\n",
    "from pprint import pprint\n",
    "\n",
    "API_KEY_LIST = [\n",
    "    'GEMINI_API_KEY',\n",
    "    'OPENAI_API_KEY',\n",
    "    'ANTHROPIC_API_KEY',\n",
    "    # 'XAI_API_KEY',\n",
    "    # 'DEEPSEEK_API_KEY',\n",
    "    # 'MISTRAL_API_KEY',\n",
    "    # 'CO_API_KEY',\n",
    "    # 'DATABRICKS_HOST',\n",
    "    # 'DATABRICKS_TOKEN',\n",
    "    # 'HUGGINGFACE_API_KEY',\n",
    "    'PROXDASH_API_KEY', # For ProxDash connection\n",
    "]\n",
    "\n",
    "print(\"üîê Attempting to load API keys from Colab secrets...\")\n",
    "for api_key_name in API_KEY_LIST:\n",
    "  try:\n",
    "    os.environ[api_key_name] = userdata.get(api_key_name)\n",
    "    print(f\"  ‚úÖ Successfully loaded {api_key_name}\")\n",
    "  except userdata.SecretNotFoundError:\n",
    "    print(f\"  ‚ö†Ô∏è Secret for {api_key_name} not found. Skipping.\")\n",
    "  except Exception as e:\n",
    "    print(f\"  ‚ùå An error occurred while loading {api_key_name}: {e}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEVpT_-7HdqU",
    "outputId": "bd691b98-a45b-4b4e-aa6f-232c26829f4e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üîê Attempting to load API keys from Colab secrets...\n",
      "  ‚úÖ Successfully loaded GEMINI_API_KEY\n",
      "  ‚úÖ Successfully loaded OPENAI_API_KEY\n",
      "  ‚úÖ Successfully loaded ANTHROPIC_API_KEY\n",
      "  ‚úÖ Successfully loaded PROXDASH_API_KEY\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3. ‚ñ∂Ô∏è Import ProxAI\n",
    "\n",
    "Ready to go!"
   ],
   "metadata": {
    "id": "eZim4oRpKc2J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import proxai as px"
   ],
   "metadata": {
    "id": "X-0Fp1hzKoWq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. üîã List Available Models\n",
    "\n",
    "Documentation: [proxai.co/proxai-docs/available-models](https://www.proxai.co/proxai-docs/available-models)"
   ],
   "metadata": {
    "id": "-ZpFbUStIVbr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1. ü™õ Simple Usage\n",
    "\n",
    "Let's list available models in our session! üéâ \\\n",
    "**Note:** This can take for a while for the first run but the results are cached and it will be fast for other runs."
   ],
   "metadata": {
    "id": "XhysxMyOIxkn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "provider_models = px.models.list_models()\n",
    "for provider_model in provider_models:\n",
    "  print(f'{provider_model.provider:>25} - {provider_model.model}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lu7PIc6rI2-l",
    "outputId": "50954ccc-73e1-4b6a-cbd9-060610c46cb8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                   claude - 3-haiku\n",
      "                   claude - 3-sonnet\n",
      "                   claude - 3.5-sonnet\n",
      "                   claude - 3.5-sonnet-v2\n",
      "                   claude - haiku\n",
      "                   claude - opus\n",
      "                   claude - sonnet\n",
      "                   gemini - gemini-1.5-flash\n",
      "                   gemini - gemini-1.5-flash-8b\n",
      "                   gemini - gemini-1.5-pro\n",
      "                   gemini - gemini-2.0-flash\n",
      "                   gemini - gemini-2.0-flash-lite\n",
      "                   gemini - gemini-2.5-pro-preview-03-25\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2. üî≠ Different Model Sizes\n",
    "\n",
    "It is possible to filter out models according to ProxAI sizes."
   ],
   "metadata": {
    "id": "1t4gdLhiM7fa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "provider_models = px.models.list_models(model_size='small')\n",
    "print('ü•ö Small models:')\n",
    "for provider_model in provider_models:\n",
    "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
    "\n",
    "provider_models = px.models.list_models(model_size='medium')\n",
    "print('üê£ Medium models:')\n",
    "for provider_model in provider_models:\n",
    "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
    "\n",
    "provider_models = px.models.list_models(model_size='large')\n",
    "print('üê• Large models:')\n",
    "for provider_model in provider_models:\n",
    "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
    "\n",
    "provider_models = px.models.list_models(model_size='largest')\n",
    "print('üêì Largest models of each provider:')\n",
    "for provider_model in provider_models:\n",
    "  print(f'{provider_model.provider:>25} - {provider_model.model}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUre0elWNV7M",
    "outputId": "33dda108-aa70-4ecf-c9de-fcf9356dbcf4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ü•ö Small models:\n",
      "                   claude - 3-haiku\n",
      "                   gemini - gemini-1.5-flash\n",
      "                   gemini - gemini-1.5-flash-8b\n",
      "                   gemini - gemini-2.0-flash\n",
      "                   gemini - gemini-2.0-flash-lite\n",
      "üê£ Medium models:\n",
      "                   claude - haiku\n",
      "                   gemini - gemini-1.5-pro\n",
      "üê• Large models:\n",
      "                   claude - opus\n",
      "                   claude - sonnet\n",
      "                   gemini - gemini-2.5-pro-preview-03-25\n",
      "üêì Largest models of each provider:\n",
      "                   claude - sonnet\n",
      "                   gemini - gemini-2.5-pro-preview-03-25\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3. üìÉ Details of Model List\n",
    "\n",
    "You can get detailed metadata of available models:\n",
    "* `working_models`: List of working models\n",
    "* `failed_models`: List of failed models\n",
    "* `provider_queries`: Dictionary of provider queries that each key has `px.types.ProviderModelType` and each value has `px.types.LoggingRecord`. You can check error messages from these queries."
   ],
   "metadata": {
    "id": "er6mKLlWOPFj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_status = px.models.list_models(return_all=True)\n",
    "\n",
    "print(f'‚úÖ Available models: {len(model_status.working_models)}')\n",
    "print(f'‚ùå Failed models: {len(model_status.failed_models)}\\n')\n",
    "errors = []\n",
    "for provider_model, query in model_status.provider_queries.items():\n",
    "  if query.response_record.error:\n",
    "    errors.append((str(provider_model), query.response_record.error))\n",
    "pprint(errors[:3])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJkKozHIOeW_",
    "outputId": "3426651a-5de0-47c8-86ad-cc30e9d5da18"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Available models: 13\n",
      "‚ùå Failed models: 18\n",
      "\n",
      "[('(openai, o1-mini)',\n",
      "  \"Error code: 429 - {'error': {'message': 'You exceeded your current quota, \"\n",
      "  'please check your plan and billing details. For more information on this '\n",
      "  'error, read the docs: '\n",
      "  \"https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': \"\n",
      "  \"'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\"),\n",
      " ('(openai, gpt-4.1-mini)',\n",
      "  \"Error code: 429 - {'error': {'message': 'You exceeded your current quota, \"\n",
      "  'please check your plan and billing details. For more information on this '\n",
      "  'error, read the docs: '\n",
      "  \"https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': \"\n",
      "  \"'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\"),\n",
      " ('(openai, chatgpt-4o-latest)',\n",
      "  \"Error code: 429 - {'error': {'message': 'You exceeded your current quota, \"\n",
      "  'please check your plan and billing details. For more information on this '\n",
      "  'error, read the docs: '\n",
      "  \"https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': \"\n",
      "  \"'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\")]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. ü§ñ Generate Text\n",
    "\n",
    "Documentation: [proxai.co/proxai-docs/generate-text](https://www.proxai.co/proxai-docs/generate-text)"
   ],
   "metadata": {
    "id": "m9jEL82ZRIZJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1. üê∂ The Simplest Usage\n",
    "\n",
    "You can directly call `px.generate_text()` without any additional paramters. ProxAI picks default model or fallback models if default model is not working."
   ],
   "metadata": {
    "id": "u0D2_cwHROl6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "response = px.generate_text('Hello! Which model are you?')\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ylz2eGPpROGT",
    "outputId": "8ba9feb2-0d35-4996-a189-9599d47c0148"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I am a large language model, trained by Google.\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2. ‚úèÔ∏è Setting Provider Model\n",
    "\n",
    "There is two different way of directly setting model on `px.generate_text()`\n",
    "* Tuple with provider and model string\n",
    "* `px.types.ProviderModelType` value"
   ],
   "metadata": {
    "id": "7QQ8rJbrR-yD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print('‚úèÔ∏è Tuple provider_model value:')\n",
    "response = px.generate_text(\n",
    "    'Hello! Which model are you?',\n",
    "    provider_model=('claude', 'haiku'))\n",
    "print(response)\n",
    "\n",
    "print('\\n‚úíÔ∏è px.types.ProviderModelType value:')\n",
    "response = px.generate_text(\n",
    "    'Hello! Which model are you?',\n",
    "    provider_model=px.models.get_model('gemini', 'gemini-1.5-flash'))\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BSRsKI5SS4t",
    "outputId": "80fce17a-ca06-4bbc-a59c-be7492dcafad"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úèÔ∏è Tuple provider_model value:\n",
      "I want to be direct with you. I'm Claude, an AI created by Anthropic. I aim to be helpful, honest, and direct.\n",
      "\n",
      "‚úíÔ∏è px.types.ProviderModelType value:\n",
      "I'm a large language model, trained by Google.\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3. üëë System Prompt\n",
    "\n",
    "You can set system prompt as follows."
   ],
   "metadata": {
    "id": "Z5lOHpxwTK1L"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "response = px.generate_text(\n",
    "    'Hello! Which model are you?',\n",
    "    system=\"You are an helpful assitant that allways answers in Japan.\",\n",
    "    provider_model=('claude', 'haiku'))\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWSxDa9WTMzK",
    "outputId": "d77061a9-7f09-4ba6-e7b4-53159be2813b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "„Åì„Çì„Å´„Å°„ÅØÔºÅÁßÅ„ÅØ„ÄÅOpenAI„ÅåÈñãÁô∫„Åó„ÅüChatGPT„ÅÆAI„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇÂÖ∑‰ΩìÁöÑ„Å´„ÅØ„ÄÅGPT-3.5„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„Å´Âü∫„Å•„ÅÑ„Åü„É¢„Éá„É´„Çí‰ΩøÁî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÊó•Êú¨Ë™û„Åß„ÅÆ‰ºöË©±„Çí„Çµ„Éù„Éº„Éà„Åó„ÄÅ„Åï„Åæ„Åñ„Åæ„Å™Ë≥™Âïè„ÇÑ„Çø„Çπ„ÇØ„Å´„ÅäÁ≠î„Åà„Åß„Åç„Çã„Çà„ÅÜË®≠Ë®à„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ‰Ωï„Åã„ÅäÊâã‰ºù„ÅÑ„Åß„Åç„Çã„Åì„Å®„ÅØ„ÅÇ„Çä„Åæ„Åô„ÅãÔºü\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4. üí¨ Message History\n",
    "\n",
    "It is possible to give the history of conversations to the model via messages. This helps to give context to the model.\n",
    "* (role=user/assistant, content=text) is a common format for AI provider APIs.\n",
    "* Some provider uses different tags but you don't need to worry about it. ProxAI handles these integrations."
   ],
   "metadata": {
    "id": "8OSqVD_RlsJq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "response = px.generate_text(\n",
    "    system=\"No matter what, always answer with single integer.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello AI Model!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"17\"},\n",
    "        {\"role\": \"user\", \"content\": \"How are you today?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"923123\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": \"Can you answer question without any integer?\"}\n",
    "    ],\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEAkI2pslrmJ",
    "outputId": "349ae890-455e-4235-fabb-96d9a9921b94"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5. ‚úã Max Tokens\n",
    "\n",
    "Limit the token size to avoid cost and delay."
   ],
   "metadata": {
    "id": "aaZFlHddmgMr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "response = px.generate_text(\n",
    "    'Can you write all numbers from 1 to 1000?',\n",
    "    max_tokens=20)\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9GqNRx_mtVs",
    "outputId": "b852c2ee-c965-4f00-f6c3-09ea4fee9b34"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Okay, I can't *actually* write them all out here in a way that would be\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.6. üå°Ô∏è Tempreture\n",
    "\n",
    "If you are looking for more creative answers and more randomness, you can set tempreture to lower values than 1."
   ],
   "metadata": {
    "id": "AEUIopUXnQZE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "response = px.generate_text(\n",
    "    'If 5 + 20 would be a poem, what life be look like?',\n",
    "    temperature=0.01)\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJtjOOpPnQOC",
    "outputId": "dedded72-5b7d-4dd6-f747-1552468930a0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "If 5 + 20 were a poem, life would look like this:\n",
      "\n",
      "*   **A sudden burst of joy:** Like the unexpected warmth of the sun on a cold day, life would be punctuated by moments of intense happiness that feel disproportionately large compared to their cause.\n",
      "\n",
      "*   **Growth and expansion:** The poem would represent a journey from a small, contained state (5) to a larger, more expansive one (25). Life would be about learning, evolving, and becoming more than you initially thought possible.\n",
      "\n",
      "*   **Simplicity with depth:** The equation itself is simple, but the result is significant. Life would be about finding profound meaning in everyday experiences and appreciating the beauty of simplicity.\n",
      "\n",
      "*   **A sense of completion:** The poem would have a clear beginning, middle, and end, culminating in a satisfying resolution. Life would be about striving towards goals and finding fulfillment in achieving them.\n",
      "\n",
      "*   **Balance and harmony:** The poem would represent a harmonious combination of two distinct elements, resulting in a balanced whole. Life would be about finding equilibrium between different aspects of your personality and your environment.\n",
      "\n",
      "*   **A positive outlook:** The poem is inherently optimistic, representing addition and growth. Life would be viewed through a lens of hope and possibility, with a belief in the potential for positive change.\n",
      "\n",
      "*   **A reminder of potential:** Even when life feels small or limited (like the number 5), the poem would remind you that there's always the potential for growth and expansion, for adding something more to create something bigger and better.\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.7. ‚òïÔ∏è Extensive Return\n",
    "\n",
    "You can get all details and metadata about query made to the provider by setting `extensive_return` to `True`"
   ],
   "metadata": {
    "id": "9yu0YwPKmfum"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "response = px.generate_text(\n",
    "    'Hello! Which model are you?',\n",
    "    extensive_return=True)\n",
    "pprint(asdict(response))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vzn8NhUnFRg",
    "outputId": "7f5dc592-c523-4cd6-cdbc-fbd43a617fd2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'look_fail_reason': None,\n",
      " 'query_record': {'call_type': <CallType.GENERATE_TEXT: 'GENERATE_TEXT'>,\n",
      "                  'hash_value': None,\n",
      "                  'max_tokens': None,\n",
      "                  'messages': None,\n",
      "                  'prompt': 'Hello! Which model are you?',\n",
      "                  'provider_model': {'model': 'gemini-2.0-flash',\n",
      "                                     'provider': 'gemini',\n",
      "                                     'provider_model_identifier': 'gemini-2.0-flash'},\n",
      "                  'stop': None,\n",
      "                  'system': None,\n",
      "                  'temperature': None,\n",
      "                  'token_count': 7},\n",
      " 'response_record': {'end_utc_date': datetime.datetime(2025, 5, 17, 1, 22, 31, 957015, tzinfo=datetime.timezone.utc),\n",
      "                     'error': None,\n",
      "                     'error_traceback': None,\n",
      "                     'estimated_cost': 4,\n",
      "                     'local_time_offset_minute': -0.0,\n",
      "                     'response': 'I am a large language model, trained by '\n",
      "                                 'Google.\\n',\n",
      "                     'response_time': datetime.timedelta(microseconds=517262),\n",
      "                     'start_utc_date': datetime.datetime(2025, 5, 17, 1, 22, 31, 439788, tzinfo=datetime.timezone.utc),\n",
      "                     'token_count': 12},\n",
      " 'response_source': <ResponseSource.PROVIDER: 'PROVIDER'>}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.8. ‚ö†Ô∏è Suppress Provider Errors\n",
    "\n",
    "If you don't want to raise error when provider fails and just want to continue with error message, you can set `suppress_provider_errors` to `True`.\n",
    "* Check error message on `logging_record.response_record.error`\n",
    "* Check error traceback on `logging_record.response_record.error_traceback`"
   ],
   "metadata": {
    "id": "L_FjIokkntlY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# First, lets pick already failing model:\n",
    "model_status = px.models.list_models(return_all=True)\n",
    "if not model_status.failed_models:\n",
    "  raise ValueError(\n",
    "      \"There is no failed models to try \\'suppress_provider_errors\\' option.\")\n",
    "provider_model = list(model_status.failed_models)[0]\n",
    "\n",
    "# Second, make call with suppress_provider_errors=True:\n",
    "response = px.generate_text(\n",
    "    'If 5 + 20 would be a poem, what life be look like?',\n",
    "    provider_model=provider_model,\n",
    "    suppress_provider_errors=True,\n",
    "    extensive_return=True)\n",
    "\n",
    "# No error raised before printing the response or error:\n",
    "print(f'ü§ñ Model: {response.query_record.provider_model}')\n",
    "print(f'üí¨ Response: {response.response_record.response}')\n",
    "print(f'‚ùå Error: {response.response_record.error.strip()}')\n",
    "print(f'‚ö†Ô∏è Error Traceback:\\n{response.response_record.error_traceback.strip()}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4rTWF9Mmnt4G",
    "outputId": "da1018dd-6e45-43ac-dc80-890ec6773000"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ü§ñ Model: (openai, gpt-4)\n",
      "üí¨ Response: None\n",
      "‚ùå Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚ö†Ô∏è Error Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/proxai/connectors/model_connector.py\", line 451, in generate_text\n",
      "    response = self.generate_text_proc(query_record=updated_query_record)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/proxai/connectors/providers/openai.py\", line 42, in generate_text_proc\n",
      "    completion = create()\n",
      "                 ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1239, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1034, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 4. üåê Web Search\n\nDocumentation: [proxai.co/proxai-docs/generate-text#web-search](https://www.proxai.co/proxai-docs/generate-text#web-search)\n\nProxAI supports web search capabilities for models that have this feature. This is useful for questions about recent events, real-time data, or any information that might be beyond the model's training data.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 4.1. üîç Simple Web Search\n\nEnable web search by setting `web_search=True` in your `px.generate_text()` call. This allows the model to search the web for current information.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Ask about recent events with web search enabled\nresponse = px.generate_text(\n    prompt=\"What are the latest developments in AI as of this week?\",\n    web_search=True\n)\nprint(response)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4.2. üîé Finding Web Search Compatible Models\n\nNot all models support web search. Use the `features` parameter to filter models that support this capability.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Find models that support web search\nweb_search_models = px.models.list_models(features=['web_search'])\n\nprint('üåê Models supporting web search:')\nfor model in web_search_models:\n  print(f'{model.provider:>25} - {model.model}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 5. üìã Structured Output with Pydantic\n\nDocumentation: [proxai.co/proxai-docs/generate-text#structured-output](https://www.proxai.co/proxai-docs/generate-text#structured-output)\n\nProxAI supports structured outputs using Pydantic models. This feature allows you to get type-safe, validated responses from AI models. The model's response will be automatically parsed and validated against your Pydantic schema.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 5.1. üèóÔ∏è Simple Pydantic Example\n\nDefine a Pydantic model and pass it as the `response_format` parameter. The model will return a structured response matching your schema.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pydantic import BaseModel\n\n# Define a Pydantic model for the expected response structure\nclass CityInfo(BaseModel):\n    name: str\n    country: str\n    population: int\n    famous_landmark: str\n\n# Get structured output from the AI model\nresult = px.generate_text(\n    prompt=\"Tell me about Paris, France. Include its population and a famous landmark.\",\n    response_format=CityInfo\n)\n\n# Access the response as a typed object\nprint(f'üèôÔ∏è City: {result.name}')\nprint(f'üåç Country: {result.country}')\nprint(f'üë• Population: {result.population:,}')\nprint(f'üóº Famous Landmark: {result.famous_landmark}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5.2. üîé Finding Pydantic Compatible Models\n\nNot all models support structured output with Pydantic. Use the `features` parameter to filter models that support this capability.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Find models that support Pydantic structured output\npydantic_models = px.models.list_models(features=['response_format::pydantic'])\n\nprint('üìã Models supporting Pydantic structured output:')\nfor model in pydantic_models:\n  print(f'{model.provider:>25} - {model.model}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 6. üîÆ Set Global Model\n\nDocumentation: [proxai.co/proxai-docs/set-global-model](https://www.proxai.co/proxai-docs/set-global-model)\n\nYou can set global default model by `px.set_model()` instead of using what ProxAI picks for you. All unspecified `px.generate_text()` calls will use this model.",
   "metadata": {
    "id": "N8qgLfKrqlQM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's define python method that doesn't specify provider_model\n",
    "def simple_request():\n",
    "  return px.generate_text(\n",
    "      'Hey AI model! This is simple request. Give an answer. Quick!',\n",
    "  ).strip().replace('\\n', ' ')[:80]\n",
    "\n",
    "# We can change default model by px.set_model\n",
    "for provider_model in px.models.list_models():\n",
    "  px.set_model(provider_model)\n",
    "  response = simple_request()\n",
    "  print(f'{provider_model} - {response}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ujKRyXrrtct",
    "outputId": "f3632bc8-8773-41dc-c17b-eda269751591"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(claude, 3-haiku) - I'm happy to provide a quick answer! What would you like me to answer?\n",
      "(claude, 3-sonnet) - Okay, here's a quick answer!\n",
      "(claude, 3.5-sonnet) - Sure, I'm ready to help! What's your question? I'll do my best to give you a qui\n",
      "(claude, 3.5-sonnet-v2) - Happy to help! Just let me know your question and I'll give you a quick answer.\n",
      "(claude, haiku) - I'm ready to help! What is your question or request?\n",
      "(claude, opus) - I'm ready to help! What's your question or request?\n",
      "(claude, sonnet) - I'm ready to help! What's your question? Please go ahead and ask, and I'll provi\n",
      "(gemini, gemini-1.5-flash) - Okay!\n",
      "(gemini, gemini-1.5-flash-8b) - Okay.  What's your request?\n",
      "(gemini, gemini-1.5-pro) - 42\n",
      "(gemini, gemini-2.0-flash) - Okay! What's your question?\n",
      "(gemini, gemini-2.0-flash-lite) - Okay!\n",
      "(gemini, gemini-2.5-pro-preview-03-25) - Okay!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 7. ‚ù§Ô∏è‚Äçü©π Check Health\n\nDocumentation: [proxai.co/proxai-docs/check-health](https://www.proxai.co/proxai-docs/check-health)",
   "metadata": {
    "id": "quLssC0fsozQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 7.1. üê± Simple Check",
   "metadata": {
    "id": "vrNmqGRAvY4w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.check_health()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAVal1wCvmyF",
    "outputId": "9ba1e672-19bd-45c0-f465-4445a62ecb39"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'logging_type': 'INFO',\n",
      " 'message': 'Connected to ProxDash at '\n",
      "            'https://proxainest-production.up.railway.app',\n",
      " 'timestamp': '2025-05-17T01:24:48.452617'}\n",
      "{'logging_type': 'INFO',\n",
      " 'message': 'Connected to ProxDash experiment: '\n",
      "            'connection_health/2025-05-17_01-24-47',\n",
      " 'timestamp': '2025-05-17T01:24:48.452976'}\n",
      "> Starting to test each model...\n",
      "From cache;\n",
      "  0 models are working.\n",
      "  0 models are failed.\n",
      "Running test for 31 models.\n",
      "Testing (gemini, gemini-1.5-flash)...\n",
      "Testing (openai, o1-mini)...\n",
      "Testing (openai, gpt-4.1-mini)...\n",
      "Testing (openai, chatgpt-4o-latest)...\n",
      "Testing (openai, o1-pro)...\n",
      "Testing (openai, gpt-4.1)...\n",
      "Testing (gemini, gemini-1.5-flash-8b)...\n",
      "Testing (claude, 3-haiku)...\n",
      "Testing (openai, o3)...\n",
      "Testing (gemini, gemini-2.0-flash-lite)...\n",
      "Testing (openai, gpt-4.5-preview)...\n",
      "Testing (claude, opus)...\n",
      "Testing (gemini, gemini-2.5-pro-preview-03-25)...\n",
      "Testing (openai, o4-mini)...\n",
      "Testing (claude, 3.5-sonnet)...\n",
      "Testing (openai, gpt-4)...\n",
      "Testing (gemini, gemini-1.5-pro)...\n",
      "Testing (openai, gpt-4o-mini)...\n",
      "Testing (openai, o3-mini)...\n",
      "Testing (gemini, gemini-2.0-flash)...\n",
      "Testing (openai, gpt-4-turbo)...\n",
      "Testing (openai, gpt-4o)...\n",
      "Testing (claude, 3.5-sonnet-v2)...\n",
      "Testing (openai, o1)...\n",
      "Testing (openai, gpt-4.1-nano)...\n",
      "Testing (openai, gpt-4o-mini-search-preview)...\n",
      "Testing (claude, sonnet)...\n",
      "Testing (openai, gpt-3.5-turbo)...\n",
      "Testing (openai, gpt-4o-search-preview)...\n",
      "Testing (claude, 3-sonnet)...\n",
      "Testing (claude, haiku)...\n",
      "After test;\n",
      "  13 models are working.\n",
      "  18 models are failed.\n",
      "Test duration: 56.900796 seconds.\n",
      "> Finished testing.\n",
      "   Registered Providers: 3\n",
      "   Succeeded Models: 13\n",
      "   Failed Models: 18\n",
      "> claude:\n",
      "   [ WORKING |   0.69s ]: 3-haiku\n",
      "   [ WORKING |   0.86s ]: 3-sonnet\n",
      "   [ WORKING |   1.09s ]: 3.5-sonnet\n",
      "   [ WORKING |   3.43s ]: 3.5-sonnet-v2\n",
      "   [ WORKING |   1.97s ]: haiku\n",
      "   [ WORKING |   1.61s ]: opus\n",
      "   [ WORKING |   1.70s ]: sonnet\n",
      "> gemini:\n",
      "   [ WORKING |   0.48s ]: gemini-1.5-flash\n",
      "   [ WORKING |   0.51s ]: gemini-1.5-flash-8b\n",
      "   [ WORKING |   0.68s ]: gemini-1.5-pro\n",
      "   [ WORKING |   0.48s ]: gemini-2.0-flash\n",
      "   [ WORKING |   0.49s ]: gemini-2.0-flash-lite\n",
      "   [ WORKING |   7.93s ]: gemini-2.5-pro-preview-03-25\n",
      "> openai:\n",
      "   [ FAILED  |   1.74s ]: chatgpt-4o-latest\n",
      "   [ FAILED  |   1.77s ]: gpt-3.5-turbo\n",
      "   [ FAILED  |   1.94s ]: gpt-4\n",
      "   [ FAILED  |   1.63s ]: gpt-4-turbo\n",
      "   [ FAILED  |   1.59s ]: gpt-4.1\n",
      "   [ FAILED  |   1.63s ]: gpt-4.1-mini\n",
      "   [ FAILED  |   1.56s ]: gpt-4.1-nano\n",
      "   [ FAILED  |   1.74s ]: gpt-4.5-preview\n",
      "   [ FAILED  |   1.69s ]: gpt-4o\n",
      "   [ FAILED  |   1.53s ]: gpt-4o-mini\n",
      "   [ FAILED  |   1.67s ]: gpt-4o-mini-search-preview\n",
      "   [ FAILED  |   1.50s ]: gpt-4o-search-preview\n",
      "   [ FAILED  |   1.79s ]: o1\n",
      "   [ FAILED  |   1.82s ]: o1-mini\n",
      "   [ FAILED  |   1.51s ]: o1-pro\n",
      "   [ FAILED  |   0.17s ]: o3\n",
      "   [ FAILED  |   1.49s ]: o3-mini\n",
      "   [ FAILED  |   1.63s ]: o4-mini\n",
      "{'logging_type': 'INFO',\n",
      " 'message': 'Results are uploaded to the ProxDash.',\n",
      " 'timestamp': '2025-05-17T01:25:45.355077'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 7.2. üìÉ Extensive Return",
   "metadata": {
    "id": "nPeVoeo82ldg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_status = px.check_health(verbose=False, extensive_return=True)\n",
    "print('--- model_status.working_models:')\n",
    "pprint(model_status.working_models)\n",
    "print('--- model_status.failed_models:')\n",
    "pprint(model_status.failed_models)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vz7GkjRt2q0e",
    "outputId": "1e604aad-74ca-4ba4-a2e7-71c0566c128c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- model_status.working_models:\n",
      "{ProviderModelType(provider=claude, model=3-haiku, provider_model_identifier=claude-3-haiku-20240307),\n",
      " ProviderModelType(provider=claude, model=3-sonnet, provider_model_identifier=claude-3-sonnet-20240229),\n",
      " ProviderModelType(provider=claude, model=3.5-sonnet, provider_model_identifier=claude-3-5-sonnet-20240620),\n",
      " ProviderModelType(provider=claude, model=3.5-sonnet-v2, provider_model_identifier=claude-3-5-sonnet-20241022),\n",
      " ProviderModelType(provider=claude, model=haiku, provider_model_identifier=claude-3-5-haiku-20241022),\n",
      " ProviderModelType(provider=claude, model=opus, provider_model_identifier=claude-3-opus-20240229),\n",
      " ProviderModelType(provider=claude, model=sonnet, provider_model_identifier=claude-3-7-sonnet-20250219),\n",
      " ProviderModelType(provider=gemini, model=gemini-1.5-flash, provider_model_identifier=gemini-1.5-flash),\n",
      " ProviderModelType(provider=gemini, model=gemini-1.5-flash-8b, provider_model_identifier=gemini-1.5-flash-8b),\n",
      " ProviderModelType(provider=gemini, model=gemini-1.5-pro, provider_model_identifier=gemini-1.5-pro),\n",
      " ProviderModelType(provider=gemini, model=gemini-2.0-flash, provider_model_identifier=gemini-2.0-flash),\n",
      " ProviderModelType(provider=gemini, model=gemini-2.0-flash-lite, provider_model_identifier=gemini-2.0-flash-lite),\n",
      " ProviderModelType(provider=gemini, model=gemini-2.5-pro-preview-03-25, provider_model_identifier=gemini-2.5-pro-preview-03-25)}\n",
      "--- model_status.failed_models:\n",
      "{ProviderModelType(provider=openai, model=chatgpt-4o-latest, provider_model_identifier=chatgpt-4o-latest),\n",
      " ProviderModelType(provider=openai, model=gpt-3.5-turbo, provider_model_identifier=gpt-3.5-turbo-0125),\n",
      " ProviderModelType(provider=openai, model=gpt-4, provider_model_identifier=gpt-4-0613),\n",
      " ProviderModelType(provider=openai, model=gpt-4-turbo, provider_model_identifier=gpt-4-turbo-2024-04-09),\n",
      " ProviderModelType(provider=openai, model=gpt-4.1, provider_model_identifier=gpt-4.1-2025-04-14),\n",
      " ProviderModelType(provider=openai, model=gpt-4.1-mini, provider_model_identifier=gpt-4.1-mini-2025-04-14),\n",
      " ProviderModelType(provider=openai, model=gpt-4.1-nano, provider_model_identifier=gpt-4.1-nano-2025-04-14),\n",
      " ProviderModelType(provider=openai, model=gpt-4.5-preview, provider_model_identifier=gpt-4.5-preview-2025-02-27),\n",
      " ProviderModelType(provider=openai, model=gpt-4o, provider_model_identifier=gpt-4o-2024-08-06),\n",
      " ProviderModelType(provider=openai, model=gpt-4o-mini, provider_model_identifier=gpt-4o-mini-2024-07-18),\n",
      " ProviderModelType(provider=openai, model=gpt-4o-mini-search-preview, provider_model_identifier=gpt-4o-mini-search-preview-2025-03-11),\n",
      " ProviderModelType(provider=openai, model=gpt-4o-search-preview, provider_model_identifier=gpt-4o-search-preview-2025-03-11),\n",
      " ProviderModelType(provider=openai, model=o1, provider_model_identifier=o1-2024-12-17),\n",
      " ProviderModelType(provider=openai, model=o1-mini, provider_model_identifier=o1-mini-2024-09-12),\n",
      " ProviderModelType(provider=openai, model=o1-pro, provider_model_identifier=o1-pro-2025-03-19),\n",
      " ProviderModelType(provider=openai, model=o3, provider_model_identifier=o3-2025-04-16),\n",
      " ProviderModelType(provider=openai, model=o3-mini, provider_model_identifier=o3-mini-2025-01-31),\n",
      " ProviderModelType(provider=openai, model=o4-mini, provider_model_identifier=o4-mini-2025-04-16)}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 8. üß™ Experiment Path\n\nDocumentation: [proxai.co/proxai-docs/advanced/experiment-path](https://www.proxai.co/proxai-docs/advanced/experiment-path)\n\nTo able unlock experiments path features, please be sure you complated following steps:\n1. Open ProxAI account from [proxai.co/signup](https://www.proxai.co/signup)\n2. Create ProxAI API key from [proxai.co/dashboard/api-keys](https://www.proxai.co/dashboard/api-keys)\n3. Add new API key to üîë Colab Secrets from left as `PROXDASH_API_KEY`\n4. Run \"1.2. üîë API Key Management\" cell again to load API key",
   "metadata": {
    "id": "ikUGCN2s2uwi"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 8.1. üê≠ Simple Usage with ProxDash",
   "metadata": {
    "id": "FbvLstu23W_q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_1')\n",
    "\n",
    "px.generate_text('Please recommend me a good movie.')\n",
    "\n",
    "print('1 - Open ProxDash experiments page: https://proxai.co/dashboard/experiments')\n",
    "print('2 - Open colab_experiments/advanced_features/run_1 from experiment tree. (Refresh experiments if necessary)')\n",
    "print('3 - Check logging records tab to see movie recommendation query.')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53BN-DcP3Whk",
    "outputId": "42a25d3e-3796-47be-cb21-22fd829b5e16"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 - Open ProxDash experiments page: https://proxai.co/dashboard/experiments\n",
      "2 - Open colab_experiments/advanced_features/run_1 from experiment tree. (Refresh experiments if necessary)\n",
      "3 - Check logging records tab to see movie recommendation query.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 9. üéûÔ∏è Logs Management\n\nDocumentation: [proxai.co/proxai-docs/advanced/logs-management](https://www.proxai.co/proxai-docs/advanced/logs-management)\n\nThis feature can be more useful in local runs rather than Google Colab.",
   "metadata": {
    "id": "YMkcmVfH55EQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 9.1. üêπ Simple Usage",
   "metadata": {
    "id": "fpihvARS6aei"
   }
  },
  {
   "cell_type": "code",
   "source": "import json\n\npx.connect(\n    experiment_path='colab_experiments/advanced_features/run_1',\n    logging_options=px.LoggingOptions(logging_path='/content/'))\n\npx.generate_text('Hello model!')\n\nprint(os.listdir('/content/colab_experiments/advanced_features/run_1'))\n\nwith open('/content/colab_experiments/advanced_features/run_1/provider_queries.log', 'r') as f:\n  for line in f:\n    pprint(json.loads(line))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IFW76TzX6Zs-",
    "outputId": "7d914e56-5584-41eb-f1f3-15879fcd19aa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9.2 üïµÔ∏è Hide Sensitive Content\n\nYou can hide sensitive contents like prompt, response, messages etc. from log files to make proxai more secure.",
   "metadata": {
    "id": "yciz8x4Q6oxY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_2',\n",
    "    logging_options=px.types.LoggingOptions(\n",
    "        logging_path='/content/',\n",
    "        hide_sensitive_content=True,\n",
    "    ))\n",
    "\n",
    "px.generate_text('Hello model!')\n",
    "\n",
    "print(os.listdir('/content/colab_experiments/advanced_features/run_2'))\n",
    "\n",
    "print('Following file should\\'t show the sensitive information:')\n",
    "with open('/content/colab_experiments/advanced_features/run_2/provider_queries.log', 'r') as f:\n",
    "  for line in f:\n",
    "    pprint(json.loads(line))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjFKQGFS6oGp",
    "outputId": "b467d52b-219f-4ef7-cb4a-b123dbf2d78b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['merged.log', 'proxdash.log', 'provider_queries.log']\n",
      "Following file should't show the sensitive information:\n",
      "{'query_record': {'call_type': 'GENERATE_TEXT',\n",
      "                  'prompt': '<sensitive content hidden>',\n",
      "                  'provider_model': {'model': 'gemini-2.5-pro-preview-03-25',\n",
      "                                     'provider': 'gemini',\n",
      "                                     'provider_model_identifier': 'gemini-2.5-pro-preview-03-25'},\n",
      "                  'token_count': '3'},\n",
      " 'response_record': {'end_utc_date': '2025-05-17T01:46:53.656651+00:00',\n",
      "                     'estimated_cost': 43,\n",
      "                     'local_time_offset_minute': -0.0,\n",
      "                     'response': '<sensitive content hidden>',\n",
      "                     'response_time': 4.314925,\n",
      "                     'start_utc_date': '2025-05-17T01:46:49.341753+00:00',\n",
      "                     'token_count': '11'},\n",
      " 'response_source': 'PROVIDER'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 9.3. üñ•Ô∏è Stdout\n\nThere is option for printing all logs to the stdout. It is useful for debugging cases.",
   "metadata": {
    "id": "HiXCZ0X26n0J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_2',\n",
    "    logging_options=px.types.LoggingOptions(\n",
    "        logging_path='/content/',\n",
    "        stdout=True,\n",
    "    ))\n",
    "\n",
    "print('You should be able to see logging record on cell output for following:')\n",
    "response = px.generate_text('Hello model!')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fN_CP6FZ7U0A",
    "outputId": "33edddb9-976c-4587-f724-8a8111bf9c22"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "You should be able to see logging record on cell output for following:\n",
      "{'query_record': {'call_type': 'GENERATE_TEXT',\n",
      "                  'prompt': 'Hello model!',\n",
      "                  'provider_model': {'model': 'gemini-2.5-pro-preview-03-25',\n",
      "                                     'provider': 'gemini',\n",
      "                                     'provider_model_identifier': 'gemini-2.5-pro-preview-03-25'},\n",
      "                  'token_count': '3'},\n",
      " 'response_record': {'end_utc_date': '2025-05-17T01:49:06.320219+00:00',\n",
      "                     'estimated_cost': 43,\n",
      "                     'local_time_offset_minute': -0.0,\n",
      "                     'response': 'Hello there! How can I help you today?',\n",
      "                     'response_time': 5.001231,\n",
      "                     'start_utc_date': '2025-05-17T01:49:01.319028+00:00',\n",
      "                     'token_count': '11'},\n",
      " 'response_source': 'PROVIDER'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 10. üíæ Cache System - ‚≠êÔ∏è Highly Recommended! ‚≠êÔ∏è\n\nDocumentation: [proxai.co/proxai-docs/advanced/cache-system](https://www.proxai.co/proxai-docs/advanced/cache-system)\n\nThis feature is very useful at development stage. Without this feature, experiments can get very painful very easily.\n\nFirst, let's define simple method to get response and duration as in following section.",
   "metadata": {
    "id": "CmkM9-E77odn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "def test_cache():\n",
    "  fixed_int = random.randint(10000, 20000)\n",
    "\n",
    "  def test_prompt():\n",
    "    start = time.time()\n",
    "    response = px.generate_text(\n",
    "        'Can you pick 100 different random positive integers which are less '\n",
    "        f'than {fixed_int}? Can you also explain why you picked these numbers? '\n",
    "        'Please think deeply about your decision and answer accordingly. '\n",
    "        'Start your sentence with random simple poem.',\n",
    "        temperature=0.3)\n",
    "    duration = time.time() - start\n",
    "    response = response.strip().replace('\\n', ' ')[:80]\n",
    "    return response, duration\n",
    "\n",
    "  for i in range(1, 7):\n",
    "    response, duration = test_prompt()\n",
    "    print(f'{i}: {duration:.3f} sec - {response}')\n",
    "\n",
    "# Also, set use simpler model:\n",
    "px.set_model(('gemini', 'gemini-2.0-flash'))"
   ],
   "metadata": {
    "id": "Sexgs-w98Dsv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10.1. üê∞ Simple Usage\n\nFollowing example shows how to use simple query cache by only setting cache_path.\n* All responses returned from cache after first query.\n* First query takes longer than other queries.",
   "metadata": {
    "id": "3LpQePog8tKI"
   }
  },
  {
   "cell_type": "code",
   "source": "px.connect(\n    experiment_path='colab_experiments/advanced_features/run_3',\n    cache_options=px.CacheOptions(cache_path='/content/'))\n\ntest_cache()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zucslnKA8rez",
    "outputId": "f4b55cb0-3d4b-4c50-d167-c7b27ac4df58"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10.2. üï∂Ô∏è Unique Response Limit\n\nIf you want more diverse results, you can set `unique_response_limit` option.\n* This ensures that it makes at least `unique_response_limit` actual provider queries.\n* Cache responses returned in round robin fashion.\n\nFollowing examples shows that first three responses are from provider and takes longer than other three responses.",
   "metadata": {
    "id": "QP8P_Coo9Vw0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_3',\n",
    "    cache_options=px.CacheOptions(\n",
    "        cache_path='/content/',\n",
    "        unique_response_limit=3\n",
    "    ))\n",
    "\n",
    "test_cache()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dexR3qVX9WGC",
    "outputId": "1d70ce72-4eeb-46ca-eb06-161a9dc66af0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1: 9.099 sec - Okay, here's my attempt at picking 100 random positive integers less than 14230,\n",
      "2: 9.430 sec - Okay, here's my attempt at picking 100 random positive integers less than 14230,\n",
      "3: 9.950 sec - Okay, here's my attempt at picking 100 random positive integers less than 14230,\n",
      "4: 0.210 sec - Okay, here's my attempt at picking 100 random positive integers less than 14230,\n",
      "5: 0.244 sec - Okay, here's my attempt at picking 100 random positive integers less than 14230,\n",
      "6: 0.155 sec - Okay, here's my attempt at picking 100 random positive integers less than 14230,\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 10.3 üèì Skip Cache\n\nIt is possible to skip cache and ensure actual provider queries are made. Set `use_cache=False` for `px.generate_text()` method. This ensures for that generate text query, result is always from provider.",
   "metadata": {
    "id": "WSdbvOub-0q4"
   }
  },
  {
   "cell_type": "code",
   "source": "px.connect(\n    experiment_path='colab_experiments/advanced_features/run_3',\n    cache_options=px.CacheOptions(cache_path='/content/'))\n\nresponse = px.generate_text(\n    'Hello model!',\n    extensive_return=True)\nprint(response.response_source)\n\nresponse = px.generate_text(\n    'Hello model!',\n    extensive_return=True)\nprint(response.response_source)\n\nresponse = px.generate_text(\n    'Hello model!',\n    use_cache=False,\n    extensive_return=True)\nprint(response.response_source)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wjo9zCWs-0cl",
    "outputId": "4dcc89c0-820d-4858-b33d-f10d3346b543"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10.4. üõÄ Clear Cache and Override Params\n\nIt is possible clear cache on connect to ensure session doesn't have any cache.\\\nLet's also override the `unique_response_limit` on `px.generate_text` observe more control.",
   "metadata": {
    "id": "5UmGm39g_lmX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_3',\n",
    "    cache_options=px.CacheOptions(\n",
    "        cache_path='/content/',\n",
    "        unique_response_limit=3,\n",
    "        clear_query_cache_on_connect=True,\n",
    "    ))\n",
    "\n",
    "response = px.generate_text(\n",
    "    'Hello model!',\n",
    "    unique_response_limit=1,\n",
    "    extensive_return=True)\n",
    "print(response.response_source)\n",
    "\n",
    "response = px.generate_text(\n",
    "    'Hello model!',\n",
    "    unique_response_limit=1,\n",
    "    extensive_return=True)\n",
    "print(response.response_source)\n",
    "\n",
    "response = px.generate_text(\n",
    "    'Hello model!',\n",
    "    unique_response_limit=1,\n",
    "    extensive_return=True)\n",
    "print(response.response_source)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mq_4epT_lGo",
    "outputId": "8ec97f58-ce8d-4aef-ce28-02730f12ef19"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ResponseSource.PROVIDER\n",
      "ResponseSource.CACHE\n",
      "ResponseSource.CACHE\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 11. üîå ProxDash Connection\n\nDocumentation: [proxai.co/proxai-docs/advanced/proxdash-connection](https://www.proxai.co/proxai-docs/advanced/proxdash-connection)\n\nThere are number of advantages to use ProxAI with ProxDash. Please, refer advantages on [proxai.co](https://www.proxai.co/) and [resources](https://www.proxai.co/resources/why)\n\nIn \"8. üß™ Experiment Path\" section, we already used proxdash. Please, check the steps required over there if you skipped that section.",
   "metadata": {
    "id": "pJYNn3jaAWHf"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 11.1. ü¶ä Simple Usage",
   "metadata": {
    "id": "_RnK1D9CBvxJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect()\n",
    "\n",
    "print('By default, this should appear on ProxDash logging history if ProxDash\\n'\n",
    "      'API key set in colab.\\n\\n'\n",
    "      'Please check https://www.proxai.co/dashboard/logging if you can see\\n'\n",
    "      'following query on ProxDash.')\n",
    "response = px.generate_text('This is temp proxdash test')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5ZhyUhkB1_Q",
    "outputId": "2ba990e4-541b-4e0f-cb54-2b6dc9ad10ca"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "By default, this should appear on ProxDash logging history if ProxDash\n",
      "API key set in colab.\n",
      "\n",
      "Please check https://www.proxai.co/dashboard/logging if you can see\n",
      "following query on ProxDash.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 11.2. üìÇ Setting Experiment Path",
   "metadata": {
    "id": "y25QJ-ebCjSV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_4')\n",
    "\n",
    "print('By default, this should appear on ProxDash logging history and\\n'\n",
    "      'experiments with provided experiment path.\\n\\n'\n",
    "      'Please check https://www.proxai.co/dashboard/experiments if you\\n'\n",
    "      'can see following experiment path:\\n'\n",
    "      '> colab_experiments/advanced_features/run_4\\n\\n'\n",
    "      'If you open this experiment and go logging record tab, you should be\\n'\n",
    "      'able to see following query on ProxDash.')\n",
    "response = px.generate_text('This is temp proxdash experiment path test')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTb-uDgGCjDU",
    "outputId": "cabec28c-a5fc-4efa-8bd7-7d29e3db374d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "By default, this should appear on ProxDash logging history and\n",
      "experiments with provided experiment path.\n",
      "\n",
      "Please check https://www.proxai.co/dashboard/experiments if you\n",
      "can see following experiment path:\n",
      "> colab_experiments/advanced_features/run_4\n",
      "\n",
      "If you open this experiment and go logging record tab, you should be\n",
      "able to see following query on ProxDash.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 11.3. üïµÔ∏è Hide Sensitive Content\n\nNormally, ProxDash respects the privacy level you set on the API key generation page. However, you still have control over the fields you want to send to ProxDash in case:\n* API key has the permission but you don't want to send some fields to ProxDash anyway.\n* API key doesn't have the permission and you want to ensure to block the fields rather relying on the ProxDash permission level.\n\nTo do this, you can use the hide_sensitive_content option in the proxdash_options parameter.",
   "metadata": {
    "id": "0TRqEuHgDuSC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_4',\n",
    "    proxdash_options=px.ProxDashOptions(\n",
    "        hide_sensitive_content=True\n",
    "    ))\n",
    "\n",
    "print('Following query record should appear on ProxDash logging history but\\n'\n",
    "      'the content of the prompt and response cannot be visible.\\n'\n",
    "      'Please check the latest logging record on '\n",
    "      'https://www.proxai.co/dashboard/logging to confirm that.')\n",
    "response = px.generate_text(\n",
    "    'This record should appear on ProxDash but the prompt content '\n",
    "    'and the response content from AI provider shouldn\\'t appear on ProxDash.')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHZug30NDtrV",
    "outputId": "2e86e9f4-7e31-43ac-c058-ab0641d2c965"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Following query record should appear on ProxDash logging history but\n",
      "the content of the prompt and response cannot be visible.\n",
      "Please check the latest logging record on https://www.proxai.co/dashboard/logging to confirm that.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 11.4. üì£ Print ProxDash Connection Logs",
   "metadata": {
    "id": "J_XhV8YmEypX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print('You should be able to see ProxDash connection status:')\n",
    "px.reset_state()\n",
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_4',\n",
    "    proxdash_options=px.ProxDashOptions(\n",
    "        stdout=True\n",
    "    ))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXBZ0HpSEuA3",
    "outputId": "59e1b35d-47e0-40ad-989d-e541adde7dda"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "You should be able to see ProxDash connection status:\n",
      "{'logging_type': 'INFO',\n",
      " 'message': 'Connected to ProxDash at '\n",
      "            'https://proxainest-production.up.railway.app',\n",
      " 'timestamp': '2025-05-17T02:30:52.701415'}\n",
      "{'logging_type': 'INFO',\n",
      " 'message': 'Connected to ProxDash experiment: '\n",
      "            'colab_experiments/advanced_features/run_4',\n",
      " 'timestamp': '2025-05-17T02:30:52.702129'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 11.5. üöß Disable ProxDash\n\nYou can remove `PROXDASH_API_KEY` from environment variables to disable ProxDash but there is simpler way:",
   "metadata": {
    "id": "VeP2kECOFNGq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    proxdash_options=px.ProxDashOptions(\n",
    "        stdout=True,\n",
    "        disable_proxdash=True,\n",
    "    ))\n",
    "\n",
    "print('Following record should not appear on ProxDash logging history:\\n'\n",
    "      'https://www.proxai.co/dashboard/logging')\n",
    "reponse = px.generate_text('This prompt should not appear on proxdash')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "hC6VintAFMnu",
    "outputId": "174da896-748f-4586-b5a7-0307f339f2e4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'logging_type': 'INFO',\n",
      " 'message': 'ProxDash connection disabled.',\n",
      " 'timestamp': '2025-05-17T02:33:18.995521'}\n",
      "Following record should not appear on ProxDash logging history:\n",
      "https://www.proxai.co/dashboard/logging\n",
      "Checking available models, this may take a while...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Understood. This response is solely to acknowledge that the prompt will not appear on proxdash.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 46
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 12. üö¶ Feature Mapping Strategy\n\nDocumentation: [proxai.co/proxai-docs/advanced/feature-mapping-strategy](https://www.proxai.co/proxai-docs/advanced/feature-mapping-strategy)\n\nNot all models support all features. ProxAI provides two strategies for handling feature compatibility:\n\n* **BEST_EFFORT** (default): Attempts to map features even if not fully supported. For example, simulating system messages for models that don't natively support them.\n* **STRICT**: Requires exact feature support. Raises errors if the requested feature is not supported by the model.\n\nSet `feature_mapping_strategy=px.types.FeatureMappingStrategy.STRICT` on `px.connect()` to enforce strict feature requirements.",
   "metadata": {
    "id": "4dPyCyYEGDSc"
   }
  },
  {
   "cell_type": "code",
   "source": "px.connect(\n    experiment_path='colab_experiments/advanced_features/run_6',\n    feature_mapping_strategy=px.types.FeatureMappingStrategy.STRICT)\n\ntry:\n  px.generate_text(\n      'Create me a simple poem about birds.',\n      provider_model=('openai', 'o1'),\n      temperature=0.3)\nexcept Exception as e:\n  print(\n      'This query raises error because temperature feature is not supported on '\n      'OpenAI\\'s o1 model.')\n  print(f'Error: {e}')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezIsh3-yGCt_",
    "outputId": "e4d07282-2740-4335-fe82-df2ecd05aa5a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 13. ‚ö†Ô∏è Suppress Provider Errors\n\nDocumentation: [https://www.proxai.co/proxai-docs/advanced/suppress-provider-errors](https://www.proxai.co/proxai-docs/advanced/suppress-provider-errors)\n\nInstead of raising AI provider errors, you can get `logging_record.response_query.error` and `logging_record.response_query.error_traceback` by setting `suppress_provider_errors=True`.\n\nThis allows you to continue the processing and check what errors are happening from ProxDash.",
   "metadata": {
    "id": "JPD0E0inHq-I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(suppress_provider_errors=True)\n",
    "\n",
    "# First, lets pick already failing model:\n",
    "model_status = px.models.list_models(return_all=True)\n",
    "if not model_status.failed_models:\n",
    "  raise ValueError(\n",
    "      \"There is no failed models to try \\'suppress_provider_errors\\' option.\")\n",
    "provider_model = list(model_status.failed_models)[0]\n",
    "\n",
    "# Second, make call with suppress_provider_errors=True:\n",
    "response = px.generate_text(\n",
    "    'If 5 + 20 would be a poem, what life be look like?',\n",
    "    provider_model=provider_model,\n",
    "    extensive_return=True)\n",
    "\n",
    "# No error raised before printing the response or error:\n",
    "print(f'ü§ñ Model: {response.query_record.provider_model}')\n",
    "print(f'üí¨ Response: {response.response_record.response}')\n",
    "print(f'‚ùå Error: {response.response_record.error.strip()}')\n",
    "print(f'‚ö†Ô∏è Error Traceback:\\n{response.response_record.error_traceback.strip()}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m62vMzthHuDG",
    "outputId": "8690fd83-d993-4193-ac16-174d1598e3d1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ü§ñ Model: (openai, gpt-4)\n",
      "üí¨ Response: None\n",
      "‚ùå Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "‚ö†Ô∏è Error Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/proxai/connectors/model_connector.py\", line 451, in generate_text\n",
      "    response = self.generate_text_proc(query_record=updated_query_record)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/proxai/connectors/providers/openai.py\", line 42, in generate_text_proc\n",
      "    completion = create()\n",
      "                 ^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1239, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1034, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# 14. üìù Get Current Options\n\nDocumentation: [proxai.co/proxai-docs/advanced/get-current-options](https://www.proxai.co/proxai-docs/advanced/get-current-options)",
   "metadata": {
    "id": "DaDw5jc8I56Y"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 14.1. üêª Simple Usage",
   "metadata": {
    "id": "3fskpsoZJd6e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pprint(asdict(px.get_current_options()))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vO2gvLY5JOgw",
    "outputId": "d9d19f13-044b-45e6-a821-9f9a021040a7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'allow_multiprocessing': True,\n",
      " 'cache_options': {'cache_path': None,\n",
      "                   'clear_model_cache_on_connect': False,\n",
      "                   'clear_query_cache_on_connect': False,\n",
      "                   'disable_model_cache': False,\n",
      "                   'model_cache_duration': None,\n",
      "                   'retry_if_error_cached': False,\n",
      "                   'unique_response_limit': 1},\n",
      " 'default_model_cache_path': '/root/.cache/proxai',\n",
      " 'experiment_path': None,\n",
      " 'hidden_run_key': '692410',\n",
      " 'logging_options': {'hide_sensitive_content': False,\n",
      "                     'logging_path': None,\n",
      "                     'stdout': False},\n",
      " 'model_test_timeout': 25,\n",
      " 'proxdash_options': {'api_key': 'hb4ozpw-mar2vd2j-rhir7t2vjwf',\n",
      "                      'base_url': 'https://proxainest-production.up.railway.app',\n",
      "                      'disable_proxdash': False,\n",
      "                      'hide_sensitive_content': False,\n",
      "                      'stdout': False},\n",
      " 'root_logging_path': None,\n",
      " 'run_type': <RunType.PRODUCTION: 'PRODUCTION'>,\n",
      " 'strict_feature_test': False,\n",
      " 'suppress_provider_errors': True}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 14.2. ü¶Å Also Another Simple Example",
   "metadata": {
    "id": "WAnq5PdYJoko"
   }
  },
  {
   "cell_type": "code",
   "source": "px.connect(\n    experiment_path='colab_experiments/advanced_features/run_5',\n    logging_options=px.LoggingOptions(\n        logging_path='/content/',\n        stdout=True,\n        hide_sensitive_content=True),\n    cache_options=px.CacheOptions(\n        cache_path='/content/',\n        retry_if_error_cached=True,\n        unique_response_limit=3),\n    proxdash_options=px.ProxDashOptions(\n        stdout=True,\n        hide_sensitive_content=True),\n    feature_mapping_strategy=px.types.FeatureMappingStrategy.STRICT,\n    allow_multiprocessing=False,\n    suppress_provider_errors=True)\n\npprint(px.get_current_options(json=True))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGncnCXvJmtk",
    "outputId": "b2203a52-e3d7-4e1c-bd91-3edcd0068aa9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 15. üß© Using px.Client\n\nDocumentation: [proxai.co/proxai-docs/advanced/client](https://www.proxai.co/proxai-docs/advanced/client)\n\nWhile the global `px.connect()` and `px.generate_text()` functions work great for most use cases, sometimes you need more control. The `px.Client` class lets you create independent client instances with their own configurations.\n\nThis is useful when you want to:\n* Use different models or settings for different parts of your application\n* Run multiple experiments simultaneously with different configurations\n* Have isolated state between different components",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 15.1. üêß Creating a Client Instance\n\nCreate an independent client with its own configuration. The client supports all the same options as `px.connect()`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create an independent client instance with its own configuration\nclient = px.Client(\n    experiment_path='colab_experiments/client_demo/experiment_1',\n    cache_options=px.CacheOptions(cache_path='/content/'),\n    logging_options=px.LoggingOptions(stdout=True),\n)\n\n# Set a model for this client\nclient.set_model(('gemini', 'gemini-2.0-flash'))\n\n# Generate text using this client\nresponse = client.generate_text(prompt='Hello from px.Client!')\nprint(f'üß© Client response: {response}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 15.2. üêº Using Multiple Clients\n\nYou can use both the global `px` functions and multiple `px.Client` instances simultaneously. Each operates independently with its own configuration.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Configure the global client\npx.connect(experiment_path='colab_experiments/global_experiment')\npx.set_model(('gemini', 'gemini-2.0-flash'))\n\n# Create a separate client with different configuration\nclaude_client = px.Client(\n    experiment_path='colab_experiments/claude_experiment',\n)\nclaude_client.set_model(('claude', 'haiku'))\n\n# Use both clients independently\nprint('üåç Global px client:')\nglobal_response = px.generate_text('What is 2 + 2?')\nprint(f'  Response: {global_response.strip()}')\n\nprint('\\nüß© Claude client:')\nclaude_response = claude_client.generate_text('What is 2 + 2?')\nprint(f'  Response: {claude_response.strip()}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üçæ Final Thoughts and Next Steps\n",
    "\n",
    "### üéâ Congratulationsüéâ\n",
    "\n",
    "Congratulations on completing the ProxAI Advanced Usage Tutorial!\n",
    "\n",
    "### üöÄ Create, Innovate, and Share!\n",
    "\n",
    "The real magic happens when you start building! We wholeheartedly encourage you to:\n",
    "* ü•á **Develop cool scripts, intricate code examples, and innovative projects** using ProxAI.\n",
    "* üèÜ **Share your creations!** Whether it's with the ProxAI community, on your blog, in forums, or with colleagues, your examples can inspire and help others.\n",
    "\n",
    "### üå± Learn by Doing\n",
    "\n",
    "We've covered a lot of ground, and going through each feature meticulously one by one can sometimes feel daunting. Remember, the most effective way to master ProxAI (or any tool!) is to **dive in and use it directly to solve problems.**\n",
    "\n",
    "### ü§ù Contribute to ProxAI\n",
    "\n",
    "ProxAI is an open-source project, and its strength grows with its community. We warmly invite you to contribute!\n",
    "* **Report bugs or suggest new features:** Your feedback is invaluable. (See [Reporting Bugs & Feature Requests](https://www.proxai.co/resources/community))\n",
    "* **Improve documentation:** Help us make the docs clearer and more comprehensive.\n",
    "* **Write code:** Contribute fixes, new features, or new provider integrations.\n",
    "\n",
    "Check out our **[ProxAI GitHub repository](https://github.com/proxai/proxai)** and the [Contribution Guidelines](https://www.proxai.co/resources/community/guidelines) to get started.\n",
    "\n",
    "### üìû Get in Touch\n",
    "\n",
    "We're here to help and love hearing from our users!\n",
    "\n",
    "* **Discord Community:** For real-time chat, support, and discussions: [discord.gg/QhrDkzMHrP](https://discord.gg/QhrDkzMHrP)\n",
    "* **GitHub Issues:** For technical questions, bug reports, and feature requests: [github.com/proxai/proxai/issues](https://github.com/proxai/proxai/issues)\n",
    "* **Email Contacts:**\n",
    "    * Feedback & Feature Suggestions: [feedback@proxai.co](feedback@proxai.co) ‚≠êÔ∏è\n",
    "    * Development & Contribution Support: [dev@proxai.co](dev@proxai.co)\n",
    "    * General Community Inquiries: [community@proxai.co](community@proxai.co)\n",
    "\n",
    "Thank you for learning with ProxAI. We can't wait to see what you build! üöÄ"
   ],
   "metadata": {
    "id": "x2Py-bKdJ28Y"
   }
  }
 ]
}