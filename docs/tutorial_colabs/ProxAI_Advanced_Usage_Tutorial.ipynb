{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyPgmZ9cUzYIMe0vGcVa9/xE",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/proxai/proxai/blob/main/docs/tutorial_colabs/ProxAI_Advanced_Usage_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83d\ude80 ProxAI Advanced Usage Tutorial \ud83d\ude80"
   ],
   "metadata": {
    "id": "3z6UUzIzGzbW"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## \ud83d\udc4b Introduction\n\nWelcome to the ProxAI Advanced Usage Tutorial! This notebook will guide you through some of the advanced features of the ProxAI library. ProxAI unifies AI connections across different providers and offers powerful tools to manage your AI interactions.\n\nIn this tutorial, we will cover:\n1. \u26a1\ufe0f Setting up ProxAI in Google Colab\n2. \ud83d\udd0b List Available Models\n3. \ud83e\udd16 Generate Text\n4. \ud83c\udf10 Web Search\n5. \ud83d\udccb Structured Output with Pydantic\n6. \ud83d\udd2e Set Global Model\n7. \u2764\ufe0f\u200d\ud83e\ude79 Check Health\n8. \ud83e\uddea Experiment Path\n9. \ud83c\udf9e\ufe0f Logs Management\n10. \ud83d\udcbe Cache System - \u2b50\ufe0f Highly Recommended! \u2b50\ufe0f\n11. \ud83d\udd0c ProxDash Connection\n12. \ud83d\udea6 Feature Mapping Strategy\n13. \u26a0\ufe0f Suppress Provider Errors\n14. \ud83d\udcdd Get Current Options\n15. \ud83e\udde9 Using px.Client\n16. \ud83c\udf7e Final Thoughts and Next Steps\n\nRemember, the most effective way to master ProxAI (or any tool!) is to **dive in and use it directly to solve problems.** This tutorial is useful for people who want to formally dive deep into the powerful features of ProxAI.",
   "metadata": {
    "id": "8NPds7cLG1bi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. \u26a1\ufe0f Setup in Google Colab\n",
    "Documentation: [proxai.co/proxai-docs](https://www.proxai.co/proxai-docs)"
   ],
   "metadata": {
    "id": "CxLuPcknG-BS"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 1.1. \ud83d\udcbb Installation\n\nFirst, let's install the ProxAI library from PyPI.\n\nYou can track releases on the [roadmap page](https://www.proxai.co/resources/roadmap) \ud83d\uddfa\ufe0f.\n\n**Note:** After running the installation cell, you will likely need to **\ud83d\udd04 restart the Colab session** using the button that appears in the output of the cell or by going to `Runtime > Restart session`.",
   "metadata": {
    "id": "NMzcnmqIG_lx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hRecvegyGfwT",
    "outputId": "97a2b88e-233a-4235-8a3d-1acff0420606"
   },
   "outputs": [],
   "source": "!pip install proxai"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2. \ud83d\udd11 API Key Management\n",
    "\n",
    "ProxAI works with various AI providers. You'll need to add your API keys as secrets in Google Colab. This is the safest way to handle them.\n",
    "\n",
    "1.  Click on the **\ud83d\udd11 icon (Secrets)** in the left sidebar of Colab.\n",
    "2.  Add your API keys with the names ProxAI expects (e.g., `OPENAI_API_KEY`, `GEMINI_API_KEY`, `PROXDASH_API_KEY`, etc.). Refer to the [Provider Integrations documentation](https://www.proxai.co/proxai-docs/provider-integrations) for the full list of environment keys.\n",
    "\n",
    "Run the following cell to load your API keys from Colab secrets into the environment.\n",
    "\n",
    "<div style=\"background-color: #ffebee; border-left: 6px solid #f44336; padding: 10px; margin-bottom: 15px;\">\n",
    "  <p style=\"margin: 0; font-weight: bold; color: #c62828;\">\ud83d\udeab Important Security Note:</p>\n",
    "  <p style=\"margin: 0; color: #c62828;\">Never directly add API key values as string variables inside the Colab cells. Even after deletion, they can be retrieved from the Colab history.</p>\n",
    "</div>"
   ],
   "metadata": {
    "id": "UsWwX2J-HPKd"
   }
  },
  {
   "cell_type": "code",
   "source": "import os\nfrom google.colab import userdata\nfrom dataclasses import asdict\nfrom pprint import pprint\n\nAPI_KEY_LIST = [\n    'ANTHROPIC_API_KEY',\n    'CO_API_KEY',\n    'DATABRICKS_TOKEN',\n    'DATABRICKS_HOST',\n    'DEEPSEEK_API_KEY',\n    'GEMINI_API_KEY',\n    'XAI_API_KEY',\n    'HF_TOKEN',\n    'MISTRAL_API_KEY',\n    'OPENAI_API_KEY',\n    'PROXDASH_API_KEY',\n]\n\nprint(\"\ud83d\udd10 Attempting to load API keys from Colab secrets...\")\nfor api_key_name in API_KEY_LIST:\n  try:\n    os.environ[api_key_name] = userdata.get(api_key_name)\n    print(f\"  \u2705 Successfully loaded {api_key_name}\")\n  except userdata.SecretNotFoundError:\n    print(f\"  \u26a0\ufe0f Secret for {api_key_name} not found. Skipping.\")\n  except Exception as e:\n    print(f\"  \u274c An error occurred while loading {api_key_name}: {e}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEVpT_-7HdqU",
    "outputId": "bd691b98-a45b-4b4e-aa6f-232c26829f4e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3. \u25b6\ufe0f Import ProxAI\n",
    "\n",
    "Ready to go!"
   ],
   "metadata": {
    "id": "eZim4oRpKc2J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import proxai as px"
   ],
   "metadata": {
    "id": "X-0Fp1hzKoWq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. \ud83d\udd0b List Available Models\n",
    "\n",
    "Documentation: [proxai.co/proxai-docs/available-models](https://www.proxai.co/proxai-docs/available-models)"
   ],
   "metadata": {
    "id": "-ZpFbUStIVbr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1. \ud83e\ude9b Simple Usage\n",
    "\n",
    "Let's list available models in our session! \ud83c\udf89 \\\n",
    "**Note:** This can take for a while for the first run but the results are cached and it will be fast for other runs."
   ],
   "metadata": {
    "id": "XhysxMyOIxkn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "provider_models = px.models.list_models()\n",
    "for provider_model in provider_models:\n",
    "  print(f'{provider_model.provider:>25} - {provider_model.model}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lu7PIc6rI2-l",
    "outputId": "50954ccc-73e1-4b6a-cbd9-060610c46cb8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2. \ud83d\udd2d Different Model Sizes\n",
    "\n",
    "It is possible to filter out models according to ProxAI sizes."
   ],
   "metadata": {
    "id": "1t4gdLhiM7fa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "provider_models = px.models.list_models(model_size='small')\n",
    "print('\ud83e\udd5a Small models:')\n",
    "for provider_model in provider_models:\n",
    "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
    "\n",
    "provider_models = px.models.list_models(model_size='medium')\n",
    "print('\ud83d\udc23 Medium models:')\n",
    "for provider_model in provider_models:\n",
    "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
    "\n",
    "provider_models = px.models.list_models(model_size='large')\n",
    "print('\ud83d\udc25 Large models:')\n",
    "for provider_model in provider_models:\n",
    "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
    "\n",
    "provider_models = px.models.list_models(model_size='largest')\n",
    "print('\ud83d\udc13 Largest models of each provider:')\n",
    "for provider_model in provider_models:\n",
    "  print(f'{provider_model.provider:>25} - {provider_model.model}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUre0elWNV7M",
    "outputId": "33dda108-aa70-4ecf-c9de-fcf9356dbcf4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3. \ud83d\udcc3 Details of Model List\n",
    "\n",
    "You can get detailed metadata of available models:\n",
    "* `working_models`: List of working models\n",
    "* `failed_models`: List of failed models\n",
    "* `provider_queries`: Dictionary of provider queries that each key has `px.types.ProviderModelType` and each value has `px.types.LoggingRecord`. You can check error messages from these queries."
   ],
   "metadata": {
    "id": "er6mKLlWOPFj"
   }
  },
  {
   "cell_type": "code",
   "source": "model_status = px.models.list_working_models(model_size='small', return_all=True)\n\nprint(f'\u2705 Available models: {len(model_status.working_models)}')\nprint(f'\u274c Failed models: {len(model_status.failed_models)}\\n')\nerrors = []\nfor provider_model, query in model_status.provider_queries.items():\n  if query.response_record.error:\n    errors.append((str(provider_model), query.response_record.error))\npprint(errors[:3])",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJkKozHIOeW_",
    "outputId": "3426651a-5de0-47c8-86ad-cc30e9d5da18"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. \ud83e\udd16 Generate Text\n",
    "\n",
    "Documentation: [proxai.co/proxai-docs/generate-text](https://www.proxai.co/proxai-docs/generate-text)"
   ],
   "metadata": {
    "id": "m9jEL82ZRIZJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1. \ud83d\udc36 The Simplest Usage\n",
    "\n",
    "You can directly call `px.generate_text()` without any additional paramters. ProxAI picks default model or fallback models if default model is not working."
   ],
   "metadata": {
    "id": "u0D2_cwHROl6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "response = px.generate_text('Hello! Which model are you?')\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ylz2eGPpROGT",
    "outputId": "8ba9feb2-0d35-4996-a189-9599d47c0148"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2. \u270f\ufe0f Setting Provider Model\n",
    "\n",
    "There is two different way of directly setting model on `px.generate_text()`\n",
    "* Tuple with provider and model string\n",
    "* `px.types.ProviderModelType` value"
   ],
   "metadata": {
    "id": "7QQ8rJbrR-yD"
   }
  },
  {
   "cell_type": "code",
   "source": "print('\u270f\ufe0f Tuple provider_model value:')\nresponse = px.generate_text(\n    'Hello! Which model are you?',\n    provider_model=('claude', 'haiku-4.5'))\nprint(response)\n\nprint('\\n\u2712\ufe0f px.types.ProviderModelType value:')\nresponse = px.generate_text(\n    'Hello! Which model are you?',\n    provider_model=px.models.get_model('gemini', 'gemini-3-flash'))\nprint(response)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BSRsKI5SS4t",
    "outputId": "80fce17a-ca06-4bbc-a59c-be7492dcafad"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3. \ud83d\udc51 System Prompt\n",
    "\n",
    "You can set system prompt as follows."
   ],
   "metadata": {
    "id": "Z5lOHpxwTK1L"
   }
  },
  {
   "cell_type": "code",
   "source": "response = px.generate_text(\n    'Hello! Which model are you?',\n    system=\"You are an helpful assitant that allways answers in Japan.\",\n    provider_model=('claude', 'haiku-4.5'))\nprint(response)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWSxDa9WTMzK",
    "outputId": "d77061a9-7f09-4ba6-e7b4-53159be2813b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4. \ud83d\udcac Message History\n",
    "\n",
    "It is possible to give the history of conversations to the model via messages. This helps to give context to the model.\n",
    "* (role=user/assistant, content=text) is a common format for AI provider APIs.\n",
    "* Some provider uses different tags but you don't need to worry about it. ProxAI handles these integrations."
   ],
   "metadata": {
    "id": "8OSqVD_RlsJq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "response = px.generate_text(\n",
    "    system=\"No matter what, always answer with single integer.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello AI Model!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"17\"},\n",
    "        {\"role\": \"user\", \"content\": \"How are you today?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"923123\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": \"Can you answer question without any integer?\"}\n",
    "    ],\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEAkI2pslrmJ",
    "outputId": "349ae890-455e-4235-fabb-96d9a9921b94"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5. \u270b Max Tokens\n",
    "\n",
    "Limit the token size to avoid cost and delay."
   ],
   "metadata": {
    "id": "aaZFlHddmgMr"
   }
  },
  {
   "cell_type": "code",
   "source": "response = px.generate_text(\n    'Can you write all numbers from 1 to 1000?',\n    max_tokens=100)\nprint(response)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9GqNRx_mtVs",
    "outputId": "b852c2ee-c965-4f00-f6c3-09ea4fee9b34"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.6. \ud83c\udf21\ufe0f Tempreture\n",
    "\n",
    "If you are looking for more creative answers and more randomness, you can set tempreture to lower values than 1."
   ],
   "metadata": {
    "id": "AEUIopUXnQZE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "response = px.generate_text(\n",
    "    'If 5 + 20 would be a poem, what life be look like?',\n",
    "    temperature=0.01)\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJtjOOpPnQOC",
    "outputId": "dedded72-5b7d-4dd6-f747-1552468930a0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.7. \u2615\ufe0f Extensive Return\n",
    "\n",
    "You can get all details and metadata about query made to the provider by setting `extensive_return` to `True`"
   ],
   "metadata": {
    "id": "9yu0YwPKmfum"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "response = px.generate_text(\n",
    "    'Hello! Which model are you?',\n",
    "    extensive_return=True)\n",
    "pprint(asdict(response))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vzn8NhUnFRg",
    "outputId": "7f5dc592-c523-4cd6-cdbc-fbd43a617fd2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.8. \u26a0\ufe0f Suppress Provider Errors\n",
    "\n",
    "If you don't want to raise error when provider fails and just want to continue with error message, you can set `suppress_provider_errors` to `True`.\n",
    "* Check error message on `logging_record.response_record.error`\n",
    "* Check error traceback on `logging_record.response_record.error_traceback`"
   ],
   "metadata": {
    "id": "L_FjIokkntlY"
   }
  },
  {
   "cell_type": "code",
   "source": "# Use the mock_failing_provider to demonstrate suppress_provider_errors\nresponse = px.generate_text(\n    'If 5 + 20 would be a poem, what life be look like?',\n    provider_model=('mock_failing_provider', 'mock_failing_model'),\n    suppress_provider_errors=True,\n    extensive_return=True)\n\n# No error raised before printing the response or error:\nprint(f'\ud83e\udd16 Model: {response.query_record.provider_model}')\nprint(f'\ud83d\udcac Response: {response.response_record.response}')\nprint(f'\u274c Error: {response.response_record.error.strip()}')\nprint(f'\u26a0\ufe0f Error Traceback:\\n{response.response_record.error_traceback.strip()}')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4rTWF9Mmnt4G",
    "outputId": "da1018dd-6e45-43ac-dc80-890ec6773000"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 4. \ud83c\udf10 Web Search\n\nDocumentation: [proxai.co/proxai-docs/generate-text#web-search](https://www.proxai.co/proxai-docs/generate-text#web-search)\n\nProxAI supports web search capabilities for models that have this feature. This is useful for questions about recent events, real-time data, or any information that might be beyond the model's training data.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 4.1. \ud83d\udd0d Simple Web Search\n\nEnable web search by setting `web_search=True` in your `px.generate_text()` call. This allows the model to search the web for current information.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Ask about recent events with web search enabled\nresponse = px.generate_text(\n    prompt=\"What are the latest developments in AI as of this week?\",\n    web_search=True\n)\nprint(response)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4.2. \ud83d\udd0e Finding Web Search Compatible Models\n\nNot all models support web search. Use the `features` parameter to filter models that support this capability.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Find models that support web search\nweb_search_models = px.models.list_models(features=['web_search'])\n\nprint('\ud83c\udf10 Models supporting web search:')\nfor model in web_search_models:\n  print(f'{model.provider:>25} - {model.model}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 5. \ud83d\udccb Structured Output with Pydantic\n\nDocumentation: [proxai.co/proxai-docs/generate-text#structured-output](https://www.proxai.co/proxai-docs/generate-text#structured-output)\n\nProxAI supports structured outputs using Pydantic models. This feature allows you to get type-safe, validated responses from AI models. The model's response will be automatically parsed and validated against your Pydantic schema.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 5.1. \ud83c\udfd7\ufe0f Simple Pydantic Example\n\nDefine a Pydantic model and pass it as the `response_format` parameter. The model will return a structured response matching your schema.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pydantic import BaseModel\n\n# Define a Pydantic model for the expected response structure\nclass CityInfo(BaseModel):\n    name: str\n    country: str\n    population: int\n    famous_landmark: str\n\n# Get structured output from the AI model\nresult = px.generate_text(\n    prompt=\"Tell me about Paris, France. Include its population and a famous landmark.\",\n    response_format=CityInfo\n)\n\n# Access the response as a typed object\nprint(f'\ud83c\udfd9\ufe0f City: {result.name}')\nprint(f'\ud83c\udf0d Country: {result.country}')\nprint(f'\ud83d\udc65 Population: {result.population:,}')\nprint(f'\ud83d\uddfc Famous Landmark: {result.famous_landmark}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5.2. \ud83d\udd0e Finding Pydantic Compatible Models\n\nNot all models support structured output with Pydantic. Use the `features` parameter to filter models that support this capability.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Find models that support Pydantic structured output\npydantic_models = px.models.list_models(features=['response_format::pydantic'])\n\nprint('\ud83d\udccb Models supporting Pydantic structured output:')\nfor model in pydantic_models:\n  print(f'{model.provider:>25} - {model.model}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 6. \ud83d\udd2e Set Global Model\n\nDocumentation: [proxai.co/proxai-docs/set-global-model](https://www.proxai.co/proxai-docs/set-global-model)\n\nYou can set global default model by `px.set_model()` instead of using what ProxAI picks for you. All unspecified `px.generate_text()` calls will use this model.",
   "metadata": {
    "id": "N8qgLfKrqlQM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's define python method that doesn't specify provider_model\n",
    "def simple_request():\n",
    "  return px.generate_text(\n",
    "      'Hey AI model! This is simple request. Give an answer. Quick!',\n",
    "  ).strip().replace('\\n', ' ')[:80]\n",
    "\n",
    "# We can change default model by px.set_model\n",
    "for provider_model in px.models.list_models():\n",
    "  px.set_model(provider_model)\n",
    "  response = simple_request()\n",
    "  print(f'{provider_model} - {response}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ujKRyXrrtct",
    "outputId": "f3632bc8-8773-41dc-c17b-eda269751591"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 7. \u2764\ufe0f\u200d\ud83e\ude79 Check Health\n\nDocumentation: [proxai.co/proxai-docs/check-health](https://www.proxai.co/proxai-docs/check-health)",
   "metadata": {
    "id": "quLssC0fsozQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 7.1. \ud83d\udc31 Simple Check",
   "metadata": {
    "id": "vrNmqGRAvY4w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.check_health()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAVal1wCvmyF",
    "outputId": "9ba1e672-19bd-45c0-f465-4445a62ecb39"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7.2. \ud83d\udcc3 Extensive Return",
   "metadata": {
    "id": "nPeVoeo82ldg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_status = px.check_health(verbose=False, extensive_return=True)\n",
    "print('--- model_status.working_models:')\n",
    "pprint(model_status.working_models)\n",
    "print('--- model_status.failed_models:')\n",
    "pprint(model_status.failed_models)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vz7GkjRt2q0e",
    "outputId": "1e604aad-74ca-4ba4-a2e7-71c0566c128c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 8. \ud83e\uddea Experiment Path\n\nDocumentation: [proxai.co/proxai-docs/advanced/experiment-path](https://www.proxai.co/proxai-docs/advanced/experiment-path)\n\nTo able unlock experiments path features, please be sure you complated following steps:\n1. Open ProxAI account from [proxai.co/signup](https://www.proxai.co/signup)\n2. Create ProxAI API key from [proxai.co/dashboard/api-keys](https://www.proxai.co/dashboard/api-keys)\n3. Add new API key to \ud83d\udd11 Colab Secrets from left as `PROXDASH_API_KEY`\n4. Run \"1.2. \ud83d\udd11 API Key Management\" cell again to load API key",
   "metadata": {
    "id": "ikUGCN2s2uwi"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 8.1. \ud83d\udc2d Simple Usage with ProxDash",
   "metadata": {
    "id": "FbvLstu23W_q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_1')\n",
    "\n",
    "px.generate_text('Please recommend me a good movie.')\n",
    "\n",
    "print('1 - Open ProxDash experiments page: https://proxai.co/dashboard/experiments')\n",
    "print('2 - Open colab_experiments/advanced_features/run_1 from experiment tree. (Refresh experiments if necessary)')\n",
    "print('3 - Check logging records tab to see movie recommendation query.')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53BN-DcP3Whk",
    "outputId": "42a25d3e-3796-47be-cb21-22fd829b5e16"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 9. \ud83c\udf9e\ufe0f Logs Management\n\nDocumentation: [proxai.co/proxai-docs/advanced/logs-management](https://www.proxai.co/proxai-docs/advanced/logs-management)\n\nThis feature can be more useful in local runs rather than Google Colab.",
   "metadata": {
    "id": "YMkcmVfH55EQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 9.1. \ud83d\udc39 Simple Usage",
   "metadata": {
    "id": "fpihvARS6aei"
   }
  },
  {
   "cell_type": "code",
   "source": "import json\n\npx.connect(\n    experiment_path='colab_experiments/advanced_features/run_1',\n    logging_options=px.LoggingOptions(logging_path='/content/'))\n\npx.generate_text('Hello model!')\n\nprint(os.listdir('/content/colab_experiments/advanced_features/run_1'))\n\nwith open('/content/colab_experiments/advanced_features/run_1/provider_queries.log', 'r') as f:\n  for line in f:\n    pprint(json.loads(line))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IFW76TzX6Zs-",
    "outputId": "7d914e56-5584-41eb-f1f3-15879fcd19aa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9.2 \ud83d\udd75\ufe0f Hide Sensitive Content\n\nYou can hide sensitive contents like prompt, response, messages etc. from log files to make proxai more secure.",
   "metadata": {
    "id": "yciz8x4Q6oxY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_2',\n",
    "    logging_options=px.types.LoggingOptions(\n",
    "        logging_path='/content/',\n",
    "        hide_sensitive_content=True,\n",
    "    ))\n",
    "\n",
    "px.generate_text('Hello model!')\n",
    "\n",
    "print(os.listdir('/content/colab_experiments/advanced_features/run_2'))\n",
    "\n",
    "print('Following file should\\'t show the sensitive information:')\n",
    "with open('/content/colab_experiments/advanced_features/run_2/provider_queries.log', 'r') as f:\n",
    "  for line in f:\n",
    "    pprint(json.loads(line))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjFKQGFS6oGp",
    "outputId": "b467d52b-219f-4ef7-cb4a-b123dbf2d78b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9.3. \ud83d\udda5\ufe0f Stdout\n\nThere is option for printing all logs to the stdout. It is useful for debugging cases.",
   "metadata": {
    "id": "HiXCZ0X26n0J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_2',\n",
    "    logging_options=px.types.LoggingOptions(\n",
    "        logging_path='/content/',\n",
    "        stdout=True,\n",
    "    ))\n",
    "\n",
    "print('You should be able to see logging record on cell output for following:')\n",
    "response = px.generate_text('Hello model!')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fN_CP6FZ7U0A",
    "outputId": "33edddb9-976c-4587-f724-8a8111bf9c22"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 10. \ud83d\udcbe Cache System - \u2b50\ufe0f Highly Recommended! \u2b50\ufe0f\n\nDocumentation: [proxai.co/proxai-docs/advanced/cache-system](https://www.proxai.co/proxai-docs/advanced/cache-system)\n\nThis feature is very useful at development stage. Without this feature, experiments can get very painful very easily.\n\nFirst, let's define simple method to get response and duration as in following section.",
   "metadata": {
    "id": "CmkM9-E77odn"
   }
  },
  {
   "cell_type": "code",
   "source": "import random\nimport time\n\ndef test_cache():\n  fixed_int = random.randint(10000, 20000)\n\n  def test_prompt():\n    start = time.time()\n    response = px.generate_text(\n        'Can you pick 100 different random positive integers which are less '\n        f'than {fixed_int}? Can you also explain why you picked these numbers? '\n        'Please think deeply about your decision and answer accordingly. '\n        'Start your sentence with random simple poem.',\n        temperature=0.3)\n    duration = time.time() - start\n    response = response.strip().replace('\\n', ' ')[:80]\n    return response, duration\n\n  for i in range(1, 7):\n    response, duration = test_prompt()\n    print(f'{i}: {duration:.3f} sec - {response}')\n\n# Also, set use simpler model:\npx.set_model(('gemini', 'gemini-3-flash'))",
   "metadata": {
    "id": "Sexgs-w98Dsv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10.1. \ud83d\udc30 Simple Usage\n\nFollowing example shows how to use simple query cache by only setting cache_path.\n* All responses returned from cache after first query.\n* First query takes longer than other queries.",
   "metadata": {
    "id": "3LpQePog8tKI"
   }
  },
  {
   "cell_type": "code",
   "source": "px.connect(\n    experiment_path='colab_experiments/advanced_features/run_3',\n    cache_options=px.CacheOptions(cache_path='/content/'))\n\ntest_cache()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zucslnKA8rez",
    "outputId": "f4b55cb0-3d4b-4c50-d167-c7b27ac4df58"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10.2. \ud83d\udd76\ufe0f Unique Response Limit\n\nIf you want more diverse results, you can set `unique_response_limit` option.\n* This ensures that it makes at least `unique_response_limit` actual provider queries.\n* Cache responses returned in round robin fashion.\n\nFollowing examples shows that first three responses are from provider and takes longer than other three responses.",
   "metadata": {
    "id": "QP8P_Coo9Vw0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_3',\n",
    "    cache_options=px.CacheOptions(\n",
    "        cache_path='/content/',\n",
    "        unique_response_limit=3\n",
    "    ))\n",
    "\n",
    "test_cache()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dexR3qVX9WGC",
    "outputId": "1d70ce72-4eeb-46ca-eb06-161a9dc66af0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10.3 \ud83c\udfd3 Skip Cache\n\nIt is possible to skip cache and ensure actual provider queries are made. Set `use_cache=False` for `px.generate_text()` method. This ensures for that generate text query, result is always from provider.",
   "metadata": {
    "id": "WSdbvOub-0q4"
   }
  },
  {
   "cell_type": "code",
   "source": "px.connect(\n    experiment_path='colab_experiments/advanced_features/run_3',\n    cache_options=px.CacheOptions(cache_path='/content/'))\n\nresponse = px.generate_text(\n    'Hello model!',\n    extensive_return=True)\nprint(response.response_source)\n\nresponse = px.generate_text(\n    'Hello model!',\n    extensive_return=True)\nprint(response.response_source)\n\nresponse = px.generate_text(\n    'Hello model!',\n    use_cache=False,\n    extensive_return=True)\nprint(response.response_source)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wjo9zCWs-0cl",
    "outputId": "4dcc89c0-820d-4858-b33d-f10d3346b543"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10.4. \ud83d\udec0 Clear Cache and Override Params\n\nIt is possible clear cache on connect to ensure session doesn't have any cache.\\\nLet's also override the `unique_response_limit` on `px.generate_text` observe more control.",
   "metadata": {
    "id": "5UmGm39g_lmX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_3',\n",
    "    cache_options=px.CacheOptions(\n",
    "        cache_path='/content/',\n",
    "        unique_response_limit=3,\n",
    "        clear_query_cache_on_connect=True,\n",
    "    ))\n",
    "\n",
    "response = px.generate_text(\n",
    "    'Hello model!',\n",
    "    unique_response_limit=1,\n",
    "    extensive_return=True)\n",
    "print(response.response_source)\n",
    "\n",
    "response = px.generate_text(\n",
    "    'Hello model!',\n",
    "    unique_response_limit=1,\n",
    "    extensive_return=True)\n",
    "print(response.response_source)\n",
    "\n",
    "response = px.generate_text(\n",
    "    'Hello model!',\n",
    "    unique_response_limit=1,\n",
    "    extensive_return=True)\n",
    "print(response.response_source)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mq_4epT_lGo",
    "outputId": "8ec97f58-ce8d-4aef-ce28-02730f12ef19"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 11. \ud83d\udd0c ProxDash Connection\n\nDocumentation: [proxai.co/proxai-docs/advanced/proxdash-connection](https://www.proxai.co/proxai-docs/advanced/proxdash-connection)\n\nThere are number of advantages to use ProxAI with ProxDash. Please, refer advantages on [proxai.co](https://www.proxai.co/) and [resources](https://www.proxai.co/resources/why)\n\nIn \"8. \ud83e\uddea Experiment Path\" section, we already used proxdash. Please, check the steps required over there if you skipped that section.",
   "metadata": {
    "id": "pJYNn3jaAWHf"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 11.1. \ud83e\udd8a Simple Usage",
   "metadata": {
    "id": "_RnK1D9CBvxJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect()\n",
    "\n",
    "print('By default, this should appear on ProxDash logging history if ProxDash\\n'\n",
    "      'API key set in colab.\\n\\n'\n",
    "      'Please check https://www.proxai.co/dashboard/logging if you can see\\n'\n",
    "      'following query on ProxDash.')\n",
    "response = px.generate_text('This is temp proxdash test')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5ZhyUhkB1_Q",
    "outputId": "2ba990e4-541b-4e0f-cb54-2b6dc9ad10ca"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11.2. \ud83d\udcc2 Setting Experiment Path",
   "metadata": {
    "id": "y25QJ-ebCjSV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_4')\n",
    "\n",
    "print('By default, this should appear on ProxDash logging history and\\n'\n",
    "      'experiments with provided experiment path.\\n\\n'\n",
    "      'Please check https://www.proxai.co/dashboard/experiments if you\\n'\n",
    "      'can see following experiment path:\\n'\n",
    "      '> colab_experiments/advanced_features/run_4\\n\\n'\n",
    "      'If you open this experiment and go logging record tab, you should be\\n'\n",
    "      'able to see following query on ProxDash.')\n",
    "response = px.generate_text('This is temp proxdash experiment path test')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTb-uDgGCjDU",
    "outputId": "cabec28c-a5fc-4efa-8bd7-7d29e3db374d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11.3. \ud83d\udd75\ufe0f Hide Sensitive Content\n\nNormally, ProxDash respects the privacy level you set on the API key generation page. However, you still have control over the fields you want to send to ProxDash in case:\n* API key has the permission but you don't want to send some fields to ProxDash anyway.\n* API key doesn't have the permission and you want to ensure to block the fields rather relying on the ProxDash permission level.\n\nTo do this, you can use the hide_sensitive_content option in the proxdash_options parameter.",
   "metadata": {
    "id": "0TRqEuHgDuSC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_4',\n",
    "    proxdash_options=px.ProxDashOptions(\n",
    "        hide_sensitive_content=True\n",
    "    ))\n",
    "\n",
    "print('Following query record should appear on ProxDash logging history but\\n'\n",
    "      'the content of the prompt and response cannot be visible.\\n'\n",
    "      'Please check the latest logging record on '\n",
    "      'https://www.proxai.co/dashboard/logging to confirm that.')\n",
    "response = px.generate_text(\n",
    "    'This record should appear on ProxDash but the prompt content '\n",
    "    'and the response content from AI provider shouldn\\'t appear on ProxDash.')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHZug30NDtrV",
    "outputId": "2e86e9f4-7e31-43ac-c058-ab0641d2c965"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11.4. \ud83d\udce3 Print ProxDash Connection Logs",
   "metadata": {
    "id": "J_XhV8YmEypX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print('You should be able to see ProxDash connection status:')\n",
    "px.reset_state()\n",
    "px.connect(\n",
    "    experiment_path='colab_experiments/advanced_features/run_4',\n",
    "    proxdash_options=px.ProxDashOptions(\n",
    "        stdout=True\n",
    "    ))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXBZ0HpSEuA3",
    "outputId": "59e1b35d-47e0-40ad-989d-e541adde7dda"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11.5. \ud83d\udea7 Disable ProxDash\n\nYou can remove `PROXDASH_API_KEY` from environment variables to disable ProxDash but there is simpler way:",
   "metadata": {
    "id": "VeP2kECOFNGq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "px.connect(\n",
    "    proxdash_options=px.ProxDashOptions(\n",
    "        stdout=True,\n",
    "        disable_proxdash=True,\n",
    "    ))\n",
    "\n",
    "print('Following record should not appear on ProxDash logging history:\\n'\n",
    "      'https://www.proxai.co/dashboard/logging')\n",
    "reponse = px.generate_text('This prompt should not appear on proxdash')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "hC6VintAFMnu",
    "outputId": "174da896-748f-4586-b5a7-0307f339f2e4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 12. \ud83d\udea6 Feature Mapping Strategy\n\nDocumentation: [proxai.co/proxai-docs/advanced/feature-mapping-strategy](https://www.proxai.co/proxai-docs/advanced/feature-mapping-strategy)\n\nNot all models support all features. ProxAI provides two strategies for handling feature compatibility:\n\n* **BEST_EFFORT** (default): Attempts to map features even if not fully supported. For example, simulating system messages for models that don't natively support them.\n* **STRICT**: Requires exact feature support. Raises errors if the requested feature is not supported by the model.\n\nSet `feature_mapping_strategy=px.types.FeatureMappingStrategy.STRICT` on `px.connect()` to enforce strict feature requirements.",
   "metadata": {
    "id": "4dPyCyYEGDSc"
   }
  },
  {
   "cell_type": "code",
   "source": "px.connect(\n    experiment_path='colab_experiments/advanced_features/run_6',\n    feature_mapping_strategy=px.types.FeatureMappingStrategy.STRICT)\n\ntry:\n  px.generate_text(\n      'Create me a simple poem about birds.',\n      provider_model=('openai', 'o1'),\n      temperature=0.3)\nexcept Exception as e:\n  print(\n      'This query raises error because temperature feature is not supported on '\n      'OpenAI\\'s o1 model.')\n  print(f'Error: {e}')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezIsh3-yGCt_",
    "outputId": "e4d07282-2740-4335-fe82-df2ecd05aa5a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 13. \u26a0\ufe0f Suppress Provider Errors\n\nDocumentation: [https://www.proxai.co/proxai-docs/advanced/suppress-provider-errors](https://www.proxai.co/proxai-docs/advanced/suppress-provider-errors)\n\nInstead of raising AI provider errors, you can get `logging_record.response_query.error` and `logging_record.response_query.error_traceback` by setting `suppress_provider_errors=True`.\n\nThis allows you to continue the processing and check what errors are happening from ProxDash.",
   "metadata": {
    "id": "JPD0E0inHq-I"
   }
  },
  {
   "cell_type": "code",
   "source": "px.connect(suppress_provider_errors=True)\n\n# Use the mock_failing_provider to demonstrate suppress_provider_errors\nresponse = px.generate_text(\n    'If 5 + 20 would be a poem, what life be look like?',\n    provider_model=('mock_failing_provider', 'mock_failing_model'),\n    extensive_return=True)\n\n# No error raised before printing the response or error:\nprint(f'\ud83e\udd16 Model: {response.query_record.provider_model}')\nprint(f'\ud83d\udcac Response: {response.response_record.response}')\nprint(f'\u274c Error: {response.response_record.error.strip()}')\nprint(f'\u26a0\ufe0f Error Traceback:\\n{response.response_record.error_traceback.strip()}')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m62vMzthHuDG",
    "outputId": "8690fd83-d993-4193-ac16-174d1598e3d1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 14. \ud83d\udcdd Get Current Options\n\nDocumentation: [proxai.co/proxai-docs/advanced/get-current-options](https://www.proxai.co/proxai-docs/advanced/get-current-options)",
   "metadata": {
    "id": "DaDw5jc8I56Y"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 14.1. \ud83d\udc3b Simple Usage",
   "metadata": {
    "id": "3fskpsoZJd6e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pprint(asdict(px.get_current_options()))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vO2gvLY5JOgw",
    "outputId": "d9d19f13-044b-45e6-a821-9f9a021040a7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 14.2. \ud83e\udd81 Also Another Simple Example",
   "metadata": {
    "id": "WAnq5PdYJoko"
   }
  },
  {
   "cell_type": "code",
   "source": "px.connect(\n    experiment_path='colab_experiments/advanced_features/run_5',\n    logging_options=px.LoggingOptions(\n        logging_path='/content/',\n        stdout=True,\n        hide_sensitive_content=True),\n    cache_options=px.CacheOptions(\n        cache_path='/content/',\n        retry_if_error_cached=True,\n        unique_response_limit=3),\n    proxdash_options=px.ProxDashOptions(\n        stdout=True,\n        hide_sensitive_content=True),\n    feature_mapping_strategy=px.types.FeatureMappingStrategy.STRICT,\n    allow_multiprocessing=False,\n    suppress_provider_errors=True)\n\npprint(px.get_current_options(json=True))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGncnCXvJmtk",
    "outputId": "b2203a52-e3d7-4e1c-bd91-3edcd0068aa9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 15. \ud83e\udde9 Using px.Client\n\nDocumentation: [proxai.co/proxai-docs/advanced/client](https://www.proxai.co/proxai-docs/advanced/client)\n\nWhile the global `px.connect()` and `px.generate_text()` functions work great for most use cases, sometimes you need more control. The `px.Client` class lets you create independent client instances with their own configurations.\n\nThis is useful when you want to:\n* Use different models or settings for different parts of your application\n* Run multiple experiments simultaneously with different configurations\n* Have isolated state between different components",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 15.1. \ud83d\udc27 Creating a Client Instance\n\nCreate an independent client with its own configuration. The client supports all the same options as `px.connect()`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create an independent client instance with its own configuration\nclient = px.Client(\n    experiment_path='colab_experiments/client_demo/experiment_1',\n    cache_options=px.CacheOptions(cache_path='/content/'),\n    logging_options=px.LoggingOptions(stdout=True),\n)\n\n# Set a model for this client\nclient.set_model(('gemini', 'gemini-3-flash'))\n\n# Generate text using this client\nresponse = client.generate_text(prompt='Hello from px.Client!')\nprint(f'\ud83e\udde9 Client response: {response}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 15.2. \ud83d\udc3c Using Multiple Clients\n\nYou can use both the global `px` functions and multiple `px.Client` instances simultaneously. Each operates independently with its own configuration.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Configure the global client\npx.connect(experiment_path='colab_experiments/global_experiment')\npx.set_model(('gemini', 'gemini-3-flash'))\n\n# Create a separate client with different configuration\nclaude_client = px.Client(\n    experiment_path='colab_experiments/claude_experiment',\n)\nclaude_client.set_model(('claude', 'haiku-4.5'))\n\n# Use both clients independently\nprint('\ud83c\udf0d Global px client:')\nglobal_response = px.generate_text('What is 2 + 2?', extensive_return=True)\nprint(f'  Model: {global_response.query_record.provider_model}')\nprint(f'  Response: {global_response.response_record.response.strip()}')\n\nprint('\\n\ud83e\udde9 Claude client:')\nclaude_response = claude_client.generate_text('What is 2 + 2?', extensive_return=True)\nprint(f'  Model: {claude_response.query_record.provider_model}')\nprint(f'  Response: {claude_response.response_record.response.strip()}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83c\udf7e Final Thoughts and Next Steps\n",
    "\n",
    "### \ud83c\udf89 Congratulations\ud83c\udf89\n",
    "\n",
    "Congratulations on completing the ProxAI Advanced Usage Tutorial!\n",
    "\n",
    "### \ud83d\ude80 Create, Innovate, and Share!\n",
    "\n",
    "The real magic happens when you start building! We wholeheartedly encourage you to:\n",
    "* \ud83e\udd47 **Develop cool scripts, intricate code examples, and innovative projects** using ProxAI.\n",
    "* \ud83c\udfc6 **Share your creations!** Whether it's with the ProxAI community, on your blog, in forums, or with colleagues, your examples can inspire and help others.\n",
    "\n",
    "### \ud83c\udf31 Learn by Doing\n",
    "\n",
    "We've covered a lot of ground, and going through each feature meticulously one by one can sometimes feel daunting. Remember, the most effective way to master ProxAI (or any tool!) is to **dive in and use it directly to solve problems.**\n",
    "\n",
    "### \ud83e\udd1d Contribute to ProxAI\n",
    "\n",
    "ProxAI is an open-source project, and its strength grows with its community. We warmly invite you to contribute!\n",
    "* **Report bugs or suggest new features:** Your feedback is invaluable. (See [Reporting Bugs & Feature Requests](https://www.proxai.co/resources/community))\n",
    "* **Improve documentation:** Help us make the docs clearer and more comprehensive.\n",
    "* **Write code:** Contribute fixes, new features, or new provider integrations.\n",
    "\n",
    "Check out our **[ProxAI GitHub repository](https://github.com/proxai/proxai)** and the [Contribution Guidelines](https://www.proxai.co/resources/community/guidelines) to get started.\n",
    "\n",
    "### \ud83d\udcde Get in Touch\n",
    "\n",
    "We're here to help and love hearing from our users!\n",
    "\n",
    "* **Discord Community:** For real-time chat, support, and discussions: [discord.gg/QhrDkzMHrP](https://discord.gg/QhrDkzMHrP)\n",
    "* **GitHub Issues:** For technical questions, bug reports, and feature requests: [github.com/proxai/proxai/issues](https://github.com/proxai/proxai/issues)\n",
    "* **Email Contacts:**\n",
    "    * Feedback & Feature Suggestions: [feedback@proxai.co](feedback@proxai.co) \u2b50\ufe0f\n",
    "    * Development & Contribution Support: [dev@proxai.co](dev@proxai.co)\n",
    "    * General Community Inquiries: [community@proxai.co](community@proxai.co)\n",
    "\n",
    "Thank you for learning with ProxAI. We can't wait to see what you build! \ud83d\ude80"
   ],
   "metadata": {
    "id": "x2Py-bKdJ28Y"
   }
  }
 ]
}