{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPgmZ9cUzYIMe0vGcVa9/xE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/proxai/proxai/blob/main/docs/tutorial_colabs/ProxAI_Advanced_Usage_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸš€ ProxAI Advanced Usage Tutorial ğŸš€"
      ],
      "metadata": {
        "id": "3z6UUzIzGzbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ‘‹ Introduction\n",
        "\n",
        "Welcome to the ProxAI Advanced Usage Tutorial! This notebook will guide you through some of the advanced features of the ProxAI library. ProxAI unifies AI connections across different providers and offers powerful tools to manage your AI interactions.\n",
        "\n",
        "In this tutorial, we will cover:\n",
        "1. âš¡ï¸ Setting up ProxAI in Google Colab\n",
        "2. ğŸ”‹ List Available Models\n",
        "3. ğŸ¤– Generate Text\n",
        "4. ğŸ”® Set Global Model\n",
        "5. ğŸ§ª Experiment Path\n",
        "6. ğŸï¸ Logs Management\n",
        "7. ğŸ’¾ Cache System - â­ï¸ Highly Recommended! â­ï¸\n",
        "8. ğŸ”Œ ProxDash Connection\n",
        "9. ğŸš¦ Strict Feature Test\n",
        "10. âš ï¸ Suppress Provider Errors\n",
        "11. ğŸ“ Get Current Options\n",
        "12. ğŸ¾ Final Thoughts and Next Steps\n",
        "\n",
        "Remember, the most effective way to master ProxAI (or any tool!) is to **dive in and use it directly to solve problems.** This tutorial is useful for people who want to formally dive deep into the powerful features of ProxAI."
      ],
      "metadata": {
        "id": "8NPds7cLG1bi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. âš¡ï¸ Setup in Google Colab\n",
        "Documentation: [proxai.co/proxai-docs](https://www.proxai.co/proxai-docs)"
      ],
      "metadata": {
        "id": "CxLuPcknG-BS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. ğŸ’» Installation (UPDATE THIS TO PIP)\n",
        "\n",
        "First, let's install the ProxAI library. We'll clone the repository and install it.\n",
        "\n",
        "You can track releases on the [roadmap page](/resources/roadmap) ğŸ—ºï¸.\n",
        "\n",
        "**Note:** After running the installation cell, you will likely need to **ğŸ”„ restart the Colab session** using the button that appears in the output of the cell or by going to `Runtime > Restart session`."
      ],
      "metadata": {
        "id": "NMzcnmqIG_lx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hRecvegyGfwT",
        "outputId": "97a2b88e-233a-4235-8a3d-1acff0420606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'proxai'...\n",
            "remote: Enumerating objects: 1965, done.\u001b[K\n",
            "remote: Counting objects: 100% (155/155), done.\u001b[K\n",
            "remote: Compressing objects: 100% (147/147), done.\u001b[K\n",
            "remote: Total 1965 (delta 6), reused 9 (delta 6), pack-reused 1810 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1965/1965), 2.30 MiB | 6.48 MiB/s, done.\n",
            "Resolving deltas: 100% (1272/1272), done.\n",
            "/content/proxai\n",
            "Processing /content/proxai\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting anthropic<0.22.0,>=0.21.3 (from proxai==0.1.0)\n",
            "  Downloading anthropic-0.21.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting cohere<6.0.0,>=5.1.7 (from proxai==0.1.0)\n",
            "  Downloading cohere-5.15.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting databricks-sdk<0.36.0,>=0.35.0 (from databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0)\n",
            "  Downloading databricks_sdk-0.35.0-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting google-genai<1.9.0 (from proxai==0.1.0)\n",
            "  Downloading google_genai-1.8.0-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting mistralai<0.2.0,>=0.1.8 (from proxai==0.1.0)\n",
            "  Downloading mistralai-0.1.8-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.14.3 in /usr/local/lib/python3.11/dist-packages (from proxai==0.1.0) (1.78.1)\n",
            "Collecting urllib3<2.0.0 (from proxai==0.1.0)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (4.13.2)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0.0,>=5.1.7->proxai==0.1.0)\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere<6.0.0,>=5.1.7->proxai==0.1.0)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.1.7->proxai==0.1.0) (2.33.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.1.7->proxai==0.1.0) (2.32.3)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0.0,>=5.1.7->proxai==0.1.0)\n",
            "  Downloading types_requests-2.32.0.20250515-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<0.36.0,>=0.35.0->databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0) (2.38.0)\n",
            "Collecting langchain-openai (from databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0)\n",
            "  Downloading langchain_openai-0.3.17-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<1.9.0->proxai==0.1.0) (15.0.1)\n",
            "INFO: pip is looking at multiple versions of mistralai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-genai<1.9.0 (from proxai==0.1.0)\n",
            "  Downloading google_genai-1.7.0-py3-none-any.whl.metadata (32 kB)\n",
            "  Downloading google_genai-1.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting websockets<15.0dev,>=13.0 (from google-genai<1.9.0->proxai==0.1.0)\n",
            "  Downloading websockets-14.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting google-genai<1.9.0 (from proxai==0.1.0)\n",
            "  Downloading google_genai-1.4.0-py3-none-any.whl.metadata (29 kB)\n",
            "  Downloading google_genai-1.3.0-py3-none-any.whl.metadata (28 kB)\n",
            "  Downloading google_genai-1.2.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from anthropic<0.22.0,>=0.21.3->proxai==0.1.0)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.10 in /usr/local/lib/python3.11/dist-packages (from mistralai<0.2.0,>=0.1.8->proxai==0.1.0) (3.10.18)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (0.16.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.14.3->proxai==0.1.0) (0.9.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.14.3->proxai==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<0.36.0,>=0.35.0->databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<0.36.0,>=0.35.0->databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<0.36.0,>=0.35.0->databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0) (4.9.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.1.7->proxai==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.0->anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (0.31.2)\n",
            "INFO: pip is looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0.0,>=5.1.7->proxai==0.1.0)\n",
            "  Downloading types_requests-2.32.0.20250328-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading types_requests-2.32.0.20250306-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading types_requests-2.32.0.20250301-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading types_requests-2.32.0.20240907-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading types_requests-2.32.0.20240905-py3-none-any.whl.metadata (1.9 kB)\n",
            "INFO: pip is still looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading types_requests-2.32.0.20240712-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading types_requests-2.32.0.20240622-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading types_requests-2.32.0.20240602-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading types_requests-2.32.0.20240523-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading types_requests-2.32.0.20240521-py3-none-any.whl.metadata (1.8 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading types_requests-2.31.0.20240406-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading types_requests-2.31.0.20240403-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading types_requests-2.31.0.20240402-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading types_requests-2.31.0.20240311-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading types_requests-2.31.0.20240310-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading types_requests-2.31.0.20240218-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading types_requests-2.31.0.20240125-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading types_requests-2.31.0.20240106-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading types_requests-2.31.0.20231231-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading types_requests-2.31.0.10-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading types_requests-2.31.0.9-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading types_requests-2.31.0.8-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading types_requests-2.31.0.7-py3-none-any.whl.metadata (1.4 kB)\n",
            "  Downloading types_requests-2.31.0.6-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting types-urllib3 (from types-requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.1.7->proxai==0.1.0)\n",
            "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain-openai->databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0) (0.3.59)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai->databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.22.0,>=0.21.3->proxai==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-openai->databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0) (0.3.42)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-openai->databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-openai->databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0) (1.33)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<0.36.0,>=0.35.0->databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0) (0.6.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai->databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-openai->databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.59->langchain-openai->databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.59->langchain-openai->databricks-sdk[openai]<0.36.0,>=0.35.0->proxai==0.1.0) (0.23.0)\n",
            "Downloading anthropic-0.21.3-py3-none-any.whl (851 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m851.6/851.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.15.0-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading databricks_sdk-0.35.0-py3-none-any.whl (568 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m568.4/568.4 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_genai-1.2.0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistralai-0.1.8-py3-none-any.whl (15 kB)\n",
            "Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n",
            "Downloading websockets-14.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m169.9/169.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.17-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: proxai\n",
            "  Building wheel for proxai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for proxai: filename=proxai-0.1.0-py3-none-any.whl size=63676 sha256=545755d414a99b180aaffc2e5b70bb89778467942c4b4afd169dee961862d9ff\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b601uxzo/wheels/ea/5d/54/f4ca4311fe2da8ee7210762d372ddc9a65537f650a07607438\n",
            "Successfully built proxai\n",
            "Installing collected packages: types-urllib3, websockets, urllib3, types-requests, httpx-sse, fastavro, httpx, mistralai, google-genai, databricks-sdk, cohere, anthropic, langchain-openai, proxai\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: google-genai\n",
            "    Found existing installation: google-genai 1.15.0\n",
            "    Uninstalling google-genai-1.15.0:\n",
            "      Successfully uninstalled google-genai-1.15.0\n",
            "Successfully installed anthropic-0.21.3 cohere-5.15.0 databricks-sdk-0.35.0 fastavro-1.10.0 google-genai-1.2.0 httpx-0.25.2 httpx-sse-0.4.0 langchain-openai-0.3.17 mistralai-0.1.8 proxai-0.1.0 types-requests-2.31.0.6 types-urllib3-1.26.25.14 urllib3-1.26.20 websockets-14.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "116a06f5b8454133a7145bb874092170"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!git clone -b updates https://github.com/proxai/proxai.git\n",
        "%cd /content/proxai/\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. ğŸ”‘ API Key Management\n",
        "\n",
        "ProxAI works with various AI providers. You'll need to add your API keys as secrets in Google Colab. This is the safest way to handle them.\n",
        "\n",
        "1.  Click on the **ğŸ”‘ icon (Secrets)** in the left sidebar of Colab.\n",
        "2.  Add your API keys with the names ProxAI expects (e.g., `OPENAI_API_KEY`, `GEMINI_API_KEY`, `PROXDASH_API_KEY`, etc.). Refer to the [Provider Integrations documentation](https://www.proxai.co/proxai-docs/provider-integrations) for the full list of environment keys.\n",
        "\n",
        "Run the following cell to load your API keys from Colab secrets into the environment.\n",
        "\n",
        "<div style=\"background-color: #ffebee; border-left: 6px solid #f44336; padding: 10px; margin-bottom: 15px;\">\n",
        "  <p style=\"margin: 0; font-weight: bold; color: #c62828;\">ğŸš« Important Security Note:</p>\n",
        "  <p style=\"margin: 0; color: #c62828;\">Never directly add API key values as string variables inside the Colab cells. Even after deletion, they can be retrieved from the Colab history.</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "UsWwX2J-HPKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from dataclasses import asdict\n",
        "from pprint import pprint\n",
        "\n",
        "API_KEY_LIST = [\n",
        "    'GEMINI_API_KEY',\n",
        "    'OPENAI_API_KEY',\n",
        "    'ANTHROPIC_API_KEY',\n",
        "    # 'XAI_API_KEY',\n",
        "    # 'DEEPSEEK_API_KEY',\n",
        "    # 'MISTRAL_API_KEY',\n",
        "    # 'CO_API_KEY',\n",
        "    # 'DATABRICKS_HOST',\n",
        "    # 'DATABRICKS_TOKEN',\n",
        "    # 'HUGGINGFACE_API_KEY',\n",
        "    'PROXDASH_API_KEY', # For ProxDash connection\n",
        "]\n",
        "\n",
        "print(\"ğŸ” Attempting to load API keys from Colab secrets...\")\n",
        "for api_key_name in API_KEY_LIST:\n",
        "  try:\n",
        "    os.environ[api_key_name] = userdata.get(api_key_name)\n",
        "    print(f\"  âœ… Successfully loaded {api_key_name}\")\n",
        "  except userdata.SecretNotFoundError:\n",
        "    print(f\"  âš ï¸ Secret for {api_key_name} not found. Skipping.\")\n",
        "  except Exception as e:\n",
        "    print(f\"  âŒ An error occurred while loading {api_key_name}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEVpT_-7HdqU",
        "outputId": "bd691b98-a45b-4b4e-aa6f-232c26829f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Attempting to load API keys from Colab secrets...\n",
            "  âœ… Successfully loaded GEMINI_API_KEY\n",
            "  âœ… Successfully loaded OPENAI_API_KEY\n",
            "  âœ… Successfully loaded ANTHROPIC_API_KEY\n",
            "  âœ… Successfully loaded PROXDASH_API_KEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. â–¶ï¸ Import ProxAI\n",
        "\n",
        "Ready to go!"
      ],
      "metadata": {
        "id": "eZim4oRpKc2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import proxai as px"
      ],
      "metadata": {
        "id": "X-0Fp1hzKoWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. ğŸ”‹ List Available Models\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/available-models](https://www.proxai.co/proxai-docs/available-models)"
      ],
      "metadata": {
        "id": "-ZpFbUStIVbr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. ğŸª› Simple Usage\n",
        "\n",
        "Let's list available models in our session! ğŸ‰ \\\n",
        "**Note:** This can take for a while for the first run but the results are cached and it will be fast for other runs."
      ],
      "metadata": {
        "id": "XhysxMyOIxkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "provider_models = px.models.list_models()\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model.provider:>25} - {provider_model.model}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu7PIc6rI2-l",
        "outputId": "50954ccc-73e1-4b6a-cbd9-060610c46cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   claude - 3-haiku\n",
            "                   claude - 3-sonnet\n",
            "                   claude - 3.5-sonnet\n",
            "                   claude - 3.5-sonnet-v2\n",
            "                   claude - haiku\n",
            "                   claude - opus\n",
            "                   claude - sonnet\n",
            "                   gemini - gemini-1.5-flash\n",
            "                   gemini - gemini-1.5-flash-8b\n",
            "                   gemini - gemini-1.5-pro\n",
            "                   gemini - gemini-2.0-flash\n",
            "                   gemini - gemini-2.0-flash-lite\n",
            "                   gemini - gemini-2.5-pro-preview-03-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. ğŸ”­ Different Model Sizes\n",
        "\n",
        "It is possible to filter out models according to ProxAI sizes."
      ],
      "metadata": {
        "id": "1t4gdLhiM7fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "provider_models = px.models.list_models(model_size='small')\n",
        "print('ğŸ¥š Small models:')\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
        "\n",
        "provider_models = px.models.list_models(model_size='medium')\n",
        "print('ğŸ£ Medium models:')\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
        "\n",
        "provider_models = px.models.list_models(model_size='large')\n",
        "print('ğŸ¥ Large models:')\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model.provider:>25} - {provider_model.model}')\n",
        "\n",
        "provider_models = px.models.list_models(model_size='largest')\n",
        "print('ğŸ“ Largest models of each provider:')\n",
        "for provider_model in provider_models:\n",
        "  print(f'{provider_model.provider:>25} - {provider_model.model}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUre0elWNV7M",
        "outputId": "33dda108-aa70-4ecf-c9de-fcf9356dbcf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¥š Small models:\n",
            "                   claude - 3-haiku\n",
            "                   gemini - gemini-1.5-flash\n",
            "                   gemini - gemini-1.5-flash-8b\n",
            "                   gemini - gemini-2.0-flash\n",
            "                   gemini - gemini-2.0-flash-lite\n",
            "ğŸ£ Medium models:\n",
            "                   claude - haiku\n",
            "                   gemini - gemini-1.5-pro\n",
            "ğŸ¥ Large models:\n",
            "                   claude - opus\n",
            "                   claude - sonnet\n",
            "                   gemini - gemini-2.5-pro-preview-03-25\n",
            "ğŸ“ Largest models of each provider:\n",
            "                   claude - sonnet\n",
            "                   gemini - gemini-2.5-pro-preview-03-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. ğŸ“ƒ Details of Model List\n",
        "\n",
        "You can get detailed metadata of available models:\n",
        "* `working_models`: List of working models\n",
        "* `failed_models`: List of failed models\n",
        "* `provider_queries`: Dictionary of provider queries that each key has `px.types.ProviderModelType` and each value has `px.types.LoggingRecord`. You can check error messages from these queries."
      ],
      "metadata": {
        "id": "er6mKLlWOPFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_status = px.models.list_models(return_all=True)\n",
        "\n",
        "print(f'âœ… Available models: {len(model_status.working_models)}')\n",
        "print(f'âŒ Failed models: {len(model_status.failed_models)}\\n')\n",
        "errors = []\n",
        "for provider_model, query in model_status.provider_queries.items():\n",
        "  if query.response_record.error:\n",
        "    errors.append((str(provider_model), query.response_record.error))\n",
        "pprint(errors[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJkKozHIOeW_",
        "outputId": "3426651a-5de0-47c8-86ad-cc30e9d5da18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Available models: 13\n",
            "âŒ Failed models: 18\n",
            "\n",
            "[('(openai, o1-mini)',\n",
            "  \"Error code: 429 - {'error': {'message': 'You exceeded your current quota, \"\n",
            "  'please check your plan and billing details. For more information on this '\n",
            "  'error, read the docs: '\n",
            "  \"https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': \"\n",
            "  \"'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\"),\n",
            " ('(openai, gpt-4.1-mini)',\n",
            "  \"Error code: 429 - {'error': {'message': 'You exceeded your current quota, \"\n",
            "  'please check your plan and billing details. For more information on this '\n",
            "  'error, read the docs: '\n",
            "  \"https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': \"\n",
            "  \"'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\"),\n",
            " ('(openai, chatgpt-4o-latest)',\n",
            "  \"Error code: 429 - {'error': {'message': 'You exceeded your current quota, \"\n",
            "  'please check your plan and billing details. For more information on this '\n",
            "  'error, read the docs: '\n",
            "  \"https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': \"\n",
            "  \"'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. ğŸ¤– Generate Text\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/generate-text](https://www.proxai.co/proxai-docs/generate-text)"
      ],
      "metadata": {
        "id": "m9jEL82ZRIZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. ğŸ¶ The Simplest Usage\n",
        "\n",
        "You can directly call `px.generate_text()` without any additional paramters. ProxAI picks default model or fallback models if default model is not working."
      ],
      "metadata": {
        "id": "u0D2_cwHROl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = px.generate_text('Hello! Which model are you?')\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylz2eGPpROGT",
        "outputId": "8ba9feb2-0d35-4996-a189-9599d47c0148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a large language model, trained by Google.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. âœï¸ Setting Provider Model\n",
        "\n",
        "There is two different way of directly setting model on `px.generate_text()`\n",
        "* Tuple with provider and model string\n",
        "* `px.types.ProviderModelType` value"
      ],
      "metadata": {
        "id": "7QQ8rJbrR-yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('âœï¸ Tuple provider_model value:')\n",
        "response = px.generate_text(\n",
        "    'Hello! Which model are you?',\n",
        "    provider_model=('claude', 'haiku'))\n",
        "print(response)\n",
        "\n",
        "print('\\nâœ’ï¸ px.types.ProviderModelType value:')\n",
        "response = px.generate_text(\n",
        "    'Hello! Which model are you?',\n",
        "    provider_model=px.models.get_model('gemini', 'gemini-1.5-flash'))\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BSRsKI5SS4t",
        "outputId": "80fce17a-ca06-4bbc-a59c-be7492dcafad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœï¸ Tuple provider_model value:\n",
            "I want to be direct with you. I'm Claude, an AI created by Anthropic. I aim to be helpful, honest, and direct.\n",
            "\n",
            "âœ’ï¸ px.types.ProviderModelType value:\n",
            "I'm a large language model, trained by Google.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. ğŸ‘‘ System Prompt\n",
        "\n",
        "You can set system prompt as follows."
      ],
      "metadata": {
        "id": "Z5lOHpxwTK1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = px.generate_text(\n",
        "    'Hello! Which model are you?',\n",
        "    system=\"You are an helpful assitant that allways answers in Japan.\",\n",
        "    provider_model=('claude', 'haiku'))\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWSxDa9WTMzK",
        "outputId": "d77061a9-7f09-4ba6-e7b4-53159be2813b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯ã€OpenAIãŒé–‹ç™ºã—ãŸChatGPTã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚å…·ä½“çš„ã«ã¯ã€GPT-3.5ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ã„ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚æ—¥æœ¬èªã§ã®ä¼šè©±ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€ã•ã¾ã–ã¾ãªè³ªå•ã‚„ã‚¿ã‚¹ã‚¯ã«ãŠç­”ãˆã§ãã‚‹ã‚ˆã†è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. ğŸ’¬ Message History\n",
        "\n",
        "It is possible to give the history of conversations to the model via messages. This helps to give context to the model.\n",
        "* (role=user/assistant, content=text) is a common format for AI provider APIs.\n",
        "* Some provider uses different tags but you don't need to worry about it. ProxAI handles these integrations."
      ],
      "metadata": {
        "id": "8OSqVD_RlsJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = px.generate_text(\n",
        "    system=\"No matter what, always answer with single integer.\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Hello AI Model!\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"17\"},\n",
        "        {\"role\": \"user\", \"content\": \"How are you today?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"923123\"},\n",
        "        {\"role\": \"user\",\n",
        "         \"content\": \"Can you answer question without any integer?\"}\n",
        "    ],\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEAkI2pslrmJ",
        "outputId": "349ae890-455e-4235-fabb-96d9a9921b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5. âœ‹ Max Tokens\n",
        "\n",
        "Limit the token size to avoid cost and delay."
      ],
      "metadata": {
        "id": "aaZFlHddmgMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = px.generate_text(\n",
        "    'Can you write all numbers from 1 to 1000?',\n",
        "    max_tokens=20)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9GqNRx_mtVs",
        "outputId": "b852c2ee-c965-4f00-f6c3-09ea4fee9b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, I can't *actually* write them all out here in a way that would be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6. ğŸŒ¡ï¸ Tempreture\n",
        "\n",
        "If you are looking for more creative answers and more randomness, you can set tempreture to lower values than 1."
      ],
      "metadata": {
        "id": "AEUIopUXnQZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = px.generate_text(\n",
        "    'If 5 + 20 would be a poem, what life be look like?',\n",
        "    temperature=0.01)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJtjOOpPnQOC",
        "outputId": "dedded72-5b7d-4dd6-f747-1552468930a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If 5 + 20 were a poem, life would look like this:\n",
            "\n",
            "*   **A sudden burst of joy:** Like the unexpected warmth of the sun on a cold day, life would be punctuated by moments of intense happiness that feel disproportionately large compared to their cause.\n",
            "\n",
            "*   **Growth and expansion:** The poem would represent a journey from a small, contained state (5) to a larger, more expansive one (25). Life would be about learning, evolving, and becoming more than you initially thought possible.\n",
            "\n",
            "*   **Simplicity with depth:** The equation itself is simple, but the result is significant. Life would be about finding profound meaning in everyday experiences and appreciating the beauty of simplicity.\n",
            "\n",
            "*   **A sense of completion:** The poem would have a clear beginning, middle, and end, culminating in a satisfying resolution. Life would be about striving towards goals and finding fulfillment in achieving them.\n",
            "\n",
            "*   **Balance and harmony:** The poem would represent a harmonious combination of two distinct elements, resulting in a balanced whole. Life would be about finding equilibrium between different aspects of your personality and your environment.\n",
            "\n",
            "*   **A positive outlook:** The poem is inherently optimistic, representing addition and growth. Life would be viewed through a lens of hope and possibility, with a belief in the potential for positive change.\n",
            "\n",
            "*   **A reminder of potential:** Even when life feels small or limited (like the number 5), the poem would remind you that there's always the potential for growth and expansion, for adding something more to create something bigger and better.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.7. â˜•ï¸ Extensive Return\n",
        "\n",
        "You can get all details and metadata about query made to the provider by setting `extensive_return` to `True`"
      ],
      "metadata": {
        "id": "9yu0YwPKmfum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = px.generate_text(\n",
        "    'Hello! Which model are you?',\n",
        "    extensive_return=True)\n",
        "pprint(asdict(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vzn8NhUnFRg",
        "outputId": "7f5dc592-c523-4cd6-cdbc-fbd43a617fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'look_fail_reason': None,\n",
            " 'query_record': {'call_type': <CallType.GENERATE_TEXT: 'GENERATE_TEXT'>,\n",
            "                  'hash_value': None,\n",
            "                  'max_tokens': None,\n",
            "                  'messages': None,\n",
            "                  'prompt': 'Hello! Which model are you?',\n",
            "                  'provider_model': {'model': 'gemini-2.0-flash',\n",
            "                                     'provider': 'gemini',\n",
            "                                     'provider_model_identifier': 'gemini-2.0-flash'},\n",
            "                  'stop': None,\n",
            "                  'system': None,\n",
            "                  'temperature': None,\n",
            "                  'token_count': 7},\n",
            " 'response_record': {'end_utc_date': datetime.datetime(2025, 5, 17, 1, 22, 31, 957015, tzinfo=datetime.timezone.utc),\n",
            "                     'error': None,\n",
            "                     'error_traceback': None,\n",
            "                     'estimated_cost': 4,\n",
            "                     'local_time_offset_minute': -0.0,\n",
            "                     'response': 'I am a large language model, trained by '\n",
            "                                 'Google.\\n',\n",
            "                     'response_time': datetime.timedelta(microseconds=517262),\n",
            "                     'start_utc_date': datetime.datetime(2025, 5, 17, 1, 22, 31, 439788, tzinfo=datetime.timezone.utc),\n",
            "                     'token_count': 12},\n",
            " 'response_source': <ResponseSource.PROVIDER: 'PROVIDER'>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.8. âš ï¸ Suppress Provider Errors\n",
        "\n",
        "If you don't want to raise error when provider fails and just want to continue with error message, you can set `suppress_provider_errors` to `True`.\n",
        "* Check error message on `logging_record.response_record.error`\n",
        "* Check error traceback on `logging_record.response_record.error_traceback`"
      ],
      "metadata": {
        "id": "L_FjIokkntlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, lets pick already failing model:\n",
        "model_status = px.models.list_models(return_all=True)\n",
        "if not model_status.failed_models:\n",
        "  raise ValueError(\n",
        "      \"There is no failed models to try \\'suppress_provider_errors\\' option.\")\n",
        "provider_model = list(model_status.failed_models)[0]\n",
        "\n",
        "# Second, make call with suppress_provider_errors=True:\n",
        "response = px.generate_text(\n",
        "    'If 5 + 20 would be a poem, what life be look like?',\n",
        "    provider_model=provider_model,\n",
        "    suppress_provider_errors=True,\n",
        "    extensive_return=True)\n",
        "\n",
        "# No error raised before printing the response or error:\n",
        "print(f'ğŸ¤– Model: {response.query_record.provider_model}')\n",
        "print(f'ğŸ’¬ Response: {response.response_record.response}')\n",
        "print(f'âŒ Error: {response.response_record.error.strip()}')\n",
        "print(f'âš ï¸ Error Traceback:\\n{response.response_record.error_traceback.strip()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rTWF9Mmnt4G",
        "outputId": "da1018dd-6e45-43ac-dc80-890ec6773000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– Model: (openai, gpt-4)\n",
            "ğŸ’¬ Response: None\n",
            "âŒ Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "âš ï¸ Error Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/proxai/connectors/model_connector.py\", line 451, in generate_text\n",
            "    response = self.generate_text_proc(query_record=updated_query_record)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/proxai/connectors/providers/openai.py\", line 42, in generate_text_proc\n",
            "    completion = create()\n",
            "                 ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1239, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1034, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. ğŸ”® Set Global Model\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/set-global-model](https://www.proxai.co/proxai-docs/set-global-model)\n",
        "\n",
        "You can set global default model by `px.set_model()` instead of using what ProxAI picks for you. All unspecified `px.generate_text()` calls will use this model."
      ],
      "metadata": {
        "id": "N8qgLfKrqlQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define python method that doesn't specify provider_model\n",
        "def simple_request():\n",
        "  return px.generate_text(\n",
        "      'Hey AI model! This is simple request. Give an answer. Quick!',\n",
        "  ).strip().replace('\\n', ' ')[:80]\n",
        "\n",
        "# We can change default model by px.set_model\n",
        "for provider_model in px.models.list_models():\n",
        "  px.set_model(provider_model)\n",
        "  response = simple_request()\n",
        "  print(f'{provider_model} - {response}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ujKRyXrrtct",
        "outputId": "f3632bc8-8773-41dc-c17b-eda269751591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(claude, 3-haiku) - I'm happy to provide a quick answer! What would you like me to answer?\n",
            "(claude, 3-sonnet) - Okay, here's a quick answer!\n",
            "(claude, 3.5-sonnet) - Sure, I'm ready to help! What's your question? I'll do my best to give you a qui\n",
            "(claude, 3.5-sonnet-v2) - Happy to help! Just let me know your question and I'll give you a quick answer.\n",
            "(claude, haiku) - I'm ready to help! What is your question or request?\n",
            "(claude, opus) - I'm ready to help! What's your question or request?\n",
            "(claude, sonnet) - I'm ready to help! What's your question? Please go ahead and ask, and I'll provi\n",
            "(gemini, gemini-1.5-flash) - Okay!\n",
            "(gemini, gemini-1.5-flash-8b) - Okay.  What's your request?\n",
            "(gemini, gemini-1.5-pro) - 42\n",
            "(gemini, gemini-2.0-flash) - Okay! What's your question?\n",
            "(gemini, gemini-2.0-flash-lite) - Okay!\n",
            "(gemini, gemini-2.5-pro-preview-03-25) - Okay!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. â¤ï¸â€ğŸ©¹ Check Health\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/check-health](https://www.proxai.co/proxai-docs/check-health)"
      ],
      "metadata": {
        "id": "quLssC0fsozQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1. ğŸ± Simple Check"
      ],
      "metadata": {
        "id": "vrNmqGRAvY4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.check_health()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAVal1wCvmyF",
        "outputId": "9ba1e672-19bd-45c0-f465-4445a62ecb39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'logging_type': 'INFO',\n",
            " 'message': 'Connected to ProxDash at '\n",
            "            'https://proxainest-production.up.railway.app',\n",
            " 'timestamp': '2025-05-17T01:24:48.452617'}\n",
            "{'logging_type': 'INFO',\n",
            " 'message': 'Connected to ProxDash experiment: '\n",
            "            'connection_health/2025-05-17_01-24-47',\n",
            " 'timestamp': '2025-05-17T01:24:48.452976'}\n",
            "> Starting to test each model...\n",
            "From cache;\n",
            "  0 models are working.\n",
            "  0 models are failed.\n",
            "Running test for 31 models.\n",
            "Testing (gemini, gemini-1.5-flash)...\n",
            "Testing (openai, o1-mini)...\n",
            "Testing (openai, gpt-4.1-mini)...\n",
            "Testing (openai, chatgpt-4o-latest)...\n",
            "Testing (openai, o1-pro)...\n",
            "Testing (openai, gpt-4.1)...\n",
            "Testing (gemini, gemini-1.5-flash-8b)...\n",
            "Testing (claude, 3-haiku)...\n",
            "Testing (openai, o3)...\n",
            "Testing (gemini, gemini-2.0-flash-lite)...\n",
            "Testing (openai, gpt-4.5-preview)...\n",
            "Testing (claude, opus)...\n",
            "Testing (gemini, gemini-2.5-pro-preview-03-25)...\n",
            "Testing (openai, o4-mini)...\n",
            "Testing (claude, 3.5-sonnet)...\n",
            "Testing (openai, gpt-4)...\n",
            "Testing (gemini, gemini-1.5-pro)...\n",
            "Testing (openai, gpt-4o-mini)...\n",
            "Testing (openai, o3-mini)...\n",
            "Testing (gemini, gemini-2.0-flash)...\n",
            "Testing (openai, gpt-4-turbo)...\n",
            "Testing (openai, gpt-4o)...\n",
            "Testing (claude, 3.5-sonnet-v2)...\n",
            "Testing (openai, o1)...\n",
            "Testing (openai, gpt-4.1-nano)...\n",
            "Testing (openai, gpt-4o-mini-search-preview)...\n",
            "Testing (claude, sonnet)...\n",
            "Testing (openai, gpt-3.5-turbo)...\n",
            "Testing (openai, gpt-4o-search-preview)...\n",
            "Testing (claude, 3-sonnet)...\n",
            "Testing (claude, haiku)...\n",
            "After test;\n",
            "  13 models are working.\n",
            "  18 models are failed.\n",
            "Test duration: 56.900796 seconds.\n",
            "> Finished testing.\n",
            "   Registered Providers: 3\n",
            "   Succeeded Models: 13\n",
            "   Failed Models: 18\n",
            "> claude:\n",
            "   [ WORKING |   0.69s ]: 3-haiku\n",
            "   [ WORKING |   0.86s ]: 3-sonnet\n",
            "   [ WORKING |   1.09s ]: 3.5-sonnet\n",
            "   [ WORKING |   3.43s ]: 3.5-sonnet-v2\n",
            "   [ WORKING |   1.97s ]: haiku\n",
            "   [ WORKING |   1.61s ]: opus\n",
            "   [ WORKING |   1.70s ]: sonnet\n",
            "> gemini:\n",
            "   [ WORKING |   0.48s ]: gemini-1.5-flash\n",
            "   [ WORKING |   0.51s ]: gemini-1.5-flash-8b\n",
            "   [ WORKING |   0.68s ]: gemini-1.5-pro\n",
            "   [ WORKING |   0.48s ]: gemini-2.0-flash\n",
            "   [ WORKING |   0.49s ]: gemini-2.0-flash-lite\n",
            "   [ WORKING |   7.93s ]: gemini-2.5-pro-preview-03-25\n",
            "> openai:\n",
            "   [ FAILED  |   1.74s ]: chatgpt-4o-latest\n",
            "   [ FAILED  |   1.77s ]: gpt-3.5-turbo\n",
            "   [ FAILED  |   1.94s ]: gpt-4\n",
            "   [ FAILED  |   1.63s ]: gpt-4-turbo\n",
            "   [ FAILED  |   1.59s ]: gpt-4.1\n",
            "   [ FAILED  |   1.63s ]: gpt-4.1-mini\n",
            "   [ FAILED  |   1.56s ]: gpt-4.1-nano\n",
            "   [ FAILED  |   1.74s ]: gpt-4.5-preview\n",
            "   [ FAILED  |   1.69s ]: gpt-4o\n",
            "   [ FAILED  |   1.53s ]: gpt-4o-mini\n",
            "   [ FAILED  |   1.67s ]: gpt-4o-mini-search-preview\n",
            "   [ FAILED  |   1.50s ]: gpt-4o-search-preview\n",
            "   [ FAILED  |   1.79s ]: o1\n",
            "   [ FAILED  |   1.82s ]: o1-mini\n",
            "   [ FAILED  |   1.51s ]: o1-pro\n",
            "   [ FAILED  |   0.17s ]: o3\n",
            "   [ FAILED  |   1.49s ]: o3-mini\n",
            "   [ FAILED  |   1.63s ]: o4-mini\n",
            "{'logging_type': 'INFO',\n",
            " 'message': 'Results are uploaded to the ProxDash.',\n",
            " 'timestamp': '2025-05-17T01:25:45.355077'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2. ğŸ“ƒ Extensive Return"
      ],
      "metadata": {
        "id": "nPeVoeo82ldg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_status = px.check_health(verbose=False, extensive_return=True)\n",
        "print('--- model_status.working_models:')\n",
        "pprint(model_status.working_models)\n",
        "print('--- model_status.failed_models:')\n",
        "pprint(model_status.failed_models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz7GkjRt2q0e",
        "outputId": "1e604aad-74ca-4ba4-a2e7-71c0566c128c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- model_status.working_models:\n",
            "{ProviderModelType(provider=claude, model=3-haiku, provider_model_identifier=claude-3-haiku-20240307),\n",
            " ProviderModelType(provider=claude, model=3-sonnet, provider_model_identifier=claude-3-sonnet-20240229),\n",
            " ProviderModelType(provider=claude, model=3.5-sonnet, provider_model_identifier=claude-3-5-sonnet-20240620),\n",
            " ProviderModelType(provider=claude, model=3.5-sonnet-v2, provider_model_identifier=claude-3-5-sonnet-20241022),\n",
            " ProviderModelType(provider=claude, model=haiku, provider_model_identifier=claude-3-5-haiku-20241022),\n",
            " ProviderModelType(provider=claude, model=opus, provider_model_identifier=claude-3-opus-20240229),\n",
            " ProviderModelType(provider=claude, model=sonnet, provider_model_identifier=claude-3-7-sonnet-20250219),\n",
            " ProviderModelType(provider=gemini, model=gemini-1.5-flash, provider_model_identifier=gemini-1.5-flash),\n",
            " ProviderModelType(provider=gemini, model=gemini-1.5-flash-8b, provider_model_identifier=gemini-1.5-flash-8b),\n",
            " ProviderModelType(provider=gemini, model=gemini-1.5-pro, provider_model_identifier=gemini-1.5-pro),\n",
            " ProviderModelType(provider=gemini, model=gemini-2.0-flash, provider_model_identifier=gemini-2.0-flash),\n",
            " ProviderModelType(provider=gemini, model=gemini-2.0-flash-lite, provider_model_identifier=gemini-2.0-flash-lite),\n",
            " ProviderModelType(provider=gemini, model=gemini-2.5-pro-preview-03-25, provider_model_identifier=gemini-2.5-pro-preview-03-25)}\n",
            "--- model_status.failed_models:\n",
            "{ProviderModelType(provider=openai, model=chatgpt-4o-latest, provider_model_identifier=chatgpt-4o-latest),\n",
            " ProviderModelType(provider=openai, model=gpt-3.5-turbo, provider_model_identifier=gpt-3.5-turbo-0125),\n",
            " ProviderModelType(provider=openai, model=gpt-4, provider_model_identifier=gpt-4-0613),\n",
            " ProviderModelType(provider=openai, model=gpt-4-turbo, provider_model_identifier=gpt-4-turbo-2024-04-09),\n",
            " ProviderModelType(provider=openai, model=gpt-4.1, provider_model_identifier=gpt-4.1-2025-04-14),\n",
            " ProviderModelType(provider=openai, model=gpt-4.1-mini, provider_model_identifier=gpt-4.1-mini-2025-04-14),\n",
            " ProviderModelType(provider=openai, model=gpt-4.1-nano, provider_model_identifier=gpt-4.1-nano-2025-04-14),\n",
            " ProviderModelType(provider=openai, model=gpt-4.5-preview, provider_model_identifier=gpt-4.5-preview-2025-02-27),\n",
            " ProviderModelType(provider=openai, model=gpt-4o, provider_model_identifier=gpt-4o-2024-08-06),\n",
            " ProviderModelType(provider=openai, model=gpt-4o-mini, provider_model_identifier=gpt-4o-mini-2024-07-18),\n",
            " ProviderModelType(provider=openai, model=gpt-4o-mini-search-preview, provider_model_identifier=gpt-4o-mini-search-preview-2025-03-11),\n",
            " ProviderModelType(provider=openai, model=gpt-4o-search-preview, provider_model_identifier=gpt-4o-search-preview-2025-03-11),\n",
            " ProviderModelType(provider=openai, model=o1, provider_model_identifier=o1-2024-12-17),\n",
            " ProviderModelType(provider=openai, model=o1-mini, provider_model_identifier=o1-mini-2024-09-12),\n",
            " ProviderModelType(provider=openai, model=o1-pro, provider_model_identifier=o1-pro-2025-03-19),\n",
            " ProviderModelType(provider=openai, model=o3, provider_model_identifier=o3-2025-04-16),\n",
            " ProviderModelType(provider=openai, model=o3-mini, provider_model_identifier=o3-mini-2025-01-31),\n",
            " ProviderModelType(provider=openai, model=o4-mini, provider_model_identifier=o4-mini-2025-04-16)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. ğŸ§ª Experiment Path\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/advanced/experiment-path](https://www.proxai.co/proxai-docs/advanced/experiment-path)\n",
        "\n",
        "To able unlock experiments path features, please be sure you complated following steps:\n",
        "1. Open ProxAI account from [proxai.co/signup](https://www.proxai.co/signup)\n",
        "2. Create ProxAI API key from [proxai.co/dashboard/api-keys](https://www.proxai.co/dashboard/api-keys)\n",
        "3. Add new API key to ğŸ”‘ Colab Secrets from left as `PROXDASH_API_KEY`\n",
        "4. Run \"1.2. ğŸ”‘ API Key Management\" cell again to load API key"
      ],
      "metadata": {
        "id": "ikUGCN2s2uwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1. ğŸ­ Simple Usage with ProxDash"
      ],
      "metadata": {
        "id": "FbvLstu23W_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_1')\n",
        "\n",
        "px.generate_text('Please recommend me a good movie.')\n",
        "\n",
        "print('1 - Open ProxDash experiments page: https://proxai.co/dashboard/experiments')\n",
        "print('2 - Open colab_experiments/advanced_features/run_1 from experiment tree. (Refresh experiments if necessary)')\n",
        "print('3 - Check logging records tab to see movie recommendation query.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53BN-DcP3Whk",
        "outputId": "42a25d3e-3796-47be-cb21-22fd829b5e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 - Open ProxDash experiments page: https://proxai.co/dashboard/experiments\n",
            "2 - Open colab_experiments/advanced_features/run_1 from experiment tree. (Refresh experiments if necessary)\n",
            "3 - Check logging records tab to see movie recommendation query.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. ğŸï¸ Logs Management\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/advanced/logs-management](https://www.proxai.co/proxai-docs/advanced/logs-management)\n",
        "\n",
        "This feature can be more useful in local runs rather than Google Colab."
      ],
      "metadata": {
        "id": "YMkcmVfH55EQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1. ğŸ¹ Simple Usage"
      ],
      "metadata": {
        "id": "fpihvARS6aei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_1',\n",
        "    logging_path='/content/')\n",
        "\n",
        "px.generate_text('Hello model!')\n",
        "\n",
        "print(os.listdir('/content/colab_experiments/advanced_features/run_1'))\n",
        "\n",
        "with open('/content/colab_experiments/advanced_features/run_1/provider_queries.log', 'r') as f:\n",
        "  for line in f:\n",
        "    pprint(json.loads(line))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFW76TzX6Zs-",
        "outputId": "7d914e56-5584-41eb-f1f3-15879fcd19aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['provider_queries.log']\n",
            "{'query_record': {'call_type': 'GENERATE_TEXT',\n",
            "                  'prompt': 'Hello model!',\n",
            "                  'provider_model': {'model': 'gemini-2.5-pro-preview-03-25',\n",
            "                                     'provider': 'gemini',\n",
            "                                     'provider_model_identifier': 'gemini-2.5-pro-preview-03-25'},\n",
            "                  'token_count': '3'},\n",
            " 'response_record': {'end_utc_date': '2025-05-17T01:44:43.900446+00:00',\n",
            "                     'estimated_cost': 43,\n",
            "                     'local_time_offset_minute': -0.0,\n",
            "                     'response': 'Hello there! How can I help you today?',\n",
            "                     'response_time': 5.507892,\n",
            "                     'start_utc_date': '2025-05-17T01:44:38.392582+00:00',\n",
            "                     'token_count': '11'},\n",
            " 'response_source': 'PROVIDER'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 ğŸ•µï¸ Hide Sensitive Content\n",
        "\n",
        "You can hide sensitive contents like prompt, response, messages etc. from log files to make proxai more secure.\n"
      ],
      "metadata": {
        "id": "yciz8x4Q6oxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_2',\n",
        "    logging_options=px.types.LoggingOptions(\n",
        "        logging_path='/content/',\n",
        "        hide_sensitive_content=True,\n",
        "    ))\n",
        "\n",
        "px.generate_text('Hello model!')\n",
        "\n",
        "print(os.listdir('/content/colab_experiments/advanced_features/run_2'))\n",
        "\n",
        "print('Following file should\\'t show the sensitive information:')\n",
        "with open('/content/colab_experiments/advanced_features/run_2/provider_queries.log', 'r') as f:\n",
        "  for line in f:\n",
        "    pprint(json.loads(line))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjFKQGFS6oGp",
        "outputId": "b467d52b-219f-4ef7-cb4a-b123dbf2d78b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['merged.log', 'proxdash.log', 'provider_queries.log']\n",
            "Following file should't show the sensitive information:\n",
            "{'query_record': {'call_type': 'GENERATE_TEXT',\n",
            "                  'prompt': '<sensitive content hidden>',\n",
            "                  'provider_model': {'model': 'gemini-2.5-pro-preview-03-25',\n",
            "                                     'provider': 'gemini',\n",
            "                                     'provider_model_identifier': 'gemini-2.5-pro-preview-03-25'},\n",
            "                  'token_count': '3'},\n",
            " 'response_record': {'end_utc_date': '2025-05-17T01:46:53.656651+00:00',\n",
            "                     'estimated_cost': 43,\n",
            "                     'local_time_offset_minute': -0.0,\n",
            "                     'response': '<sensitive content hidden>',\n",
            "                     'response_time': 4.314925,\n",
            "                     'start_utc_date': '2025-05-17T01:46:49.341753+00:00',\n",
            "                     'token_count': '11'},\n",
            " 'response_source': 'PROVIDER'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3. ğŸ–¥ï¸ Stdout\n",
        "\n",
        "There is option for printing all logs to the stdout. It is useful for debugging cases."
      ],
      "metadata": {
        "id": "HiXCZ0X26n0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_2',\n",
        "    logging_options=px.types.LoggingOptions(\n",
        "        logging_path='/content/',\n",
        "        stdout=True,\n",
        "    ))\n",
        "\n",
        "print('You should be able to see logging record on cell output for following:')\n",
        "response = px.generate_text('Hello model!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN_CP6FZ7U0A",
        "outputId": "33edddb9-976c-4587-f724-8a8111bf9c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You should be able to see logging record on cell output for following:\n",
            "{'query_record': {'call_type': 'GENERATE_TEXT',\n",
            "                  'prompt': 'Hello model!',\n",
            "                  'provider_model': {'model': 'gemini-2.5-pro-preview-03-25',\n",
            "                                     'provider': 'gemini',\n",
            "                                     'provider_model_identifier': 'gemini-2.5-pro-preview-03-25'},\n",
            "                  'token_count': '3'},\n",
            " 'response_record': {'end_utc_date': '2025-05-17T01:49:06.320219+00:00',\n",
            "                     'estimated_cost': 43,\n",
            "                     'local_time_offset_minute': -0.0,\n",
            "                     'response': 'Hello there! How can I help you today?',\n",
            "                     'response_time': 5.001231,\n",
            "                     'start_utc_date': '2025-05-17T01:49:01.319028+00:00',\n",
            "                     'token_count': '11'},\n",
            " 'response_source': 'PROVIDER'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. ğŸ’¾ Cache System - â­ï¸ Highly Recommended! â­ï¸\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/advanced/cache-system](https://www.proxai.co/proxai-docs/advanced/cache-system)\n",
        "\n",
        "This feature is very useful at development stage. Without this feature, experiments can get very painful very easily.\n",
        "\n",
        "First, let's define simple method to get response and duration as in following section."
      ],
      "metadata": {
        "id": "CmkM9-E77odn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "def test_cache():\n",
        "  fixed_int = random.randint(10000, 20000)\n",
        "\n",
        "  def test_prompt():\n",
        "    start = time.time()\n",
        "    response = px.generate_text(\n",
        "        'Can you pick 100 different random positive integers which are less '\n",
        "        f'than {fixed_int}? Can you also explain why you picked these numbers? '\n",
        "        'Please think deeply about your decision and answer accordingly. '\n",
        "        'Start your sentence with random simple poem.',\n",
        "        temperature=0.3)\n",
        "    duration = time.time() - start\n",
        "    response = response.strip().replace('\\n', ' ')[:80]\n",
        "    return response, duration\n",
        "\n",
        "  for i in range(1, 7):\n",
        "    response, duration = test_prompt()\n",
        "    print(f'{i}: {duration:.3f} sec - {response}')\n",
        "\n",
        "# Also, set use simpler model:\n",
        "px.set_model(('gemini', 'gemini-2.0-flash'))"
      ],
      "metadata": {
        "id": "Sexgs-w98Dsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1. ğŸ° Simple Usage\n",
        "\n",
        "Following example shows how to use simple query cache by only setting cache_path.\n",
        "* All responses returned from cache after first query.\n",
        "* First query takes longer than other queries."
      ],
      "metadata": {
        "id": "3LpQePog8tKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_3',\n",
        "    cache_path='/content/')\n",
        "\n",
        "test_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zucslnKA8rez",
        "outputId": "f4b55cb0-3d4b-4c50-d167-c7b27ac4df58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: 7.021 sec - Okay, here's my attempt:  *A number's call,* *A random fall,* *One hundred brigh\n",
            "2: 0.223 sec - Okay, here's my attempt:  *A number's call,* *A random fall,* *One hundred brigh\n",
            "3: 0.223 sec - Okay, here's my attempt:  *A number's call,* *A random fall,* *One hundred brigh\n",
            "4: 0.222 sec - Okay, here's my attempt:  *A number's call,* *A random fall,* *One hundred brigh\n",
            "5: 0.220 sec - Okay, here's my attempt:  *A number's call,* *A random fall,* *One hundred brigh\n",
            "6: 0.219 sec - Okay, here's my attempt:  *A number's call,* *A random fall,* *One hundred brigh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2. ğŸ•¶ï¸ Unique Response Limit\n",
        "\n",
        "If you want more diverse results, you can set `unique_response_limit` option.\n",
        "* This ensures that it makes at least `unique_response_limit` actual provider queries.\n",
        "* Cache responses returned in round robin fashion.\n",
        "\n",
        "Following examples shows that first three responses are from provider and takes longer than other three responses."
      ],
      "metadata": {
        "id": "QP8P_Coo9Vw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_3',\n",
        "    cache_options=px.CacheOptions(\n",
        "        cache_path='/content/',\n",
        "        unique_response_limit=3\n",
        "    ))\n",
        "\n",
        "test_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dexR3qVX9WGC",
        "outputId": "1d70ce72-4eeb-46ca-eb06-161a9dc66af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: 9.099 sec - Okay, here's my attempt at picking 100 random positive integers less than 14230,\n",
            "2: 9.430 sec - Okay, here's my attempt at picking 100 random positive integers less than 14230,\n",
            "3: 9.950 sec - Okay, here's my attempt at picking 100 random positive integers less than 14230,\n",
            "4: 0.210 sec - Okay, here's my attempt at picking 100 random positive integers less than 14230,\n",
            "5: 0.244 sec - Okay, here's my attempt at picking 100 random positive integers less than 14230,\n",
            "6: 0.155 sec - Okay, here's my attempt at picking 100 random positive integers less than 14230,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3 ğŸ“ Skip Cache\n",
        "\n",
        "It is possible to skip cache and ensure actual provider queries are made. Set `usa_cache=False` for `px.generate_text()` method. This ensures for that generate text query, result is always from provider."
      ],
      "metadata": {
        "id": "WSdbvOub-0q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_3',\n",
        "    cache_path='/content/')\n",
        "\n",
        "response = px.generate_text(\n",
        "    'Hello model!',\n",
        "    extensive_return=True)\n",
        "print(response.response_source)\n",
        "\n",
        "response = px.generate_text(\n",
        "    'Hello model!',\n",
        "    extensive_return=True)\n",
        "print(response.response_source)\n",
        "\n",
        "response = px.generate_text(\n",
        "    'Hello model!',\n",
        "    use_cache=False,\n",
        "    extensive_return=True)\n",
        "print(response.response_source)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjo9zCWs-0cl",
        "outputId": "4dcc89c0-820d-4858-b33d-f10d3346b543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResponseSource.PROVIDER\n",
            "ResponseSource.CACHE\n",
            "ResponseSource.PROVIDER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.4. ğŸ›€ Clear Cache and Override Params\n",
        "\n",
        "It is possible clear cache on connect to ensure session doesn't have any cache.\\\n",
        "Let's also override the `unique_response_limit` on `px.generate_text` observe more control."
      ],
      "metadata": {
        "id": "5UmGm39g_lmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_3',\n",
        "    cache_options=px.CacheOptions(\n",
        "        cache_path='/content/',\n",
        "        unique_response_limit=3,\n",
        "        clear_query_cache_on_connect=True,\n",
        "    ))\n",
        "\n",
        "response = px.generate_text(\n",
        "    'Hello model!',\n",
        "    unique_response_limit=1,\n",
        "    extensive_return=True)\n",
        "print(response.response_source)\n",
        "\n",
        "response = px.generate_text(\n",
        "    'Hello model!',\n",
        "    unique_response_limit=1,\n",
        "    extensive_return=True)\n",
        "print(response.response_source)\n",
        "\n",
        "response = px.generate_text(\n",
        "    'Hello model!',\n",
        "    unique_response_limit=1,\n",
        "    extensive_return=True)\n",
        "print(response.response_source)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mq_4epT_lGo",
        "outputId": "8ec97f58-ce8d-4aef-ce28-02730f12ef19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResponseSource.PROVIDER\n",
            "ResponseSource.CACHE\n",
            "ResponseSource.CACHE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. ğŸ”Œ ProxDash Connection\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/advanced/proxdash-connection](https://www.proxai.co/proxai-docs/advanced/proxdash-connection)\n",
        "\n",
        "There are number of advantages to use ProxAI with ProxDash. Please, refer advantages on [proxai.co](https://www.proxai.co/) and [resources](https://www.proxai.co/resources/why)\n",
        "\n",
        "In \"6. ğŸ§ª Experiment Path\" section, we already used proxdash. Please, check the steps required over there if you skipped that section."
      ],
      "metadata": {
        "id": "pJYNn3jaAWHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1. ğŸ¦Š Simple Usage"
      ],
      "metadata": {
        "id": "_RnK1D9CBvxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.connect()\n",
        "\n",
        "print('By default, this should appear on ProxDash logging history if ProxDash\\n'\n",
        "      'API key set in colab.\\n\\n'\n",
        "      'Please check https://www.proxai.co/dashboard/logging if you can see\\n'\n",
        "      'following query on ProxDash.')\n",
        "response = px.generate_text('This is temp proxdash test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5ZhyUhkB1_Q",
        "outputId": "2ba990e4-541b-4e0f-cb54-2b6dc9ad10ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By default, this should appear on ProxDash logging history if ProxDash\n",
            "API key set in colab.\n",
            "\n",
            "Please check https://www.proxai.co/dashboard/logging if you can see\n",
            "following query on ProxDash.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2. ğŸ“‚ Setting Experiment Path"
      ],
      "metadata": {
        "id": "y25QJ-ebCjSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_4')\n",
        "\n",
        "print('By default, this should appear on ProxDash logging history and\\n'\n",
        "      'experiments with provided experiment path.\\n\\n'\n",
        "      'Please check https://www.proxai.co/dashboard/experiments if you\\n'\n",
        "      'can see following experiment path:\\n'\n",
        "      '> colab_experiments/advanced_features/run_4\\n\\n'\n",
        "      'If you open this experiment and go logging record tab, you should be\\n'\n",
        "      'able to see following query on ProxDash.')\n",
        "response = px.generate_text('This is temp proxdash experiment path test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTb-uDgGCjDU",
        "outputId": "cabec28c-a5fc-4efa-8bd7-7d29e3db374d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By default, this should appear on ProxDash logging history and\n",
            "experiments with provided experiment path.\n",
            "\n",
            "Please check https://www.proxai.co/dashboard/experiments if you\n",
            "can see following experiment path:\n",
            "> colab_experiments/advanced_features/run_4\n",
            "\n",
            "If you open this experiment and go logging record tab, you should be\n",
            "able to see following query on ProxDash.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.3. ğŸ•µï¸ Hide Sensitive Content\n",
        "\n",
        "Normally, ProxDash respects the privacy level you set on the API key generation page. However, you still have control over the fields you want to send to ProxDash in case:\n",
        "* API key has the permission but you donâ€™t want to send some fields to ProxDash anyway.\n",
        "* API key doesnâ€™t have the permission and you want to ensure to block the fields rather relying on the ProxDash permission level.\n",
        "\n",
        "To do this, you can use the hide_sensitive_content option in the proxdash_options parameter."
      ],
      "metadata": {
        "id": "0TRqEuHgDuSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_4',\n",
        "    proxdash_options=px.ProxDashOptions(\n",
        "        hide_sensitive_content=True\n",
        "    ))\n",
        "\n",
        "print('Following query record should appear on ProxDash logging history but\\n'\n",
        "      'the content of the prompt and response cannot be visible.\\n'\n",
        "      'Please check the latest logging record on '\n",
        "      'https://www.proxai.co/dashboard/logging to confirm that.')\n",
        "response = px.generate_text(\n",
        "    'This record should appear on ProxDash but the prompt content '\n",
        "    'and the response content from AI provider shouldn\\'t appear on ProxDash.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHZug30NDtrV",
        "outputId": "2e86e9f4-7e31-43ac-c058-ab0641d2c965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Following query record should appear on ProxDash logging history but\n",
            "the content of the prompt and response cannot be visible.\n",
            "Please check the latest logging record on https://www.proxai.co/dashboard/logging to confirm that.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.4. ğŸ“£ Print ProxDash Connection Logs"
      ],
      "metadata": {
        "id": "J_XhV8YmEypX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('You should be able to see ProxDash connection status:')\n",
        "px.reset_state()\n",
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_4',\n",
        "    proxdash_options=px.ProxDashOptions(\n",
        "        stdout=True\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXBZ0HpSEuA3",
        "outputId": "59e1b35d-47e0-40ad-989d-e541adde7dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You should be able to see ProxDash connection status:\n",
            "{'logging_type': 'INFO',\n",
            " 'message': 'Connected to ProxDash at '\n",
            "            'https://proxainest-production.up.railway.app',\n",
            " 'timestamp': '2025-05-17T02:30:52.701415'}\n",
            "{'logging_type': 'INFO',\n",
            " 'message': 'Connected to ProxDash experiment: '\n",
            "            'colab_experiments/advanced_features/run_4',\n",
            " 'timestamp': '2025-05-17T02:30:52.702129'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.5. ğŸš§ Disable ProxDash\n",
        "\n",
        "You can remove `PROXDASH_API_KEY` from environment variables to disable ProxDash but there is simpler way:"
      ],
      "metadata": {
        "id": "VeP2kECOFNGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.connect(\n",
        "    proxdash_options=px.ProxDashOptions(\n",
        "        stdout=True,\n",
        "        disable_proxdash=True,\n",
        "    ))\n",
        "\n",
        "print('Following record should not appear on ProxDash logging history:\\n'\n",
        "      'https://www.proxai.co/dashboard/logging')\n",
        "reponse = px.generate_text('This prompt should not appear on proxdash')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "hC6VintAFMnu",
        "outputId": "174da896-748f-4586-b5a7-0307f339f2e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'logging_type': 'INFO',\n",
            " 'message': 'ProxDash connection disabled.',\n",
            " 'timestamp': '2025-05-17T02:33:18.995521'}\n",
            "Following record should not appear on ProxDash logging history:\n",
            "https://www.proxai.co/dashboard/logging\n",
            "Checking available models, this may take a while...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Understood. This response is solely to acknowledge that the prompt will not appear on proxdash.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. ğŸš¦ Strict feature Test\n",
        "\n",
        "Documentation: [https://www.proxai.co/proxai-docs/advanced/strict-feature-test](https://www.proxai.co/proxai-docs/advanced/strict-feature-test)\n",
        "\n",
        "Not all models are supporting all features. ProxAI tries best effort to handle feature requirements.\n",
        "* If you want to fail if feature is not supported on model, you can set `strict_feature_test=False` on `px.connect()`"
      ],
      "metadata": {
        "id": "4dPyCyYEGDSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_6',\n",
        "    strict_feature_test=True)\n",
        "\n",
        "try:\n",
        "  px.generate_text(\n",
        "      'Create me a simple poem about birds.',\n",
        "      provider_model=('openai', 'o1'),\n",
        "      temperature=0.3)\n",
        "except Exception as e:\n",
        "  print(\n",
        "      'This query raises error because temperature feature is not supported on '\n",
        "      'OpenAI\\'s o1 model.')\n",
        "  print(f'Error: {e}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezIsh3-yGCt_",
        "outputId": "e4d07282-2740-4335-fe82-df2ecd05aa5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This query raises error because temperature feature is not supported on OpenAI's o1 model.\n",
            "Error: o1 does not support temperature.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. âš ï¸ Suppress Provider Errors\n",
        "\n",
        "Documentation: [https://www.proxai.co/proxai-docs/advanced/suppress-provider-errors](https://www.proxai.co/proxai-docs/advanced/suppress-provider-errors)\n",
        "\n",
        "Instead of raising AI provider errors, you can get `logging_record.response_query.error` and `logging_record.response_query.error_traceback` by setting `suppress_provider_errors=True`.\n",
        "\n",
        "This allows you to continue the processing and check what errors are happening from ProxDash."
      ],
      "metadata": {
        "id": "JPD0E0inHq-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.connect(suppress_provider_errors=True)\n",
        "\n",
        "# First, lets pick already failing model:\n",
        "model_status = px.models.list_models(return_all=True)\n",
        "if not model_status.failed_models:\n",
        "  raise ValueError(\n",
        "      \"There is no failed models to try \\'suppress_provider_errors\\' option.\")\n",
        "provider_model = list(model_status.failed_models)[0]\n",
        "\n",
        "# Second, make call with suppress_provider_errors=True:\n",
        "response = px.generate_text(\n",
        "    'If 5 + 20 would be a poem, what life be look like?',\n",
        "    provider_model=provider_model,\n",
        "    extensive_return=True)\n",
        "\n",
        "# No error raised before printing the response or error:\n",
        "print(f'ğŸ¤– Model: {response.query_record.provider_model}')\n",
        "print(f'ğŸ’¬ Response: {response.response_record.response}')\n",
        "print(f'âŒ Error: {response.response_record.error.strip()}')\n",
        "print(f'âš ï¸ Error Traceback:\\n{response.response_record.error_traceback.strip()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m62vMzthHuDG",
        "outputId": "8690fd83-d993-4193-ac16-174d1598e3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– Model: (openai, gpt-4)\n",
            "ğŸ’¬ Response: None\n",
            "âŒ Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "âš ï¸ Error Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/proxai/connectors/model_connector.py\", line 451, in generate_text\n",
            "    response = self.generate_text_proc(query_record=updated_query_record)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/proxai/connectors/providers/openai.py\", line 42, in generate_text_proc\n",
            "    completion = create()\n",
            "                 ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1239, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1034, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. ğŸ“ Get Current Options\n",
        "\n",
        "Documentation: [proxai.co/proxai-docs/advanced/get-current-options](https://www.proxai.co/proxai-docs/advanced/get-current-options)"
      ],
      "metadata": {
        "id": "DaDw5jc8I56Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.1. ğŸ» Simple Usage"
      ],
      "metadata": {
        "id": "3fskpsoZJd6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(asdict(px.get_current_options()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO2gvLY5JOgw",
        "outputId": "d9d19f13-044b-45e6-a821-9f9a021040a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'allow_multiprocessing': True,\n",
            " 'cache_options': {'cache_path': None,\n",
            "                   'clear_model_cache_on_connect': False,\n",
            "                   'clear_query_cache_on_connect': False,\n",
            "                   'disable_model_cache': False,\n",
            "                   'model_cache_duration': None,\n",
            "                   'retry_if_error_cached': False,\n",
            "                   'unique_response_limit': 1},\n",
            " 'default_model_cache_path': '/root/.cache/proxai',\n",
            " 'experiment_path': None,\n",
            " 'hidden_run_key': '692410',\n",
            " 'logging_options': {'hide_sensitive_content': False,\n",
            "                     'logging_path': None,\n",
            "                     'stdout': False},\n",
            " 'model_test_timeout': 25,\n",
            " 'proxdash_options': {'api_key': 'hb4ozpw-mar2vd2j-rhir7t2vjwf',\n",
            "                      'base_url': 'https://proxainest-production.up.railway.app',\n",
            "                      'disable_proxdash': False,\n",
            "                      'hide_sensitive_content': False,\n",
            "                      'stdout': False},\n",
            " 'root_logging_path': None,\n",
            " 'run_type': <RunType.PRODUCTION: 'PRODUCTION'>,\n",
            " 'strict_feature_test': False,\n",
            " 'suppress_provider_errors': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.2. ğŸ¦ Also Another Simple Example"
      ],
      "metadata": {
        "id": "WAnq5PdYJoko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px.connect(\n",
        "    experiment_path='colab_experiments/advanced_features/run_5',\n",
        "    logging_path='/content/',\n",
        "    logging_options=px.LoggingOptions(\n",
        "        stdout=True,\n",
        "        hide_sensitive_content=True),\n",
        "    cache_path='/content/',\n",
        "    cache_options=px.CacheOptions(\n",
        "        retry_if_error_cached=True,\n",
        "        unique_response_limit=3),\n",
        "    proxdash_options=px.ProxDashOptions(\n",
        "        stdout=True,\n",
        "        hide_sensitive_content=True),\n",
        "    strict_feature_test=True,\n",
        "    allow_multiprocessing=False,\n",
        "    suppress_provider_errors=True)\n",
        "\n",
        "pprint(px.get_current_options(json=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGncnCXvJmtk",
        "outputId": "b2203a52-e3d7-4e1c-bd91-3edcd0068aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'logging_type': 'INFO',\n",
            " 'message': 'Connected to ProxDash experiment: '\n",
            "            'colab_experiments/advanced_features/run_5',\n",
            " 'timestamp': '2025-05-17T02:51:27.822901'}\n",
            "{'allow_multiprocessing': False,\n",
            " 'cache_options': {'cache_path': '/content/',\n",
            "                   'clear_model_cache_on_connect': False,\n",
            "                   'clear_query_cache_on_connect': False,\n",
            "                   'retry_if_error_cached': True,\n",
            "                   'unique_response_limit': 3},\n",
            " 'default_model_cache_path': '/root/.cache/proxai',\n",
            " 'experiment_path': 'colab_experiments/advanced_features/run_5',\n",
            " 'hidden_run_key': '692410',\n",
            " 'logging_options': {'hide_sensitive_content': True,\n",
            "                     'logging_path': '/content/colab_experiments/advanced_features/run_5',\n",
            "                     'stdout': True},\n",
            " 'model_test_timeout': 25,\n",
            " 'proxdash_options': {'disable_proxdash': False,\n",
            "                      'hide_sensitive_content': True,\n",
            "                      'stdout': True},\n",
            " 'root_logging_path': '/content/',\n",
            " 'run_type': 'PRODUCTION',\n",
            " 'strict_feature_test': True,\n",
            " 'suppress_provider_errors': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¾ Final Thoughts and Next Steps\n",
        "\n",
        "### ğŸ‰ CongratulationsğŸ‰\n",
        "\n",
        "Congratulations on completing the ProxAI Advanced Usage Tutorial!\n",
        "\n",
        "### ğŸš€ Create, Innovate, and Share!\n",
        "\n",
        "The real magic happens when you start building! We wholeheartedly encourage you to:\n",
        "* ğŸ¥‡ **Develop cool scripts, intricate code examples, and innovative projects** using ProxAI.\n",
        "* ğŸ† **Share your creations!** Whether it's with the ProxAI community, on your blog, in forums, or with colleagues, your examples can inspire and help others.\n",
        "\n",
        "### ğŸŒ± Learn by Doing\n",
        "\n",
        "We've covered a lot of ground, and going through each feature meticulously one by one can sometimes feel daunting. Remember, the most effective way to master ProxAI (or any tool!) is to **dive in and use it directly to solve problems.**\n",
        "\n",
        "### ğŸ¤ Contribute to ProxAI\n",
        "\n",
        "ProxAI is an open-source project, and its strength grows with its community. We warmly invite you to contribute!\n",
        "* **Report bugs or suggest new features:** Your feedback is invaluable. (See [Reporting Bugs & Feature Requests](https://www.proxai.co/resources/community))\n",
        "* **Improve documentation:** Help us make the docs clearer and more comprehensive.\n",
        "* **Write code:** Contribute fixes, new features, or new provider integrations.\n",
        "\n",
        "Check out our **[ProxAI GitHub repository](https://github.com/proxai/proxai)** and the [Contribution Guidelines](https://www.proxai.co/resources/community/guidelines) to get started.\n",
        "\n",
        "### ğŸ“ Get in Touch\n",
        "\n",
        "We're here to help and love hearing from our users!\n",
        "\n",
        "* **Discord Community:** For real-time chat, support, and discussions: [discord.gg/QhrDkzMHrP](https://discord.gg/QhrDkzMHrP)\n",
        "* **GitHub Issues:** For technical questions, bug reports, and feature requests: [github.com/proxai/proxai/issues](https://github.com/proxai/proxai/issues)\n",
        "* **Email Contacts:**\n",
        "    * Feedback & Feature Suggestions: [feedback@proxai.co](feedback@proxai.co) â­ï¸\n",
        "    * Development & Contribution Support: [dev@proxai.co](dev@proxai.co)\n",
        "    * General Community Inquiries: [community@proxai.co](community@proxai.co)\n",
        "\n",
        "Thank you for learning with ProxAI. We can't wait to see what you build! ğŸš€"
      ],
      "metadata": {
        "id": "x2Py-bKdJ28Y"
      }
    }
  ]
}